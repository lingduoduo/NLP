{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "87262cd5-43e6-4b53-8725-6bd0c19caa84",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### A cluster has been created for this demo\n",
    "To run this demo, just select the cluster `dbdemos-llm-rag-chatbot-ling_huang` from the dropdown menu ([open cluster configuration](https://dbc-57a6b8b4-32b6.cloud.databricks.com/#setting/clusters/1212-021800-o0bqrfse/configuration)). <br />\n",
    "*Note: If the cluster was deleted after 30 days, you can re-create it with `dbdemos.create_cluster('llm-rag-chatbot')` or re-install the demo: `dbdemos.install('llm-rag-chatbot')`*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "210172e9-f00d-48ec-927c-84ed2311d899",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 2/ Creating the chatbot with Retrieval Augmented Generation (RAG)\n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/chatbot-rag/llm-rag-self-managed-flow-2.png?raw=true\" style=\"float: right; margin-left: 10px\"  width=\"900px;\">\n",
    "\n",
    "Our Vector Search Index is now ready!\n",
    "\n",
    "Let's now create and deploy a new Model Serving Endpoint to perform RAG.\n",
    "\n",
    "The flow will be the following:\n",
    "\n",
    "- A user asks a question\n",
    "- The question is sent to our serverless Chatbot RAG endpoint\n",
    "- The endpoint compute the embeddings and searches for docs similar to the question, leveraging the Vector Search Index\n",
    "- The endpoint creates a prompt enriched with the doc\n",
    "- The prompt is sent to the Foundation Model Serving Endpoint\n",
    "- We display the output to our users!\n",
    "\n",
    "\n",
    "<!-- Collect usage data (view). Remove it to disable collection or disable tracker during installation. View README for more details.  -->\n",
    "<img width=\"1px\" src=\"https://ppxrzfxige.execute-api.us-west-2.amazonaws.com/v1/analytics?category=data-science&org_id=local&notebook=%2F02-Deploy-RAG-Chatbot-Model&demo_name=llm-rag-chatbot&event=VIEW&path=%2F_dbdemos%2Fdata-science%2Fllm-rag-chatbot%2F02-Deploy-RAG-Chatbot-Model&version=1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb26794f-b7dc-4c42-bc21-af91c089ca42",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "*Note: RAG performs document searches using Databricks Vector Search. In this notebook, we assume that the search index is ready for use. Make sure you run the previous [01-Data-Preparation-and-Index]($./01-Data-Preparation-and-Index [DO NOT EDIT]) notebook.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ab4d64b-6376-4788-884f-3da570118212",
     "showTitle": true,
     "title": "Install the required libraries"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\nCollecting git+https://github.com/mlflow/mlflow.git@gateway-migration\n  Cloning https://github.com/mlflow/mlflow.git (to revision gateway-migration) to /tmp/pip-req-build-1okzxa3f\n  Running command git clone --filter=blob:none --quiet https://github.com/mlflow/mlflow.git /tmp/pip-req-build-1okzxa3f\n  Running command git checkout -b gateway-migration --track origin/gateway-migration\n  Switched to a new branch 'gateway-migration'\n  branch 'gateway-migration' set up to track 'origin/gateway-migration'.\n  Resolved https://github.com/mlflow/mlflow.git to commit db1e9e851f873aefe2bdc4938a316d665e67fed2\n  Running command git submodule update --init --recursive -q\n  Installing build dependencies: started\n  Installing build dependencies: finished with status 'done'\n  Getting requirements to build wheel: started\n  Getting requirements to build wheel: finished with status 'done'\n  Preparing metadata (pyproject.toml): started\n  Preparing metadata (pyproject.toml): finished with status 'done'\nRequirement already satisfied: langchain==0.0.344 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-be45fc48-b3b0-4aa4-97cc-8b8e48f59b90/lib/python3.10/site-packages (0.0.344)\nRequirement already satisfied: databricks-vectorsearch==0.22 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-be45fc48-b3b0-4aa4-97cc-8b8e48f59b90/lib/python3.10/site-packages (0.22)\nRequirement already satisfied: databricks-sdk==0.12.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-be45fc48-b3b0-4aa4-97cc-8b8e48f59b90/lib/python3.10/site-packages (0.12.0)\nRequirement already satisfied: PyYAML>=5.3 in /databricks/python3/lib/python3.10/site-packages (from langchain==0.0.344) (6.0)\nRequirement already satisfied: pydantic<3,>=1 in /databricks/python3/lib/python3.10/site-packages (from langchain==0.0.344) (1.10.6)\nRequirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /databricks/python3/lib/python3.10/site-packages (from langchain==0.0.344) (0.6.2)\nRequirement already satisfied: langsmith<0.1.0,>=0.0.63 in /databricks/python3/lib/python3.10/site-packages (from langchain==0.0.344) (0.0.64)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /databricks/python3/lib/python3.10/site-packages (from langchain==0.0.344) (1.33)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /databricks/python3/lib/python3.10/site-packages (from langchain==0.0.344) (1.4.39)\nRequirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /databricks/python3/lib/python3.10/site-packages (from langchain==0.0.344) (4.0.3)\nRequirement already satisfied: numpy<2,>=1 in /databricks/python3/lib/python3.10/site-packages (from langchain==0.0.344) (1.23.5)\nRequirement already satisfied: tenacity<9.0.0,>=8.1.0 in /databricks/python3/lib/python3.10/site-packages (from langchain==0.0.344) (8.1.0)\nRequirement already satisfied: requests<3,>=2 in /databricks/python3/lib/python3.10/site-packages (from langchain==0.0.344) (2.28.1)\nRequirement already satisfied: langchain-core<0.1,>=0.0.8 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-be45fc48-b3b0-4aa4-97cc-8b8e48f59b90/lib/python3.10/site-packages (from langchain==0.0.344) (0.0.13)\nRequirement already satisfied: anyio<4.0 in /databricks/python3/lib/python3.10/site-packages (from langchain==0.0.344) (3.5.0)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /databricks/python3/lib/python3.10/site-packages (from langchain==0.0.344) (3.8.6)\nRequirement already satisfied: mlflow-skinny<3,>=2.4.0 in /databricks/python3/lib/python3.10/site-packages (from databricks-vectorsearch==0.22) (2.8.1)\nRequirement already satisfied: protobuf<5,>=3.12.0 in /databricks/python3/lib/python3.10/site-packages (from databricks-vectorsearch==0.22) (4.24.0)\nRequirement already satisfied: gitpython<4,>=2.1.0 in /databricks/python3/lib/python3.10/site-packages (from mlflow==2.8.2.dev0) (3.1.27)\nRequirement already satisfied: sqlparse<1,>=0.4.0 in /databricks/python3/lib/python3.10/site-packages (from mlflow==2.8.2.dev0) (0.4.2)\nRequirement already satisfied: scipy<2 in /databricks/python3/lib/python3.10/site-packages (from mlflow==2.8.2.dev0) (1.10.0)\nRequirement already satisfied: scikit-learn<2 in /databricks/python3/lib/python3.10/site-packages (from mlflow==2.8.2.dev0) (1.1.1)\nRequirement already satisfied: entrypoints<1 in /databricks/python3/lib/python3.10/site-packages (from mlflow==2.8.2.dev0) (0.4)\nRequirement already satisfied: pytz<2024 in /databricks/python3/lib/python3.10/site-packages (from mlflow==2.8.2.dev0) (2022.7)\nRequirement already satisfied: gunicorn<22 in /databricks/python3/lib/python3.10/site-packages (from mlflow==2.8.2.dev0) (20.1.0)\nRequirement already satisfied: cloudpickle<4 in /databricks/python3/lib/python3.10/site-packages (from mlflow==2.8.2.dev0) (2.0.0)\nRequirement already satisfied: alembic!=1.10.0,<2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-be45fc48-b3b0-4aa4-97cc-8b8e48f59b90/lib/python3.10/site-packages (from mlflow==2.8.2.dev0) (1.13.1)\nRequirement already satisfied: matplotlib<4 in /databricks/python3/lib/python3.10/site-packages (from mlflow==2.8.2.dev0) (3.7.0)\nRequirement already satisfied: databricks-cli<1,>=0.8.7 in /databricks/python3/lib/python3.10/site-packages (from mlflow==2.8.2.dev0) (0.18.0)\nRequirement already satisfied: docker<7,>=4.0.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-be45fc48-b3b0-4aa4-97cc-8b8e48f59b90/lib/python3.10/site-packages (from mlflow==2.8.2.dev0) (6.1.3)\nRequirement already satisfied: Jinja2<4,>=2.11 in /databricks/python3/lib/python3.10/site-packages (from mlflow==2.8.2.dev0) (3.1.2)\nRequirement already satisfied: pandas<3 in /databricks/python3/lib/python3.10/site-packages (from mlflow==2.8.2.dev0) (1.5.3)\nRequirement already satisfied: pyarrow<15,>=4.0.0 in /databricks/python3/lib/python3.10/site-packages (from mlflow==2.8.2.dev0) (8.0.0)\nRequirement already satisfied: click<9,>=7.0 in /databricks/python3/lib/python3.10/site-packages (from mlflow==2.8.2.dev0) (8.0.4)\nRequirement already satisfied: importlib-metadata!=4.7.0,<7,>=3.7.0 in /databricks/python3/lib/python3.10/site-packages (from mlflow==2.8.2.dev0) (4.11.3)\nRequirement already satisfied: querystring-parser<2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-be45fc48-b3b0-4aa4-97cc-8b8e48f59b90/lib/python3.10/site-packages (from mlflow==2.8.2.dev0) (1.2.4)\nRequirement already satisfied: markdown<4,>=3.3 in /databricks/python3/lib/python3.10/site-packages (from mlflow==2.8.2.dev0) (3.4.1)\nRequirement already satisfied: packaging<24 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-be45fc48-b3b0-4aa4-97cc-8b8e48f59b90/lib/python3.10/site-packages (from mlflow==2.8.2.dev0) (23.2)\nRequirement already satisfied: Flask<4 in /databricks/python3/lib/python3.10/site-packages (from mlflow==2.8.2.dev0) (2.2.5)\nRequirement already satisfied: boto3>1 in /databricks/python3/lib/python3.10/site-packages (from mlflow==2.8.2.dev0) (1.24.28)\nRequirement already satisfied: google-cloud-storage>=1.30.0 in /databricks/python3/lib/python3.10/site-packages (from mlflow==2.8.2.dev0) (2.11.0)\nRequirement already satisfied: azure-storage-file-datalake>12 in /databricks/python3/lib/python3.10/site-packages (from mlflow==2.8.2.dev0) (12.14.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /databricks/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.344) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /databricks/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.344) (22.1.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /databricks/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.344) (1.3.1)\nRequirement already satisfied: yarl<2.0,>=1.0 in /databricks/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.344) (1.9.2)\nRequirement already satisfied: multidict<7.0,>=4.5 in /databricks/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.344) (6.0.4)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /databricks/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.344) (2.0.4)\nRequirement already satisfied: typing-extensions>=4 in /databricks/python3/lib/python3.10/site-packages (from alembic!=1.10.0,<2->mlflow==2.8.2.dev0) (4.4.0)\nRequirement already satisfied: Mako in /databricks/python3/lib/python3.10/site-packages (from alembic!=1.10.0,<2->mlflow==2.8.2.dev0) (1.2.0)\nRequirement already satisfied: idna>=2.8 in /databricks/python3/lib/python3.10/site-packages (from anyio<4.0->langchain==0.0.344) (3.4)\nRequirement already satisfied: sniffio>=1.1 in /databricks/python3/lib/python3.10/site-packages (from anyio<4.0->langchain==0.0.344) (1.2.0)\nRequirement already satisfied: azure-storage-blob<13.0.0,>=12.19.0 in /databricks/python3/lib/python3.10/site-packages (from azure-storage-file-datalake>12->mlflow==2.8.2.dev0) (12.19.0)\nRequirement already satisfied: azure-core<2.0.0,>=1.28.0 in /databricks/python3/lib/python3.10/site-packages (from azure-storage-file-datalake>12->mlflow==2.8.2.dev0) (1.29.1)\nRequirement already satisfied: isodate>=0.6.1 in /databricks/python3/lib/python3.10/site-packages (from azure-storage-file-datalake>12->mlflow==2.8.2.dev0) (0.6.1)\nRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /databricks/python3/lib/python3.10/site-packages (from boto3>1->mlflow==2.8.2.dev0) (0.10.0)\nRequirement already satisfied: botocore<1.28.0,>=1.27.28 in /databricks/python3/lib/python3.10/site-packages (from boto3>1->mlflow==2.8.2.dev0) (1.27.96)\nRequirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /databricks/python3/lib/python3.10/site-packages (from boto3>1->mlflow==2.8.2.dev0) (0.6.2)\nRequirement already satisfied: pyjwt>=1.7.0 in /usr/lib/python3/dist-packages (from databricks-cli<1,>=0.8.7->mlflow==2.8.2.dev0) (2.3.0)\nRequirement already satisfied: tabulate>=0.7.7 in /databricks/python3/lib/python3.10/site-packages (from databricks-cli<1,>=0.8.7->mlflow==2.8.2.dev0) (0.8.10)\nRequirement already satisfied: urllib3<3,>=1.26.7 in /databricks/python3/lib/python3.10/site-packages (from databricks-cli<1,>=0.8.7->mlflow==2.8.2.dev0) (1.26.14)\nRequirement already satisfied: six>=1.10.0 in /usr/lib/python3/dist-packages (from databricks-cli<1,>=0.8.7->mlflow==2.8.2.dev0) (1.16.0)\nRequirement already satisfied: oauthlib>=3.1.0 in /usr/lib/python3/dist-packages (from databricks-cli<1,>=0.8.7->mlflow==2.8.2.dev0) (3.2.0)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /databricks/python3/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.344) (0.9.0)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /databricks/python3/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.344) (3.20.1)\nRequirement already satisfied: websocket-client>=0.32.0 in /databricks/python3/lib/python3.10/site-packages (from docker<7,>=4.0.0->mlflow==2.8.2.dev0) (0.58.0)\nRequirement already satisfied: Werkzeug>=2.2.2 in /databricks/python3/lib/python3.10/site-packages (from Flask<4->mlflow==2.8.2.dev0) (2.2.2)\nRequirement already satisfied: itsdangerous>=2.0 in /databricks/python3/lib/python3.10/site-packages (from Flask<4->mlflow==2.8.2.dev0) (2.0.1)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /databricks/python3/lib/python3.10/site-packages (from gitpython<4,>=2.1.0->mlflow==2.8.2.dev0) (4.0.11)\nRequirement already satisfied: google-resumable-media>=2.6.0 in /databricks/python3/lib/python3.10/site-packages (from google-cloud-storage>=1.30.0->mlflow==2.8.2.dev0) (2.6.0)\nRequirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /databricks/python3/lib/python3.10/site-packages (from google-cloud-storage>=1.30.0->mlflow==2.8.2.dev0) (2.3.3)\nRequirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /databricks/python3/lib/python3.10/site-packages (from google-cloud-storage>=1.30.0->mlflow==2.8.2.dev0) (2.14.0)\nRequirement already satisfied: google-auth<3.0dev,>=1.25.0 in /databricks/python3/lib/python3.10/site-packages (from google-cloud-storage>=1.30.0->mlflow==2.8.2.dev0) (2.21.0)\nRequirement already satisfied: setuptools>=3.0 in /databricks/python3/lib/python3.10/site-packages (from gunicorn<22->mlflow==2.8.2.dev0) (65.6.3)\nRequirement already satisfied: zipp>=0.5 in /databricks/python3/lib/python3.10/site-packages (from importlib-metadata!=4.7.0,<7,>=3.7.0->mlflow==2.8.2.dev0) (3.11.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /databricks/python3/lib/python3.10/site-packages (from Jinja2<4,>=2.11->mlflow==2.8.2.dev0) (2.1.1)\nRequirement already satisfied: jsonpointer>=1.9 in /databricks/python3/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain==0.0.344) (2.4)\nRequirement already satisfied: pyparsing>=2.3.1 in /databricks/python3/lib/python3.10/site-packages (from matplotlib<4->mlflow==2.8.2.dev0) (3.0.9)\nRequirement already satisfied: cycler>=0.10 in /databricks/python3/lib/python3.10/site-packages (from matplotlib<4->mlflow==2.8.2.dev0) (0.11.0)\nRequirement already satisfied: fonttools>=4.22.0 in /databricks/python3/lib/python3.10/site-packages (from matplotlib<4->mlflow==2.8.2.dev0) (4.25.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /databricks/python3/lib/python3.10/site-packages (from matplotlib<4->mlflow==2.8.2.dev0) (1.4.4)\nRequirement already satisfied: python-dateutil>=2.7 in /databricks/python3/lib/python3.10/site-packages (from matplotlib<4->mlflow==2.8.2.dev0) (2.8.2)\nRequirement already satisfied: contourpy>=1.0.1 in /databricks/python3/lib/python3.10/site-packages (from matplotlib<4->mlflow==2.8.2.dev0) (1.0.5)\nRequirement already satisfied: pillow>=6.2.0 in /databricks/python3/lib/python3.10/site-packages (from matplotlib<4->mlflow==2.8.2.dev0) (9.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.0.344) (2022.12.7)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /databricks/python3/lib/python3.10/site-packages (from scikit-learn<2->mlflow==2.8.2.dev0) (2.2.0)\nRequirement already satisfied: joblib>=1.0.0 in /databricks/python3/lib/python3.10/site-packages (from scikit-learn<2->mlflow==2.8.2.dev0) (1.2.0)\nRequirement already satisfied: greenlet!=0.4.17 in /databricks/python3/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain==0.0.344) (2.0.1)\nRequirement already satisfied: cryptography>=2.1.4 in /databricks/python3/lib/python3.10/site-packages (from azure-storage-blob<13.0.0,>=12.19.0->azure-storage-file-datalake>12->mlflow==2.8.2.dev0) (39.0.1)\nRequirement already satisfied: smmap<6,>=3.0.1 in /databricks/python3/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=2.1.0->mlflow==2.8.2.dev0) (5.0.0)\nRequirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /databricks/python3/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage>=1.30.0->mlflow==2.8.2.dev0) (1.61.0)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /databricks/python3/lib/python3.10/site-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage>=1.30.0->mlflow==2.8.2.dev0) (0.2.8)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /databricks/python3/lib/python3.10/site-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage>=1.30.0->mlflow==2.8.2.dev0) (5.3.2)\nRequirement already satisfied: rsa<5,>=3.1.4 in /databricks/python3/lib/python3.10/site-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage>=1.30.0->mlflow==2.8.2.dev0) (4.9)\nRequirement already satisfied: google-crc32c<2.0dev,>=1.0 in /databricks/python3/lib/python3.10/site-packages (from google-resumable-media>=2.6.0->google-cloud-storage>=1.30.0->mlflow==2.8.2.dev0) (1.5.0)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /databricks/python3/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.0.344) (0.4.3)\nRequirement already satisfied: cffi>=1.12 in /databricks/python3/lib/python3.10/site-packages (from cryptography>=2.1.4->azure-storage-blob<13.0.0,>=12.19.0->azure-storage-file-datalake>12->mlflow==2.8.2.dev0) (1.15.1)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /databricks/python3/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-cloud-storage>=1.30.0->mlflow==2.8.2.dev0) (0.4.8)\nRequirement already satisfied: pycparser in /databricks/python3/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=2.1.4->azure-storage-blob<13.0.0,>=12.19.0->azure-storage-file-datalake>12->mlflow==2.8.2.dev0) (2.21)\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install \"git+https://github.com/mlflow/mlflow.git@gateway-migration\" langchain==0.0.344 databricks-vectorsearch==0.22 databricks-sdk==0.12.0 mlflow[databricks]\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2c07138-9a4b-4de9-83c1-d07252d9cb1f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using catalog.database `ling_test_demo`.`default`\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%run ./_resources/00-init $reset_all_data=false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a40f4f61-8275-47c1-877f-9e3aede932cc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "  \n",
    "###  This demo requires a secret to work:\n",
    "Your Model Serving Endpoint needs a secret to authenticate against your Vector Search Index (see [Documentation](https://docs.databricks.com/en/security/secrets/secrets.html)).  <br/>\n",
    "- You'll need to setup the Databricks CLI on your laptop or using this cluster terminal: <br/>\n",
    "`pip install databricks-cli` <br/>\n",
    "- Configure the CLI. You'll need your workspace URL and a PAT token from your profile page<br>\n",
    "`databricks configure`\n",
    "- Create the dbdemos scope:<br/>\n",
    "`databricks secrets create-scope dbdemos`\n",
    "- Save your service principal secret. It will be used by the Model Endpoint to autenticate. If this is a demo/test, you can use one of your PAT token.<br>\n",
    "`databricks secrets put-secret dbdemos rag_sp_token`\n",
    "\n",
    "*Note: Make sure your service principal has access to the Vector Search index:*\n",
    "\n",
    "```\n",
    "spark.sql('GRANT USAGE ON CATALOG <catalog> TO `<YOUR_SP>`');\n",
    "spark.sql('GRANT USAGE ON DATABASE <catalog>.<db> TO `<YOUR_SP>`');\n",
    "from databricks.sdk import WorkspaceClient\n",
    "import databricks.sdk.service.catalog as c\n",
    "WorkspaceClient().grants.update(c.SecurableType.TABLE, <index_name>, \n",
    "                                changes=[c.PermissionsChange(add=[c.Privilege[\"SELECT\"]], principal=\"<YOUR_SP>\")])\n",
    "  ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "062712ff-b3d0-4e5d-ae12-e36801c646ac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sp_name = spark.sql('select current_user() as user').collect()[0]['user'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27d275b4-c749-49fe-9540-d3c490d26fe5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ling.huang@caden.io\n"
     ]
    }
   ],
   "source": [
    "print(sp_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e5ecfde-73e3-4dfc-aa06-294e3dd07604",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalog = \"ling_test_demo\"\n",
    "db = \"default\"\n",
    "index_name=\"ling_test_demo.default.databricks_documentation_vs_index\"\n",
    "\n",
    "# Make sure you replace sp_name with the SP owning the token in the secret. It has be the principal in the PAT token used in the model\n",
    "sp_name = spark.sql('select current_user() as user').collect()[0]['user'] #Set to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "120dd837-69c9-465e-a748-3a8216ed4971",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f'GRANT USAGE ON CATALOG {catalog} TO `{sp_name}`');\n",
    "spark.sql(f'GRANT USAGE ON DATABASE {catalog}.{db} TO `{sp_name}`');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ee41090-00b3-4d76-b75c-e0f991efd898",
     "showTitle": true,
     "title": "Give your SP read access to your Vector Search Index"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "PermissionsList(privilege_assignments=[PrivilegeAssignment(principal='ling.huang@caden.io', privileges=['SELECT'])])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from databricks.sdk import WorkspaceClient\n",
    "import databricks.sdk.service.catalog as c\n",
    "WorkspaceClient().grants.update(c.SecurableType.TABLE, index_name, \n",
    "                                changes=[c.PermissionsChange(add=[c.Privilege[\"SELECT\"]], principal=sp_name)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d02f64c-9319-4e9f-afcb-c182ce6f53c4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Langchain retriever\n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/chatbot-rag/llm-rag-self-managed-model-1.png?raw=true\" style=\"float: right\" width=\"500px\">\n",
    "\n",
    "Let's start by building our Langchain retriever. \n",
    "\n",
    "It will be in charge of:\n",
    "\n",
    "* Creating the input question embeddings (with Databricks `bge-large-en`)\n",
    "* Calling the vector search index to find similar documents to augment the prompt with\n",
    "\n",
    "Databricks Langchain wrapper makes it easy to do in one step, handling all the underlying logic and API call for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e31ce318-9cfc-420b-b434-364f37773028",
     "showTitle": true,
     "title": "Setup authentication for our model"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://dbc-57a6b8b4-32b6.cloud.databricks.com\n"
     ]
    }
   ],
   "source": [
    "# url used to send the request to your model from the serverless endpoint\n",
    "host = \"https://\" + spark.conf.get(\"spark.databricks.workspaceUrl\")\n",
    "print(host)\n",
    "\n",
    "# os.environ['DATABRICKS_TOKEN'] = dbutils.secrets.get(\"ling_test_demo\", \"default\")\n",
    "os.environ['DATABRICKS_TOKEN'] = '-'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2134e7b-0945-48bb-bbaa-c3d5daaf5b58",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test embeddings: [0.0186004638671875, -0.0141448974609375, -0.0574951171875, 0.0034027099609375, 0.008453369140625, -0.0216064453125, -0.02471923828125, -0.004688262939453125, 0.0136566162109375, 0.050384521484375, -0.0272064208984375, -0.01470184326171875, 0.054718017578125, -0.0538330078125, -0.01035308837890625, -0.0162200927734375, -0.0188140869140625, -0.017242431640625, -0.051300048828125, 0.0177764892578125]...\n[NOTICE] Using a Personal Authentication Token (PAT). Recommended for development only. For improved performance, please use Service Principal based authentication. To disable this message, pass disable_notice=True to VectorSearchClient().\nRelevant documents: page_content='Manage billing\\nThe way you manage billing depends on the type of business relationship you have with Databricks.  \\nPay-as-you-go accounts through AWS Marketplace  \\nIf you subscribed to Databricks using AWS Marketplace, your Databricks charges appear alongside your other AWS costs. To manage your payment methods, use the AWS console.  \\nPay-as-you-go accounts paid by credit card to Databricks  \\nIf you pay Databricks directly by credit card, follow these instructions to manage billing:  \\nAs the Databricks account owner or an account-level administrator, log in to the Databricks account console.  \\nClick the Settings icon in the sidebar.  \\nIf no payment method has been added, click Add billing information and fill in the form.  \\nTo edit your existing payment method, click Edit.  \\nContract accounts  \\nIf you have a contract with Databricks, Databricks has your billing information on file.\\n\\nUpgrade your subscription' metadata={'id': 15218.0}\n"
     ]
    }
   ],
   "source": [
    "from databricks.vector_search.client import VectorSearchClient\n",
    "from langchain.vectorstores import DatabricksVectorSearch\n",
    "from langchain.embeddings import DatabricksEmbeddings\n",
    "\n",
    "# Test embedding Langchain model\n",
    "#NOTE: your question embedding model must match the one used in the chunk in the previous model \n",
    "embedding_model = DatabricksEmbeddings(endpoint=\"databricks-bge-large-en\")\n",
    "print(f\"Test embeddings: {embedding_model.embed_query('What is Apache Spark?')[:20]}...\")\n",
    "\n",
    "def get_retriever(persist_dir: str = None):\n",
    "    os.environ[\"DATABRICKS_HOST\"] = host\n",
    "    #Get the vector search index\n",
    "    vsc = VectorSearchClient(workspace_url=host, personal_access_token=os.environ[\"DATABRICKS_TOKEN\"])\n",
    "    vs_index = vsc.get_index(\n",
    "        endpoint_name=VECTOR_SEARCH_ENDPOINT_NAME,\n",
    "        index_name=index_name\n",
    "    )\n",
    "\n",
    "    # Create the retriever\n",
    "    vectorstore = DatabricksVectorSearch(\n",
    "        vs_index, text_column=\"content\", embedding=embedding_model\n",
    "    )\n",
    "    return vectorstore.as_retriever()\n",
    "\n",
    "\n",
    "# test our retriever\n",
    "vectorstore = get_retriever()\n",
    "similar_documents = vectorstore.get_relevant_documents(\"How do I track my Databricks Billing?\")\n",
    "print(f\"Relevant documents: {similar_documents[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "870f5d1d-aa82-4694-b94e-30c8c4ade5cd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Building Databricks Chat Model to query llama-2-70b-chat foundation model\n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/chatbot-rag/llm-rag-self-managed-model-3.png?raw=true\" style=\"float: right\" width=\"500px\">\n",
    "\n",
    "Our chatbot will be using llama2 foundation model to provide answer. \n",
    "\n",
    "While the model is available using the built-in [Foundation endpoint](/ml/endpoints) (using the `/serving-endpoints/databricks-llama-2-70b-chat/invocations` API), we can use Databricks Langchain Chat Model wrapper to easily build our chain.  \n",
    "\n",
    "Note: multipe type of endpoint or langchain models can be used:\n",
    "\n",
    "- Databricks Foundation models (what we'll use)\n",
    "- Your fined-tune model\n",
    "- An external model provider (such as Azure OpenAI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da4f18e3-7144-4598-a021-3999c2e000f9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test chat model: \nApache Spark is a unified analytics engine for large-scale data processing. It provides high-level APIs in Java, Python, Scala, and R, and an optimized engine that supports general execution graphs. It also provides high-level tools and libraries for data loading, transformation, and machine learning.\n\nSpark is designed to handle large-scale data processing tasks and can process data in real-time or batch mode. It is highly scalable and can handle data processing tasks that are too large for a single machine to handle. It is also highly fault-tolerant, meaning that it can continue to process data even if one or more machines fail.\n\nSpark is widely used in a variety of industries, including finance, healthcare, retail, and telecommunications. It is often used for data warehousing, machine learning, and stream processing.\n\nSome of the key features of Apache Spark include:\n\n1\n"
     ]
    }
   ],
   "source": [
    "# Test Databricks Foundation LLM model\n",
    "from langchain.chat_models import ChatDatabricks\n",
    "chat_model = ChatDatabricks(endpoint=\"databricks-llama-2-70b-chat\", max_tokens = 200)\n",
    "print(f\"Test chat model: {chat_model.predict('What is Apache Spark')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "92fd843f-fa63-45c1-9091-051ce1c982ce",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Assembling the complete RAG Chain\n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/chatbot-rag/llm-rag-self-managed-model-2.png?raw=true\" style=\"float: right\" width=\"600px\">\n",
    "\n",
    "\n",
    "Let's now merge the retriever and the model in a single Langchain chain.\n",
    "\n",
    "We will use a custom langchain template for our assistant to give proper answer.\n",
    "\n",
    "Make sure you take some time to try different templates and adjust your assistant tone and personality for your requirement.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b802588-4f10-4be1-b0fe-f2b5e3cab900",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOTICE] Using a Personal Authentication Token (PAT). Recommended for development only. For improved performance, please use Service Principal based authentication. To disable this message, pass disable_notice=True to VectorSearchClient().\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatDatabricks\n",
    "\n",
    "TEMPLATE = \"\"\"You are an assistant for Databricks users. You are answering python, coding, SQL, data engineering, spark, data science, DW and platform, API or infrastructure administration question related to Databricks. If the question is not related to one of these topics, kindly decline to answer. If you don't know the answer, just say that you don't know, don't try to make up an answer. Keep the answer as concise as possible.\n",
    "Use the following pieces of context to answer the question at the end:\n",
    "{context}\n",
    "Question: {question}\n",
    "Answer:\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(template=TEMPLATE, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=chat_model,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=get_retriever(),\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d7a84fe-0c08-43ff-aec5-1a70ee851fc1",
     "showTitle": true,
     "title": "Let's try our chatbot in the notebook directly:"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can track billing usage on your workspaces by using Databricks' built-in monitoring and cost control features. You can tag workspaces, clusters, and pools to monitor cost and accurately attribute Databricks usage to your organization's business units and teams for chargebacks. These tags propagate to detailed DBU usage reports and to AWS EC2 and AWS EBS instances for cost analysis.\n\nIn the Usage details panel, you can view a list of detailed usage in DBU or estimated cost in $USD in a table format. The cost is also broken down into SKU groups. You can toggle between estimated cost and DBUs using the $USD/DBU picker. You can also use the date range picker to specify an exact time range or choose among pre-defined options such as Last 6 months.\n\nAdditionally, you can download aggregated usage data displayed in your current graph or download overall usage\n"
     ]
    }
   ],
   "source": [
    "# langchain.debug = True #uncomment to see the chain details and the full prompt being sent\n",
    "question = {\"query\": \"How can I track billing usage on my workspaces?\"}\n",
    "answer = chain.run(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e250c32-0d14-4688-bfc6-6b81dc397a1a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Saving our model to Unity Catalog registry\n",
    "\n",
    "Now that our model is ready, we can register it within our Unity Catalog schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6da16042-6781-4d49-b161-285a8c74499c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/12/24 21:40:16 INFO mlflow.tracking.fluent: Experiment with name '/dbdemos/experiments/chatbot-rag-llm/simple' does not exist. Creating a new experiment.\n"
     ]
    }
   ],
   "source": [
    "#dbdemos__delete_this_cell\n",
    "#force the experiment to the field demos one. Required to launch as a batch\n",
    "init_experiment_for_batch(\"chatbot-rag-llm\", \"simple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f3d131c-6b77-4607-a178-f19346d7d261",
     "showTitle": true,
     "title": "Register our chain to MLFlow"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.10/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f159106dff814ee696454fd7de1fcdec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Successfully registered model 'ling_test_demo.default.dbdemos_chatbot_model'.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15f9e6e5e7a842b48b9e7e2cb302fe59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created version '1' of model 'ling_test_demo.default.dbdemos_chatbot_model'.\n2023/12/24 21:41:06 WARNING mlflow.models.model: Model logged without a signature. Signatures will be required for upcoming model registry features as they validate model inputs and denote the expected schema of model outputs. Please visit https://www.mlflow.org/docs/2.8.2/models.html#set-signature-on-logged-model for instructions on setting a model signature on your logged model.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a248ca0edf54820bee7d5e5c9058b9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1c041204fc14ef58a88b49ca85c7ea7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/12/24 21:41:39 INFO mlflow.store.artifact.cloud_artifact_repo: The progress bar can be disabled by setting the environment variable MLFLOW_ENABLE_ARTIFACTS_PROGRESS_BAR to false\nRegistered model 'ling_test_demo.default.dbdemos_chatbot_model' already exists. Creating a new version of this model...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b58a971365f7434cadac930f4fdd6da9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/12/24 21:41:45 INFO mlflow.store.artifact.cloud_artifact_repo: The progress bar can be disabled by setting the environment variable MLFLOW_ENABLE_ARTIFACTS_PROGRESS_BAR to false\nCreated version '2' of model 'ling_test_demo.default.dbdemos_chatbot_model'.\n"
     ]
    }
   ],
   "source": [
    "from mlflow.models import infer_signature\n",
    "import mlflow\n",
    "\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "model_name = f\"{catalog}.{db}.dbdemos_chatbot_model\"\n",
    "\n",
    "with mlflow.start_run(run_name=\"dbdemos_chatbot_rag\") as run:\n",
    "    signature = infer_signature(question, answer)\n",
    "    model_info = mlflow.langchain.log_model(\n",
    "        chain,\n",
    "        loader_fn=get_retriever,  # Load the retriever with DATABRICKS_TOKEN env as secret (for authentication).\n",
    "        artifact_path=\"chain\",\n",
    "        registered_model_name=model_name,\n",
    "        pip_requirements=[\n",
    "            \"git+https://github.com/mlflow/mlflow.git@gateway-migration\",\n",
    "            \"langchain==\" + langchain.__version__,\n",
    "            \"databricks-vectorsearch\",\n",
    "        ],\n",
    "        input_example=question,\n",
    "        signature=signature\n",
    "    )\n",
    "    #------------------------\n",
    "    # TODO: temporary fix to add the wheel, we won't need this after we switch to using PyPI\n",
    "    import mlflow.models.utils\n",
    "    mlflow.models.utils.add_libraries_to_model(\n",
    "        f\"models:/{model_name}/{get_latest_model_version(model_name)}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f53ac80-acfb-45fe-b341-9f4b283def24",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Deploying our Chat Model as a Serverless Model Endpoint \n",
    "\n",
    "Our model is saved in Unity Catalog. The last step is to deploy it as a Model Serving.\n",
    "\n",
    "We'll then be able to sending requests from our assistant frontend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d889ace-922e-4de6-9c5e-e8d3d5c9d231",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the endpoint dbdemos_endpoint_ling_test_demo_default, this will take a few minutes to package and deploy the endpoint...\n"
     ]
    }
   ],
   "source": [
    "# Create or update serving endpoint\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service.serving import EndpointCoreConfigInput, ServedModelInput\n",
    "\n",
    "serving_endpoint_name = f\"dbdemos_endpoint_{catalog}_{db}\"[:63]\n",
    "latest_model_version = get_latest_model_version(model_name)\n",
    "\n",
    "w = WorkspaceClient()\n",
    "endpoint_config = EndpointCoreConfigInput(\n",
    "    name=serving_endpoint_name,\n",
    "    served_models=[\n",
    "        ServedModelInput(\n",
    "            model_name=model_name,\n",
    "            model_version=latest_model_version,\n",
    "            workload_size=\"Small\",\n",
    "            scale_to_zero_enabled=True,\n",
    "            environment_vars={\n",
    "                \"DATABRICKS_TOKEN\": \"{{secrets/dbdemos/rag_sp_token}}\",  # <scope>/<secret> that contains an access token\n",
    "            }\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "existing_endpoint = next(\n",
    "    (e for e in w.serving_endpoints.list() if e.name == serving_endpoint_name), None\n",
    ")\n",
    "if existing_endpoint == None:\n",
    "    print(f\"Creating the endpoint {serving_endpoint_name}, this will take a few minutes to package and deploy the endpoint...\")\n",
    "    w.serving_endpoints.create_and_wait(name=serving_endpoint_name, config=endpoint_config)\n",
    "else:\n",
    "    print(f\"Updating the endpoint {serving_endpoint_name} to version {latest_model_version}, this will take a few minutes to package and deploy the endpoint...\")\n",
    "    w.serving_endpoints.update_config_and_wait(served_models=endpoint_config.served_models, name=serving_endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed8f4210-792b-4236-be8d-5554a8f14e72",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Our endpoint is now deployed! You can search endpoint name on the [Serving Endpoint UI](#/mlflow/endpoints) and visualize its performance!\n",
    "\n",
    "Let's run a REST query to try it in Python. As you can see, we send the `test sentence` doc and it returns an embedding representing our document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aaf9a4ee-5f13-4cf5-a82d-51ab79121cdf",
     "showTitle": true,
     "title": "Let's try to send a query to our chatbot"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can track billing usage on your workspaces by following these steps:\n\n1. Log in to the Databricks account console as the account owner or an account admin.\n2. Click the Settings icon in the sidebar and click the Subscription & Billing tab.\n3. Click the Billing page.\n4. Here, you can view the detailed billing usage for your workspaces, including the amount of data processed, storage usage, and other associated costs.\n5. You can also download billable usage logs in CSV format to an AWS S3 storage bucket for further analysis.\n6. Additionally, you can use cloud provider cost management tools or third-party tools to monitor and attribute costs across your workspaces.\n7. It's also recommended to tag clusters and pools to track cost attribution and ensure cost control.\n\nBy following these steps, you can effectively track billing usage on your workspaces and optimize your costs\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = \"How can I track billing usage on my workspaces?\"\n",
    "\n",
    "answer = w.serving_endpoints.query(serving_endpoint_name, inputs=[{\"query\": question}])\n",
    "print(answer.predictions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aeafbb90-a890-4e32-a3a7-0efbb4cf01e9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Let's give it a try, using Gradio as UI!\n",
    "\n",
    "All you now have to do is deploy your chatbot UI. Here is a simple example using Gradio ([License](https://github.com/gradio-app/gradio/blob/main/LICENSE)). Explore the chatbot gradio [implementation](https://huggingface.co/spaces/databricks-demos/chatbot/blob/main/app.py).\n",
    "\n",
    "*Note: this UI is hosted and maintained by Databricks for demo purpose and don't use the model you just created. We'll soon show you how to do that with Lakehouse Apps!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d9303fd-b95b-4d31-afa9-0b84dca3da38",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<div style=\"margin: auto; width: 1000px\"><iframe src=\"https://databricks-demos-chatbot.hf.space\" frameborder=\"0\" width=\"1000\" height=\"950\" style=\"margin: auto\"></iframe></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_gradio_app(\"databricks-demos-chatbot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f97c155-6d11-42f9-b5e1-c379eefadf0d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Congratulations! You have deployed your first GenAI RAG model!\n",
    "\n",
    "You're now ready to deploy the same logic for your internal knowledge base leveraging Lakehouse AI.\n",
    "\n",
    "We've seen how the Lakehouse AI is uniquely positioned to help you solve your GenAI challenge:\n",
    "\n",
    "- Simplify Data Ingestion and preparation with Databricks Engineering Capabilities\n",
    "- Accelerate Vector Search  deployment with fully managed indexes\n",
    "- Leverage Databricks LLama 2 foundation model endpoint\n",
    "- Deploy realtime model endpoint to perform RAG and provide Q&A capabilities\n",
    "\n",
    "Lakehouse AI is uniquely positioned to accelerate your GenAI deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c955284-2e95-447e-916a-1e5b5c2bd5fa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Cleanup\n",
    "\n",
    "To free up resources, please delete uncomment and run the below cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6136e89e-19d2-468d-a44b-46401cfff3bc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# /!\\ THIS WILL DROP YOUR DEMO SCHEMA ENTIRELY /!\\ \n",
    "# cleanup_demo(catalog, db, serving_endpoint_name, f\"{catalog}.{db}.databricks_documentation_vs_index\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "02-Deploy-RAG-Chatbot-Model",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
