{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "87262cd5-43e6-4b53-8725-6bd0c19caa84",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### A cluster has been created for this demo\n",
    "To run this demo, just select the cluster `dbdemos-llm-rag-chatbot-ling_huang` from the dropdown menu ([open cluster configuration](https://dbc-57a6b8b4-32b6.cloud.databricks.com/#setting/clusters/1212-021800-o0bqrfse/configuration)). <br />\n",
    "*Note: If the cluster was deleted after 30 days, you can re-create it with `dbdemos.create_cluster('llm-rag-chatbot')` or re-install the demo: `dbdemos.install('llm-rag-chatbot')`*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "210172e9-f00d-48ec-927c-84ed2311d899",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 2/ Creating the chatbot with Retrieval Augmented Generation (RAG)\n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/chatbot-rag/llm-rag-self-managed-flow-2.png?raw=true\" style=\"float: right; margin-left: 10px\"  width=\"900px;\">\n",
    "\n",
    "Our Vector Search Index is now ready!\n",
    "\n",
    "Let's now create and deploy a new Model Serving Endpoint to perform RAG.\n",
    "\n",
    "The flow will be the following:\n",
    "\n",
    "- A user asks a question\n",
    "- The question is sent to our serverless Chatbot RAG endpoint\n",
    "- The endpoint compute the embeddings and searches for docs similar to the question, leveraging the Vector Search Index\n",
    "- The endpoint creates a prompt enriched with the doc\n",
    "- The prompt is sent to the Foundation Model Serving Endpoint\n",
    "- We display the output to our users!\n",
    "\n",
    "\n",
    "<!-- Collect usage data (view). Remove it to disable collection or disable tracker during installation. View README for more details.  -->\n",
    "<img width=\"1px\" src=\"https://ppxrzfxige.execute-api.us-west-2.amazonaws.com/v1/analytics?category=data-science&org_id=local&notebook=%2F02-Deploy-RAG-Chatbot-Model&demo_name=llm-rag-chatbot&event=VIEW&path=%2F_dbdemos%2Fdata-science%2Fllm-rag-chatbot%2F02-Deploy-RAG-Chatbot-Model&version=1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb26794f-b7dc-4c42-bc21-af91c089ca42",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "*Note: RAG performs document searches using Databricks Vector Search. In this notebook, we assume that the search index is ready for use. Make sure you run the previous [01-Data-Preparation-and-Index]($./01-Data-Preparation-and-Index [DO NOT EDIT]) notebook.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ab4d64b-6376-4788-884f-3da570118212",
     "showTitle": true,
     "title": "Install the required libraries"
    }
   },
   "outputs": [],
   "source": [
    "%pip install \"git+https://github.com/mlflow/mlflow.git@gateway-migration\" langchain==0.0.344 databricks-vectorsearch==0.22 databricks-sdk==0.12.0 mlflow[databricks]\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2c07138-9a4b-4de9-83c1-d07252d9cb1f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./_resources/00-init $reset_all_data=false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a40f4f61-8275-47c1-877f-9e3aede932cc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "  \n",
    "###  This demo requires a secret to work:\n",
    "Your Model Serving Endpoint needs a secret to authenticate against your Vector Search Index (see [Documentation](https://docs.databricks.com/en/security/secrets/secrets.html)).  <br/>\n",
    "- You'll need to setup the Databricks CLI on your laptop or using this cluster terminal: <br/>\n",
    "`pip install databricks-cli` <br/>\n",
    "- Configure the CLI. You'll need your workspace URL and a PAT token from your profile page<br>\n",
    "`databricks configure`\n",
    "- Create the dbdemos scope:<br/>\n",
    "`databricks secrets create-scope dbdemos`\n",
    "- Save your service principal secret. It will be used by the Model Endpoint to autenticate. If this is a demo/test, you can use one of your PAT token.<br>\n",
    "`databricks secrets put-secret dbdemos rag_sp_token`\n",
    "\n",
    "*Note: Make sure your service principal has access to the Vector Search index:*\n",
    "\n",
    "```\n",
    "spark.sql('GRANT USAGE ON CATALOG <catalog> TO `<YOUR_SP>`');\n",
    "spark.sql('GRANT USAGE ON DATABASE <catalog>.<db> TO `<YOUR_SP>`');\n",
    "from databricks.sdk import WorkspaceClient\n",
    "import databricks.sdk.service.catalog as c\n",
    "WorkspaceClient().grants.update(c.SecurableType.TABLE, <index_name>, \n",
    "                                changes=[c.PermissionsChange(add=[c.Privilege[\"SELECT\"]], principal=\"<YOUR_SP>\")])\n",
    "  ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "062712ff-b3d0-4e5d-ae12-e36801c646ac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sp_name = spark.sql('select current_user() as user').collect()[0]['user'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27d275b4-c749-49fe-9540-d3c490d26fe5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(sp_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e5ecfde-73e3-4dfc-aa06-294e3dd07604",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalog = \"ling_test_demo\"\n",
    "db = \"default\"\n",
    "index_name=\"ling_test_demo.default.databricks_documentation_vs_index\"\n",
    "\n",
    "# Make sure you replace sp_name with the SP owning the token in the secret. It has be the principal in the PAT token used in the model\n",
    "sp_name = spark.sql('select current_user() as user').collect()[0]['user'] #Set to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "120dd837-69c9-465e-a748-3a8216ed4971",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f'GRANT USAGE ON CATALOG {catalog} TO `{sp_name}`');\n",
    "spark.sql(f'GRANT USAGE ON DATABASE {catalog}.{db} TO `{sp_name}`');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ee41090-00b3-4d76-b75c-e0f991efd898",
     "showTitle": true,
     "title": "Give your SP read access to your Vector Search Index"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from databricks.sdk import WorkspaceClient\n",
    "import databricks.sdk.service.catalog as c\n",
    "WorkspaceClient().grants.update(c.SecurableType.TABLE, index_name, \n",
    "                                changes=[c.PermissionsChange(add=[c.Privilege[\"SELECT\"]], principal=sp_name)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d02f64c-9319-4e9f-afcb-c182ce6f53c4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Langchain retriever\n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/chatbot-rag/llm-rag-self-managed-model-1.png?raw=true\" style=\"float: right\" width=\"500px\">\n",
    "\n",
    "Let's start by building our Langchain retriever. \n",
    "\n",
    "It will be in charge of:\n",
    "\n",
    "* Creating the input question embeddings (with Databricks `bge-large-en`)\n",
    "* Calling the vector search index to find similar documents to augment the prompt with\n",
    "\n",
    "Databricks Langchain wrapper makes it easy to do in one step, handling all the underlying logic and API call for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e31ce318-9cfc-420b-b434-364f37773028",
     "showTitle": true,
     "title": "Setup authentication for our model"
    }
   },
   "outputs": [],
   "source": [
    "# url used to send the request to your model from the serverless endpoint\n",
    "host = \"https://\" + spark.conf.get(\"spark.databricks.workspaceUrl\")\n",
    "print(host)\n",
    "\n",
    "# os.environ['DATABRICKS_TOKEN'] = dbutils.secrets.get(\"ling_test_demo\", \"default\")\n",
    "os.environ['DATABRICKS_TOKEN'] = '-'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2134e7b-0945-48bb-bbaa-c3d5daaf5b58",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.vector_search.client import VectorSearchClient\n",
    "from langchain.vectorstores import DatabricksVectorSearch\n",
    "from langchain.embeddings import DatabricksEmbeddings\n",
    "\n",
    "# Test embedding Langchain model\n",
    "#NOTE: your question embedding model must match the one used in the chunk in the previous model \n",
    "embedding_model = DatabricksEmbeddings(endpoint=\"databricks-bge-large-en\")\n",
    "print(f\"Test embeddings: {embedding_model.embed_query('What is Apache Spark?')[:20]}...\")\n",
    "\n",
    "def get_retriever(persist_dir: str = None):\n",
    "    os.environ[\"DATABRICKS_HOST\"] = host\n",
    "    #Get the vector search index\n",
    "    vsc = VectorSearchClient(workspace_url=host, personal_access_token=os.environ[\"DATABRICKS_TOKEN\"])\n",
    "    vs_index = vsc.get_index(\n",
    "        endpoint_name=VECTOR_SEARCH_ENDPOINT_NAME,\n",
    "        index_name=index_name\n",
    "    )\n",
    "\n",
    "    # Create the retriever\n",
    "    vectorstore = DatabricksVectorSearch(\n",
    "        vs_index, text_column=\"content\", embedding=embedding_model\n",
    "    )\n",
    "    return vectorstore.as_retriever()\n",
    "\n",
    "\n",
    "# test our retriever\n",
    "vectorstore = get_retriever()\n",
    "similar_documents = vectorstore.get_relevant_documents(\"How do I track my Databricks Billing?\")\n",
    "print(f\"Relevant documents: {similar_documents[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "870f5d1d-aa82-4694-b94e-30c8c4ade5cd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Building Databricks Chat Model to query llama-2-70b-chat foundation model\n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/chatbot-rag/llm-rag-self-managed-model-3.png?raw=true\" style=\"float: right\" width=\"500px\">\n",
    "\n",
    "Our chatbot will be using llama2 foundation model to provide answer. \n",
    "\n",
    "While the model is available using the built-in [Foundation endpoint](/ml/endpoints) (using the `/serving-endpoints/databricks-llama-2-70b-chat/invocations` API), we can use Databricks Langchain Chat Model wrapper to easily build our chain.  \n",
    "\n",
    "Note: multipe type of endpoint or langchain models can be used:\n",
    "\n",
    "- Databricks Foundation models (what we'll use)\n",
    "- Your fined-tune model\n",
    "- An external model provider (such as Azure OpenAI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da4f18e3-7144-4598-a021-3999c2e000f9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Test Databricks Foundation LLM model\n",
    "from langchain.chat_models import ChatDatabricks\n",
    "chat_model = ChatDatabricks(endpoint=\"databricks-llama-2-70b-chat\", max_tokens = 200)\n",
    "print(f\"Test chat model: {chat_model.predict('What is Apache Spark')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "92fd843f-fa63-45c1-9091-051ce1c982ce",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Assembling the complete RAG Chain\n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/chatbot-rag/llm-rag-self-managed-model-2.png?raw=true\" style=\"float: right\" width=\"600px\">\n",
    "\n",
    "\n",
    "Let's now merge the retriever and the model in a single Langchain chain.\n",
    "\n",
    "We will use a custom langchain template for our assistant to give proper answer.\n",
    "\n",
    "Make sure you take some time to try different templates and adjust your assistant tone and personality for your requirement.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b802588-4f10-4be1-b0fe-f2b5e3cab900",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatDatabricks\n",
    "\n",
    "TEMPLATE = \"\"\"You are an assistant for Databricks users. You are answering python, coding, SQL, data engineering, spark, data science, DW and platform, API or infrastructure administration question related to Databricks. If the question is not related to one of these topics, kindly decline to answer. If you don't know the answer, just say that you don't know, don't try to make up an answer. Keep the answer as concise as possible.\n",
    "Use the following pieces of context to answer the question at the end:\n",
    "{context}\n",
    "Question: {question}\n",
    "Answer:\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(template=TEMPLATE, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=chat_model,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=get_retriever(),\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d7a84fe-0c08-43ff-aec5-1a70ee851fc1",
     "showTitle": true,
     "title": "Let's try our chatbot in the notebook directly:"
    }
   },
   "outputs": [],
   "source": [
    "# langchain.debug = True #uncomment to see the chain details and the full prompt being sent\n",
    "question = {\"query\": \"How can I track billing usage on my workspaces?\"}\n",
    "answer = chain.run(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e250c32-0d14-4688-bfc6-6b81dc397a1a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Saving our model to Unity Catalog registry\n",
    "\n",
    "Now that our model is ready, we can register it within our Unity Catalog schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6da16042-6781-4d49-b161-285a8c74499c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#dbdemos__delete_this_cell\n",
    "#force the experiment to the field demos one. Required to launch as a batch\n",
    "init_experiment_for_batch(\"chatbot-rag-llm\", \"simple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f3d131c-6b77-4607-a178-f19346d7d261",
     "showTitle": true,
     "title": "Register our chain to MLFlow"
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.models import infer_signature\n",
    "import mlflow\n",
    "\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "model_name = f\"{catalog}.{db}.dbdemos_chatbot_model\"\n",
    "\n",
    "with mlflow.start_run(run_name=\"dbdemos_chatbot_rag\") as run:\n",
    "    signature = infer_signature(question, answer)\n",
    "    model_info = mlflow.langchain.log_model(\n",
    "        chain,\n",
    "        loader_fn=get_retriever,  # Load the retriever with DATABRICKS_TOKEN env as secret (for authentication).\n",
    "        artifact_path=\"chain\",\n",
    "        registered_model_name=model_name,\n",
    "        pip_requirements=[\n",
    "            \"git+https://github.com/mlflow/mlflow.git@gateway-migration\",\n",
    "            \"langchain==\" + langchain.__version__,\n",
    "            \"databricks-vectorsearch\",\n",
    "        ],\n",
    "        input_example=question,\n",
    "        signature=signature\n",
    "    )\n",
    "    #------------------------\n",
    "    # TODO: temporary fix to add the wheel, we won't need this after we switch to using PyPI\n",
    "    import mlflow.models.utils\n",
    "    mlflow.models.utils.add_libraries_to_model(\n",
    "        f\"models:/{model_name}/{get_latest_model_version(model_name)}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f53ac80-acfb-45fe-b341-9f4b283def24",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Deploying our Chat Model as a Serverless Model Endpoint \n",
    "\n",
    "Our model is saved in Unity Catalog. The last step is to deploy it as a Model Serving.\n",
    "\n",
    "We'll then be able to sending requests from our assistant frontend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d889ace-922e-4de6-9c5e-e8d3d5c9d231",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create or update serving endpoint\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service.serving import EndpointCoreConfigInput, ServedModelInput\n",
    "\n",
    "serving_endpoint_name = f\"dbdemos_endpoint_{catalog}_{db}\"[:63]\n",
    "latest_model_version = get_latest_model_version(model_name)\n",
    "\n",
    "w = WorkspaceClient()\n",
    "endpoint_config = EndpointCoreConfigInput(\n",
    "    name=serving_endpoint_name,\n",
    "    served_models=[\n",
    "        ServedModelInput(\n",
    "            model_name=model_name,\n",
    "            model_version=latest_model_version,\n",
    "            workload_size=\"Small\",\n",
    "            scale_to_zero_enabled=True,\n",
    "            environment_vars={\n",
    "                \"DATABRICKS_TOKEN\": \"{{secrets/dbdemos/rag_sp_token}}\",  # <scope>/<secret> that contains an access token\n",
    "            }\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "existing_endpoint = next(\n",
    "    (e for e in w.serving_endpoints.list() if e.name == serving_endpoint_name), None\n",
    ")\n",
    "if existing_endpoint == None:\n",
    "    print(f\"Creating the endpoint {serving_endpoint_name}, this will take a few minutes to package and deploy the endpoint...\")\n",
    "    w.serving_endpoints.create_and_wait(name=serving_endpoint_name, config=endpoint_config)\n",
    "else:\n",
    "    print(f\"Updating the endpoint {serving_endpoint_name} to version {latest_model_version}, this will take a few minutes to package and deploy the endpoint...\")\n",
    "    w.serving_endpoints.update_config_and_wait(served_models=endpoint_config.served_models, name=serving_endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed8f4210-792b-4236-be8d-5554a8f14e72",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Our endpoint is now deployed! You can search endpoint name on the [Serving Endpoint UI](#/mlflow/endpoints) and visualize its performance!\n",
    "\n",
    "Let's run a REST query to try it in Python. As you can see, we send the `test sentence` doc and it returns an embedding representing our document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aaf9a4ee-5f13-4cf5-a82d-51ab79121cdf",
     "showTitle": true,
     "title": "Let's try to send a query to our chatbot"
    }
   },
   "outputs": [],
   "source": [
    "question = \"How can I track billing usage on my workspaces?\"\n",
    "\n",
    "answer = w.serving_endpoints.query(serving_endpoint_name, inputs=[{\"query\": question}])\n",
    "print(answer.predictions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aeafbb90-a890-4e32-a3a7-0efbb4cf01e9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Let's give it a try, using Gradio as UI!\n",
    "\n",
    "All you now have to do is deploy your chatbot UI. Here is a simple example using Gradio ([License](https://github.com/gradio-app/gradio/blob/main/LICENSE)). Explore the chatbot gradio [implementation](https://huggingface.co/spaces/databricks-demos/chatbot/blob/main/app.py).\n",
    "\n",
    "*Note: this UI is hosted and maintained by Databricks for demo purpose and don't use the model you just created. We'll soon show you how to do that with Lakehouse Apps!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d9303fd-b95b-4d31-afa9-0b84dca3da38",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display_gradio_app(\"databricks-demos-chatbot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f97c155-6d11-42f9-b5e1-c379eefadf0d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Congratulations! You have deployed your first GenAI RAG model!\n",
    "\n",
    "You're now ready to deploy the same logic for your internal knowledge base leveraging Lakehouse AI.\n",
    "\n",
    "We've seen how the Lakehouse AI is uniquely positioned to help you solve your GenAI challenge:\n",
    "\n",
    "- Simplify Data Ingestion and preparation with Databricks Engineering Capabilities\n",
    "- Accelerate Vector Search  deployment with fully managed indexes\n",
    "- Leverage Databricks LLama 2 foundation model endpoint\n",
    "- Deploy realtime model endpoint to perform RAG and provide Q&A capabilities\n",
    "\n",
    "Lakehouse AI is uniquely positioned to accelerate your GenAI deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c955284-2e95-447e-916a-1e5b5c2bd5fa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Cleanup\n",
    "\n",
    "To free up resources, please delete uncomment and run the below cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6136e89e-19d2-468d-a44b-46401cfff3bc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# /!\\ THIS WILL DROP YOUR DEMO SCHEMA ENTIRELY /!\\ \n",
    "# cleanup_demo(catalog, db, serving_endpoint_name, f\"{catalog}.{db}.databricks_documentation_vs_index\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "02-Deploy-RAG-Chatbot-Model",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
