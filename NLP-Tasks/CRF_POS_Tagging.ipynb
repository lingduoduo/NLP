{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lingduoduo/NLP/blob/master/CRF_POS_Tagging.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQWFO0PsoGOI"
      },
      "source": [
        "# So, it is time to learn to PoS Tag!\n",
        "\n",
        "In this notebook, I'll guide you through the steps of training some models to be further utilized in our NLP Tool to do PoS Tagging. Here we won't apply any state of the art algorithm, but we won't be far either!\n",
        "\n",
        "There are different techniques for POS Tagging:\n",
        "\n",
        "* Lexical Based Methods — Assigns the POS tag the most frequently occurring with a word in the training corpus.\n",
        "* Rule-Based Methods — Assigns POS tags based on rules. For example, we can have a rule that says, words ending with “ed” or “ing” must be assigned to a verb. Rule-Based Techniques can be used along with Lexical Based approaches to allow POS Tagging of words that are not present in the training corpus but are there in the testing data.\n",
        "* Probabilistic Methods — This method assigns the POS tags based on the probability of a particular tag sequence occurring. Conditional Random Fields (CRFs) and Hidden Markov Models (HMMs) are probabilistic approaches to assign a POS Tag.\n",
        "* Deep Learning Methods — Recurrent Neural Networks can also be used for POS tagging.\n",
        "\n",
        "\n",
        "## Getting the data (Corpus)\n",
        "\n",
        "Let us start by where we'll get our data (our **corpus**). There are many sources, but two are the most commonly used:\n",
        "* **Penn Treebank** subset from nltk (you can buy the entire Treebank, if you want, but you'll have to invest some $700~).\n",
        "* The **Universal Dependencies** Treebanks, available (as of February 2020) for 90 languages (in different quality and quantity levels).\n",
        "\n",
        "These contain the hard work of many **annotators**, which went through selected sets of sentences and annotated each one by hand, forming a corpus to be used as **supervised** input for our **machine learning algorithms**.\n",
        "\n",
        "The following two cells will show how to import the corpus from each of these two sources."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4B5gdrh7Z_Ic",
        "outputId": "e931532f-6ca7-49b6-9121-3d85c1a481ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/treebank.zip.\n"
          ]
        }
      ],
      "source": [
        "#This cell loads the Penn Treebank corpus from nltk into a list variable named penn_treebank.\n",
        "\n",
        "#No need to install nltk in google colab since it is preloaded in the environments.\n",
        "#!pip install nltk\n",
        "import nltk\n",
        "\n",
        "#Ensure that the treebank corpus is downloaded\n",
        "nltk.download('treebank')\n",
        "\n",
        "#Load the treebank corpus class\n",
        "from nltk.corpus import treebank\n",
        "\n",
        "#Now we iterate over all samples from the corpus (the fileids - that are equivalent to sentences)\n",
        "#and retrieve the word and the pre-labeled PoS tag. This will be added as a list of tuples with\n",
        "#a list of words and a list of their respective PoS tags (in the same order).\n",
        "penn_treebank = []\n",
        "for fileid in treebank.fileids():\n",
        "  tokens = []\n",
        "  tags = []\n",
        "  for word, tag in treebank.tagged_words(fileid):\n",
        "    tokens.append(word)\n",
        "    tags.append(tag)\n",
        "  penn_treebank.append((tokens, tags))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4WwZYkNr1bPN",
        "outputId": "123d8717-6e89-4558-ab48-ba75ad317ccb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting conllu\n",
            "  Downloading conllu-5.0.1-py3-none-any.whl (16 kB)\n",
            "Installing collected packages: conllu\n",
            "Successfully installed conllu-5.0.1\n",
            "--2024-07-19 03:31:56--  https://lindat.mff.cuni.cz/repository/xmlui/bitstream/handle/11234/1-3105/ud-treebanks-v2.5.tgz\n",
            "Resolving lindat.mff.cuni.cz (lindat.mff.cuni.cz)... 195.113.20.140\n",
            "Connecting to lindat.mff.cuni.cz (lindat.mff.cuni.cz)|195.113.20.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 355216681 (339M) [application/x-gzip]\n",
            "Saving to: ‘ud-treebanks-v2.5.tgz’\n",
            "\n",
            "ud-treebanks-v2.5.t 100%[===================>] 338.76M  18.6MB/s    in 19s     \n",
            "\n",
            "2024-07-19 03:32:16 (18.1 MB/s) - ‘ud-treebanks-v2.5.tgz’ saved [355216681/355216681]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#This cell loads the Universal Dependecies Treekbank corpus. It'll download all the packages, but we'll only use the GUM\n",
        "#english package. We'll also install the conllu package, that was developed to parse data in the conLLu format, a\n",
        "#format common of linguistic annotated files. We'll also have a list variable, but now named ud_treebank.\n",
        "\n",
        "#Install conllu package, download the UD Treebanks corpus and unpack it.\n",
        "!pip install conllu\n",
        "!wget https://lindat.mff.cuni.cz/repository/xmlui/bitstream/handle/11234/1-3105/ud-treebanks-v2.5.tgz\n",
        "!tar zxf ud-treebanks-v2.5.tgz\n",
        "\n",
        "#The imports needed to open and parse (interpret) the conllu file. At the end we'll have a list of dicts.\n",
        "from io import open\n",
        "from conllu import parse_incr\n",
        "\n",
        "#Open the file and load the sentences to a list.\n",
        "data_file = open(\"ud-treebanks-v2.5/UD_English-GUM/en_gum-ud-train.conllu\", \"r\", encoding=\"utf-8\")\n",
        "ud_files = []\n",
        "for tokenlist in parse_incr(data_file):\n",
        "    ud_files.append(tokenlist)\n",
        "\n",
        "#Now we iterate over all samples from the corpus and retrieve the word and the pre-labeled PoS tag (upostag). This will\n",
        "#be added as a list of tuples with a list of words and a list of their respective PoS tags (in the same order).\n",
        "ud_treebank = []\n",
        "for sentence in ud_files:\n",
        "  tokens = []\n",
        "  tags = []\n",
        "  for token in sentence:\n",
        "    tokens.append(token['form'])\n",
        "    tags.append(token['upostag'])\n",
        "  ud_treebank.append((tokens, tags))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzzGnG10Ulv2"
      },
      "source": [
        "**Word of Caution!**\n",
        "\n",
        "Penn Treebank and UD Treebanks use *distinct tagsets*.\n",
        "\n",
        "We won't be able to interchange them unless we make a converter - also, we'll only be able to do so from Penn->UD, because Penn Treebank has tags more detailed than UD, and we won't be able to retrieve these details from the tags without a third function and a lot of effort.\n",
        "\n",
        "We'll only do that later, in our code.\n",
        "\n",
        "Let us continue with the explanation of the Tagger."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJgfnjRrC2JP"
      },
      "source": [
        "### Extracting Features form Words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfD5ujGijuUF"
      },
      "source": [
        "Next, we have to create a function that is able to extract features from our words. These features will be used to predict the PoS.\n",
        "\n",
        "For that,  for each word, we'll pass the sentence and word index, and we'll provide a dict with the features.\n",
        "* Is the first letter capitalised.\n",
        "* Is it the first word in the sentence?\n",
        "* Is it the last word?\n",
        "* What is the prefix of the word?\n",
        "* What is the suffix of the word?\n",
        "* Is the complete word captilised?\n",
        "* What is the previous word?\n",
        "* What is the next word?\n",
        "* Is it numeric?\n",
        "* Is it alphanumeric?\n",
        "* Is there an hyphen in the word?\n",
        "\n",
        "To explain about the feature set (can be changed, if you want), it is composed by:\n",
        "* Word: the word itself. Some words are always one PoS, others not.\n",
        "* is_first, is_last: check if it is the first or last in the sentence.\n",
        "* is_capitalized: first letter is caps? Maybe it is a proper noun...\n",
        "* is_all_caps or is_all_lower: checks for acronyms (or common words).\n",
        "* prefixes/suffixes: check word initialization/termination\n",
        "* prev_word/next_word: checks the preceding and succeding word.\n",
        "* has-hyphen: words with '-' may be adjectives.\n",
        "* is_numeric: for numbers.\n",
        "* capitals_inside: weird cases. Maybe nouns.\n",
        "\n",
        "If you're wondering, yes, this encoding WILL need a lot of memory for training (if you're not using categorical variables).\n",
        "\n",
        "And we'll have to replicate this in our main code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-IyIaTSwoo-V"
      },
      "outputs": [],
      "source": [
        "#Regex module for checking alphanumeric values.\n",
        "import re\n",
        "\n",
        "def extract_features(sentence, index):\n",
        "  return {\n",
        "      'word':sentence[index],\n",
        "      'is_first':index==0,\n",
        "      'is_last':index ==len(sentence)-1,\n",
        "      'is_capitalized':sentence[index][0].upper() == sentence[index][0],\n",
        "      'is_all_caps': sentence[index].upper() == sentence[index],\n",
        "      'is_all_lower': sentence[index].lower() == sentence[index],\n",
        "      'is_alphanumeric': int(bool((re.match('^(?=.*[0-9]$)(?=.*[a-zA-Z])',sentence[index])))),\n",
        "      'prefix-1':sentence[index][0],\n",
        "      'prefix-2':sentence[index][:2],\n",
        "      'prefix-3':sentence[index][:3],\n",
        "      'prefix-3':sentence[index][:4],\n",
        "      'suffix-1':sentence[index][-1],\n",
        "      'suffix-2':sentence[index][-2:],\n",
        "      'suffix-3':sentence[index][-3:],\n",
        "      'suffix-3':sentence[index][-4:],\n",
        "      'prev_word':'' if index == 0 else sentence[index-1],\n",
        "      'next_word':'' if index < len(sentence) else sentence[index+1],\n",
        "      'has_hyphen': '-' in sentence[index],\n",
        "      'is_numeric': sentence[index].isdigit(),\n",
        "      'capitals_inside': sentence[index][1:].lower() != sentence[index][1:]\n",
        "  }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYS5r1_m6Yr9"
      },
      "source": [
        "We now prepare the dataset for use in Machine Learning algorithms.\n",
        "\n",
        "There are two steps (three, if we're doing deep learning, but that's for later) to it:\n",
        "* Defining a function to transform the corpus to a more datsetish format.\n",
        "* Then, divide the encoded data into training and testing sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "1hiniE_wzPOC"
      },
      "outputs": [],
      "source": [
        "#Ater defining the extract_features, we define a simple function to transform our data in a more 'datasetish' format.\n",
        "#This function returns the data as two lists, one of Dicts of features and the other with the labels.\n",
        "def transform_to_dataset(tagged_sentences):\n",
        "  X, y = [], []\n",
        "  for sentence, tags in tagged_sentences:\n",
        "    sent_word_features, sent_tags = [],[]\n",
        "    for index in range(len(sentence)):\n",
        "        sent_word_features.append(extract_features(sentence, index)),\n",
        "        sent_tags.append(tags[index])\n",
        "    X.append(sent_word_features)\n",
        "    y.append(sent_tags)\n",
        "  return X, y\n",
        "\n",
        "#We divide the set BEFORE encoding. Why? To have full sentences in training/testing sets. When we encode, we do not encode\n",
        "#a sentence, but its words instead.\n",
        "\n",
        "#First, for the Penn treebank.\n",
        "penn_train_size = int(0.8*len(penn_treebank))\n",
        "penn_training = penn_treebank[:penn_train_size]\n",
        "penn_testing = penn_treebank[penn_train_size:]\n",
        "X_penn_train, y_penn_train = transform_to_dataset(penn_training)\n",
        "X_penn_test, y_penn_test = transform_to_dataset(penn_testing)\n",
        "\n",
        "#Then, for UD Treebank.\n",
        "ud_train_size = int(0.8*len(ud_treebank))\n",
        "ud_training = ud_treebank[:ud_train_size]\n",
        "ud_testing = ud_treebank[ud_train_size:]\n",
        "X_ud_train, y_ud_train = transform_to_dataset(ud_training)\n",
        "X_ud_test, y_ud_test = transform_to_dataset(ud_testing)\n",
        "\n",
        "#Third step, vectorize datasets. For that we use sklearn DictVectorizer\n",
        "#WARNING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxqHLWQS3PTY",
        "outputId": "578feb36-5fe9-47bb-c569-91ddd9bc7ba0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "159"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "len(X_penn_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DimhLK6235bF",
        "outputId": "be176d78-89d4-41a6-8d47-ad32a69a1451"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "159"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "len(y_penn_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AjnnJLV53TM1",
        "outputId": "27def2f5-b14b-454a-8b2d-a0c7b159d565"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'word': 'Pierre',\n",
              "  'is_first': True,\n",
              "  'is_last': False,\n",
              "  'is_capitalized': True,\n",
              "  'is_all_caps': False,\n",
              "  'is_all_lower': False,\n",
              "  'is_alphanumeric': 0,\n",
              "  'prefix-1': 'P',\n",
              "  'prefix-2': 'Pi',\n",
              "  'prefix-3': 'Pier',\n",
              "  'suffix-1': 'e',\n",
              "  'suffix-2': 're',\n",
              "  'suffix-3': 'erre',\n",
              "  'prev_word': '',\n",
              "  'next_word': '',\n",
              "  'has_hyphen': False,\n",
              "  'is_numeric': False,\n",
              "  'capitals_inside': False},\n",
              " {'word': 'Vinken',\n",
              "  'is_first': False,\n",
              "  'is_last': False,\n",
              "  'is_capitalized': True,\n",
              "  'is_all_caps': False,\n",
              "  'is_all_lower': False,\n",
              "  'is_alphanumeric': 0,\n",
              "  'prefix-1': 'V',\n",
              "  'prefix-2': 'Vi',\n",
              "  'prefix-3': 'Vink',\n",
              "  'suffix-1': 'n',\n",
              "  'suffix-2': 'en',\n",
              "  'suffix-3': 'nken',\n",
              "  'prev_word': 'Pierre',\n",
              "  'next_word': '',\n",
              "  'has_hyphen': False,\n",
              "  'is_numeric': False,\n",
              "  'capitals_inside': False},\n",
              " {'word': ',',\n",
              "  'is_first': False,\n",
              "  'is_last': False,\n",
              "  'is_capitalized': True,\n",
              "  'is_all_caps': True,\n",
              "  'is_all_lower': True,\n",
              "  'is_alphanumeric': 0,\n",
              "  'prefix-1': ',',\n",
              "  'prefix-2': ',',\n",
              "  'prefix-3': ',',\n",
              "  'suffix-1': ',',\n",
              "  'suffix-2': ',',\n",
              "  'suffix-3': ',',\n",
              "  'prev_word': 'Vinken',\n",
              "  'next_word': '',\n",
              "  'has_hyphen': False,\n",
              "  'is_numeric': False,\n",
              "  'capitals_inside': False},\n",
              " {'word': '61',\n",
              "  'is_first': False,\n",
              "  'is_last': False,\n",
              "  'is_capitalized': True,\n",
              "  'is_all_caps': True,\n",
              "  'is_all_lower': True,\n",
              "  'is_alphanumeric': 0,\n",
              "  'prefix-1': '6',\n",
              "  'prefix-2': '61',\n",
              "  'prefix-3': '61',\n",
              "  'suffix-1': '1',\n",
              "  'suffix-2': '61',\n",
              "  'suffix-3': '61',\n",
              "  'prev_word': ',',\n",
              "  'next_word': '',\n",
              "  'has_hyphen': False,\n",
              "  'is_numeric': True,\n",
              "  'capitals_inside': False},\n",
              " {'word': 'years',\n",
              "  'is_first': False,\n",
              "  'is_last': False,\n",
              "  'is_capitalized': False,\n",
              "  'is_all_caps': False,\n",
              "  'is_all_lower': True,\n",
              "  'is_alphanumeric': 0,\n",
              "  'prefix-1': 'y',\n",
              "  'prefix-2': 'ye',\n",
              "  'prefix-3': 'year',\n",
              "  'suffix-1': 's',\n",
              "  'suffix-2': 'rs',\n",
              "  'suffix-3': 'ears',\n",
              "  'prev_word': '61',\n",
              "  'next_word': '',\n",
              "  'has_hyphen': False,\n",
              "  'is_numeric': False,\n",
              "  'capitals_inside': False},\n",
              " {'word': 'old',\n",
              "  'is_first': False,\n",
              "  'is_last': False,\n",
              "  'is_capitalized': False,\n",
              "  'is_all_caps': False,\n",
              "  'is_all_lower': True,\n",
              "  'is_alphanumeric': 0,\n",
              "  'prefix-1': 'o',\n",
              "  'prefix-2': 'ol',\n",
              "  'prefix-3': 'old',\n",
              "  'suffix-1': 'd',\n",
              "  'suffix-2': 'ld',\n",
              "  'suffix-3': 'old',\n",
              "  'prev_word': 'years',\n",
              "  'next_word': '',\n",
              "  'has_hyphen': False,\n",
              "  'is_numeric': False,\n",
              "  'capitals_inside': False},\n",
              " {'word': ',',\n",
              "  'is_first': False,\n",
              "  'is_last': False,\n",
              "  'is_capitalized': True,\n",
              "  'is_all_caps': True,\n",
              "  'is_all_lower': True,\n",
              "  'is_alphanumeric': 0,\n",
              "  'prefix-1': ',',\n",
              "  'prefix-2': ',',\n",
              "  'prefix-3': ',',\n",
              "  'suffix-1': ',',\n",
              "  'suffix-2': ',',\n",
              "  'suffix-3': ',',\n",
              "  'prev_word': 'old',\n",
              "  'next_word': '',\n",
              "  'has_hyphen': False,\n",
              "  'is_numeric': False,\n",
              "  'capitals_inside': False},\n",
              " {'word': 'will',\n",
              "  'is_first': False,\n",
              "  'is_last': False,\n",
              "  'is_capitalized': False,\n",
              "  'is_all_caps': False,\n",
              "  'is_all_lower': True,\n",
              "  'is_alphanumeric': 0,\n",
              "  'prefix-1': 'w',\n",
              "  'prefix-2': 'wi',\n",
              "  'prefix-3': 'will',\n",
              "  'suffix-1': 'l',\n",
              "  'suffix-2': 'll',\n",
              "  'suffix-3': 'will',\n",
              "  'prev_word': ',',\n",
              "  'next_word': '',\n",
              "  'has_hyphen': False,\n",
              "  'is_numeric': False,\n",
              "  'capitals_inside': False},\n",
              " {'word': 'join',\n",
              "  'is_first': False,\n",
              "  'is_last': False,\n",
              "  'is_capitalized': False,\n",
              "  'is_all_caps': False,\n",
              "  'is_all_lower': True,\n",
              "  'is_alphanumeric': 0,\n",
              "  'prefix-1': 'j',\n",
              "  'prefix-2': 'jo',\n",
              "  'prefix-3': 'join',\n",
              "  'suffix-1': 'n',\n",
              "  'suffix-2': 'in',\n",
              "  'suffix-3': 'join',\n",
              "  'prev_word': 'will',\n",
              "  'next_word': '',\n",
              "  'has_hyphen': False,\n",
              "  'is_numeric': False,\n",
              "  'capitals_inside': False},\n",
              " {'word': 'the',\n",
              "  'is_first': False,\n",
              "  'is_last': False,\n",
              "  'is_capitalized': False,\n",
              "  'is_all_caps': False,\n",
              "  'is_all_lower': True,\n",
              "  'is_alphanumeric': 0,\n",
              "  'prefix-1': 't',\n",
              "  'prefix-2': 'th',\n",
              "  'prefix-3': 'the',\n",
              "  'suffix-1': 'e',\n",
              "  'suffix-2': 'he',\n",
              "  'suffix-3': 'the',\n",
              "  'prev_word': 'join',\n",
              "  'next_word': '',\n",
              "  'has_hyphen': False,\n",
              "  'is_numeric': False,\n",
              "  'capitals_inside': False},\n",
              " {'word': 'board',\n",
              "  'is_first': False,\n",
              "  'is_last': False,\n",
              "  'is_capitalized': False,\n",
              "  'is_all_caps': False,\n",
              "  'is_all_lower': True,\n",
              "  'is_alphanumeric': 0,\n",
              "  'prefix-1': 'b',\n",
              "  'prefix-2': 'bo',\n",
              "  'prefix-3': 'boar',\n",
              "  'suffix-1': 'd',\n",
              "  'suffix-2': 'rd',\n",
              "  'suffix-3': 'oard',\n",
              "  'prev_word': 'the',\n",
              "  'next_word': '',\n",
              "  'has_hyphen': False,\n",
              "  'is_numeric': False,\n",
              "  'capitals_inside': False},\n",
              " {'word': 'as',\n",
              "  'is_first': False,\n",
              "  'is_last': False,\n",
              "  'is_capitalized': False,\n",
              "  'is_all_caps': False,\n",
              "  'is_all_lower': True,\n",
              "  'is_alphanumeric': 0,\n",
              "  'prefix-1': 'a',\n",
              "  'prefix-2': 'as',\n",
              "  'prefix-3': 'as',\n",
              "  'suffix-1': 's',\n",
              "  'suffix-2': 'as',\n",
              "  'suffix-3': 'as',\n",
              "  'prev_word': 'board',\n",
              "  'next_word': '',\n",
              "  'has_hyphen': False,\n",
              "  'is_numeric': False,\n",
              "  'capitals_inside': False},\n",
              " {'word': 'a',\n",
              "  'is_first': False,\n",
              "  'is_last': False,\n",
              "  'is_capitalized': False,\n",
              "  'is_all_caps': False,\n",
              "  'is_all_lower': True,\n",
              "  'is_alphanumeric': 0,\n",
              "  'prefix-1': 'a',\n",
              "  'prefix-2': 'a',\n",
              "  'prefix-3': 'a',\n",
              "  'suffix-1': 'a',\n",
              "  'suffix-2': 'a',\n",
              "  'suffix-3': 'a',\n",
              "  'prev_word': 'as',\n",
              "  'next_word': '',\n",
              "  'has_hyphen': False,\n",
              "  'is_numeric': False,\n",
              "  'capitals_inside': False},\n",
              " {'word': 'nonexecutive',\n",
              "  'is_first': False,\n",
              "  'is_last': False,\n",
              "  'is_capitalized': False,\n",
              "  'is_all_caps': False,\n",
              "  'is_all_lower': True,\n",
              "  'is_alphanumeric': 0,\n",
              "  'prefix-1': 'n',\n",
              "  'prefix-2': 'no',\n",
              "  'prefix-3': 'none',\n",
              "  'suffix-1': 'e',\n",
              "  'suffix-2': 've',\n",
              "  'suffix-3': 'tive',\n",
              "  'prev_word': 'a',\n",
              "  'next_word': '',\n",
              "  'has_hyphen': False,\n",
              "  'is_numeric': False,\n",
              "  'capitals_inside': False},\n",
              " {'word': 'director',\n",
              "  'is_first': False,\n",
              "  'is_last': False,\n",
              "  'is_capitalized': False,\n",
              "  'is_all_caps': False,\n",
              "  'is_all_lower': True,\n",
              "  'is_alphanumeric': 0,\n",
              "  'prefix-1': 'd',\n",
              "  'prefix-2': 'di',\n",
              "  'prefix-3': 'dire',\n",
              "  'suffix-1': 'r',\n",
              "  'suffix-2': 'or',\n",
              "  'suffix-3': 'ctor',\n",
              "  'prev_word': 'nonexecutive',\n",
              "  'next_word': '',\n",
              "  'has_hyphen': False,\n",
              "  'is_numeric': False,\n",
              "  'capitals_inside': False},\n",
              " {'word': 'Nov.',\n",
              "  'is_first': False,\n",
              "  'is_last': False,\n",
              "  'is_capitalized': True,\n",
              "  'is_all_caps': False,\n",
              "  'is_all_lower': False,\n",
              "  'is_alphanumeric': 0,\n",
              "  'prefix-1': 'N',\n",
              "  'prefix-2': 'No',\n",
              "  'prefix-3': 'Nov.',\n",
              "  'suffix-1': '.',\n",
              "  'suffix-2': 'v.',\n",
              "  'suffix-3': 'Nov.',\n",
              "  'prev_word': 'director',\n",
              "  'next_word': '',\n",
              "  'has_hyphen': False,\n",
              "  'is_numeric': False,\n",
              "  'capitals_inside': False},\n",
              " {'word': '29',\n",
              "  'is_first': False,\n",
              "  'is_last': False,\n",
              "  'is_capitalized': True,\n",
              "  'is_all_caps': True,\n",
              "  'is_all_lower': True,\n",
              "  'is_alphanumeric': 0,\n",
              "  'prefix-1': '2',\n",
              "  'prefix-2': '29',\n",
              "  'prefix-3': '29',\n",
              "  'suffix-1': '9',\n",
              "  'suffix-2': '29',\n",
              "  'suffix-3': '29',\n",
              "  'prev_word': 'Nov.',\n",
              "  'next_word': '',\n",
              "  'has_hyphen': False,\n",
              "  'is_numeric': True,\n",
              "  'capitals_inside': False},\n",
              " {'word': '.',\n",
              "  'is_first': False,\n",
              "  'is_last': False,\n",
              "  'is_capitalized': True,\n",
              "  'is_all_caps': True,\n",
              "  'is_all_lower': True,\n",
              "  'is_alphanumeric': 0,\n",
              "  'prefix-1': '.',\n",
              "  'prefix-2': '.',\n",
              "  'prefix-3': '.',\n",
              "  'suffix-1': '.',\n",
              "  'suffix-2': '.',\n",
              "  'suffix-3': '.',\n",
              "  'prev_word': '29',\n",
              "  'next_word': '',\n",
              "  'has_hyphen': False,\n",
              "  'is_numeric': False,\n",
              "  'capitals_inside': False},\n",
              " {'word': 'Mr.',\n",
              "  'is_first': False,\n",
              "  'is_last': False,\n",
              "  'is_capitalized': True,\n",
              "  'is_all_caps': False,\n",
              "  'is_all_lower': False,\n",
              "  'is_alphanumeric': 0,\n",
              "  'prefix-1': 'M',\n",
              "  'prefix-2': 'Mr',\n",
              "  'prefix-3': 'Mr.',\n",
              "  'suffix-1': '.',\n",
              "  'suffix-2': 'r.',\n",
              "  'suffix-3': 'Mr.',\n",
              "  'prev_word': '.',\n",
              "  'next_word': '',\n",
              "  'has_hyphen': False,\n",
              "  'is_numeric': False,\n",
              "  'capitals_inside': False},\n",
              " {'word': 'Vinken',\n",
              "  'is_first': False,\n",
              "  'is_last': False,\n",
              "  'is_capitalized': True,\n",
              "  'is_all_caps': False,\n",
              "  'is_all_lower': False,\n",
              "  'is_alphanumeric': 0,\n",
              "  'prefix-1': 'V',\n",
              "  'prefix-2': 'Vi',\n",
              "  'prefix-3': 'Vink',\n",
              "  'suffix-1': 'n',\n",
              "  'suffix-2': 'en',\n",
              "  'suffix-3': 'nken',\n",
              "  'prev_word': 'Mr.',\n",
              "  'next_word': '',\n",
              "  'has_hyphen': False,\n",
              "  'is_numeric': False,\n",
              "  'capitals_inside': False},\n",
              " {'word': 'is',\n",
              "  'is_first': False,\n",
              "  'is_last': False,\n",
              "  'is_capitalized': False,\n",
              "  'is_all_caps': False,\n",
              "  'is_all_lower': True,\n",
              "  'is_alphanumeric': 0,\n",
              "  'prefix-1': 'i',\n",
              "  'prefix-2': 'is',\n",
              "  'prefix-3': 'is',\n",
              "  'suffix-1': 's',\n",
              "  'suffix-2': 'is',\n",
              "  'suffix-3': 'is',\n",
              "  'prev_word': 'Vinken',\n",
              "  'next_word': '',\n",
              "  'has_hyphen': False,\n",
              "  'is_numeric': False,\n",
              "  'capitals_inside': False},\n",
              " {'word': 'chairman',\n",
              "  'is_first': False,\n",
              "  'is_last': False,\n",
              "  'is_capitalized': False,\n",
              "  'is_all_caps': False,\n",
              "  'is_all_lower': True,\n",
              "  'is_alphanumeric': 0,\n",
              "  'prefix-1': 'c',\n",
              "  'prefix-2': 'ch',\n",
              "  'prefix-3': 'chai',\n",
              "  'suffix-1': 'n',\n",
              "  'suffix-2': 'an',\n",
              "  'suffix-3': 'rman',\n",
              "  'prev_word': 'is',\n",
              "  'next_word': '',\n",
              "  'has_hyphen': False,\n",
              "  'is_numeric': False,\n",
              "  'capitals_inside': False},\n",
              " {'word': 'of',\n",
              "  'is_first': False,\n",
              "  'is_last': False,\n",
              "  'is_capitalized': False,\n",
              "  'is_all_caps': False,\n",
              "  'is_all_lower': True,\n",
              "  'is_alphanumeric': 0,\n",
              "  'prefix-1': 'o',\n",
              "  'prefix-2': 'of',\n",
              "  'prefix-3': 'of',\n",
              "  'suffix-1': 'f',\n",
              "  'suffix-2': 'of',\n",
              "  'suffix-3': 'of',\n",
              "  'prev_word': 'chairman',\n",
              "  'next_word': '',\n",
              "  'has_hyphen': False,\n",
              "  'is_numeric': False,\n",
              "  'capitals_inside': False},\n",
              " {'word': 'Elsevier',\n",
              "  'is_first': False,\n",
              "  'is_last': False,\n",
              "  'is_capitalized': True,\n",
              "  'is_all_caps': False,\n",
              "  'is_all_lower': False,\n",
              "  'is_alphanumeric': 0,\n",
              "  'prefix-1': 'E',\n",
              "  'prefix-2': 'El',\n",
              "  'prefix-3': 'Else',\n",
              "  'suffix-1': 'r',\n",
              "  'suffix-2': 'er',\n",
              "  'suffix-3': 'vier',\n",
              "  'prev_word': 'of',\n",
              "  'next_word': '',\n",
              "  'has_hyphen': False,\n",
              "  'is_numeric': False,\n",
              "  'capitals_inside': False},\n",
              " {'word': 'N.V.',\n",
              "  'is_first': False,\n",
              "  'is_last': False,\n",
              "  'is_capitalized': True,\n",
              "  'is_all_caps': True,\n",
              "  'is_all_lower': False,\n",
              "  'is_alphanumeric': 0,\n",
              "  'prefix-1': 'N',\n",
              "  'prefix-2': 'N.',\n",
              "  'prefix-3': 'N.V.',\n",
              "  'suffix-1': '.',\n",
              "  'suffix-2': 'V.',\n",
              "  'suffix-3': 'N.V.',\n",
              "  'prev_word': 'Elsevier',\n",
              "  'next_word': '',\n",
              "  'has_hyphen': False,\n",
              "  'is_numeric': False,\n",
              "  'capitals_inside': True},\n",
              " {'word': ',',\n",
              "  'is_first': False,\n",
              "  'is_last': False,\n",
              "  'is_capitalized': True,\n",
              "  'is_all_caps': True,\n",
              "  'is_all_lower': True,\n",
              "  'is_alphanumeric': 0,\n",
              "  'prefix-1': ',',\n",
              "  'prefix-2': ',',\n",
              "  'prefix-3': ',',\n",
              "  'suffix-1': ',',\n",
              "  'suffix-2': ',',\n",
              "  'suffix-3': ',',\n",
              "  'prev_word': 'N.V.',\n",
              "  'next_word': '',\n",
              "  'has_hyphen': False,\n",
              "  'is_numeric': False,\n",
              "  'capitals_inside': False},\n",
              " {'word': 'the',\n",
              "  'is_first': False,\n",
              "  'is_last': False,\n",
              "  'is_capitalized': False,\n",
              "  'is_all_caps': False,\n",
              "  'is_all_lower': True,\n",
              "  'is_alphanumeric': 0,\n",
              "  'prefix-1': 't',\n",
              "  'prefix-2': 'th',\n",
              "  'prefix-3': 'the',\n",
              "  'suffix-1': 'e',\n",
              "  'suffix-2': 'he',\n",
              "  'suffix-3': 'the',\n",
              "  'prev_word': ',',\n",
              "  'next_word': '',\n",
              "  'has_hyphen': False,\n",
              "  'is_numeric': False,\n",
              "  'capitals_inside': False},\n",
              " {'word': 'Dutch',\n",
              "  'is_first': False,\n",
              "  'is_last': False,\n",
              "  'is_capitalized': True,\n",
              "  'is_all_caps': False,\n",
              "  'is_all_lower': False,\n",
              "  'is_alphanumeric': 0,\n",
              "  'prefix-1': 'D',\n",
              "  'prefix-2': 'Du',\n",
              "  'prefix-3': 'Dutc',\n",
              "  'suffix-1': 'h',\n",
              "  'suffix-2': 'ch',\n",
              "  'suffix-3': 'utch',\n",
              "  'prev_word': 'the',\n",
              "  'next_word': '',\n",
              "  'has_hyphen': False,\n",
              "  'is_numeric': False,\n",
              "  'capitals_inside': False},\n",
              " {'word': 'publishing',\n",
              "  'is_first': False,\n",
              "  'is_last': False,\n",
              "  'is_capitalized': False,\n",
              "  'is_all_caps': False,\n",
              "  'is_all_lower': True,\n",
              "  'is_alphanumeric': 0,\n",
              "  'prefix-1': 'p',\n",
              "  'prefix-2': 'pu',\n",
              "  'prefix-3': 'publ',\n",
              "  'suffix-1': 'g',\n",
              "  'suffix-2': 'ng',\n",
              "  'suffix-3': 'hing',\n",
              "  'prev_word': 'Dutch',\n",
              "  'next_word': '',\n",
              "  'has_hyphen': False,\n",
              "  'is_numeric': False,\n",
              "  'capitals_inside': False},\n",
              " {'word': 'group',\n",
              "  'is_first': False,\n",
              "  'is_last': False,\n",
              "  'is_capitalized': False,\n",
              "  'is_all_caps': False,\n",
              "  'is_all_lower': True,\n",
              "  'is_alphanumeric': 0,\n",
              "  'prefix-1': 'g',\n",
              "  'prefix-2': 'gr',\n",
              "  'prefix-3': 'grou',\n",
              "  'suffix-1': 'p',\n",
              "  'suffix-2': 'up',\n",
              "  'suffix-3': 'roup',\n",
              "  'prev_word': 'publishing',\n",
              "  'next_word': '',\n",
              "  'has_hyphen': False,\n",
              "  'is_numeric': False,\n",
              "  'capitals_inside': False},\n",
              " {'word': '.',\n",
              "  'is_first': False,\n",
              "  'is_last': True,\n",
              "  'is_capitalized': True,\n",
              "  'is_all_caps': True,\n",
              "  'is_all_lower': True,\n",
              "  'is_alphanumeric': 0,\n",
              "  'prefix-1': '.',\n",
              "  'prefix-2': '.',\n",
              "  'prefix-3': '.',\n",
              "  'suffix-1': '.',\n",
              "  'suffix-2': '.',\n",
              "  'suffix-3': '.',\n",
              "  'prev_word': 'group',\n",
              "  'next_word': '',\n",
              "  'has_hyphen': False,\n",
              "  'is_numeric': False,\n",
              "  'capitals_inside': False}]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "X_penn_train[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "470S2WqX3fLP",
        "outputId": "27bd9c92-3aa3-480b-8f74-ffeae4945fb1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "40"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "len(X_penn_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQWIEMVA34k_",
        "outputId": "187fce74-f0cb-43bb-df54-e9507dc71655"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "40"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "len(y_penn_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHUJ3T__3ztZ",
        "outputId": "eeb33690-637c-42d8-8d0c-c3b55285c7cd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'word': 'Savin',\n",
              " 'is_first': True,\n",
              " 'is_last': False,\n",
              " 'is_capitalized': True,\n",
              " 'is_all_caps': False,\n",
              " 'is_all_lower': False,\n",
              " 'is_alphanumeric': 0,\n",
              " 'prefix-1': 'S',\n",
              " 'prefix-2': 'Sa',\n",
              " 'prefix-3': 'Savi',\n",
              " 'suffix-1': 'n',\n",
              " 'suffix-2': 'in',\n",
              " 'suffix-3': 'avin',\n",
              " 'prev_word': '',\n",
              " 'next_word': '',\n",
              " 'has_hyphen': False,\n",
              " 'is_numeric': False,\n",
              " 'capitals_inside': False}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "X_penn_test[0][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFc51jNtDptW"
      },
      "source": [
        "# Training a Tagger\n",
        "\n",
        "Now, we can train supervised machine learning algorithms to PoS Tagging.\n",
        "\n",
        "We'll use the Conditional Random Fields (CRF) algorithm. Here's a brief explanation:\n",
        "\n",
        "* **CRF**: A variation of Markov Random Field. Okay, that might not have helped. It is a discriminative model that, in a quick summary, evaluates the probabilities that a set of states are dependant or not between themselves based on a set of observations. In this case, it evaluates the probabilities that a word observed in a context (defined by the above mentioned features) belongs to a specific PoS. In training time, it takes what is the best state given the set of current observations and probabilities.\n",
        "\n",
        "<div>\n",
        "<img src=\"https://miro.medium.com/max/681/1*8hOWH7YF5INMF2OPhKjVxA.png\" width=\"400\"/>\n",
        "</div>\n",
        "\n",
        "Want more math? Read this: https://towardsdatascience.com/conditional-random-fields-explained-e5b8256da776\n",
        "\n",
        "So, to achieve this, we'll use scikit learn (sklearn) and a sklearn compatible crf suite (skleran_crfsuit). If you don't know what is sklearn, [read this](https://scikit-learn.org/stable/getting_started.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OHTkotyWpd28",
        "outputId": "e7d6c1ce-284b-4624-a97f-36c152b04568"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sklearn_crfsuite\n",
            "  Downloading sklearn_crfsuite-0.5.0-py2.py3-none-any.whl (10 kB)\n",
            "Collecting python-crfsuite>=0.9.7 (from sklearn_crfsuite)\n",
            "  Downloading python_crfsuite-0.9.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from sklearn_crfsuite) (1.2.2)\n",
            "Requirement already satisfied: tabulate>=0.4.2 in /usr/local/lib/python3.10/dist-packages (from sklearn_crfsuite) (0.9.0)\n",
            "Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.10/dist-packages (from sklearn_crfsuite) (4.66.4)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.0->sklearn_crfsuite) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.0->sklearn_crfsuite) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.0->sklearn_crfsuite) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.0->sklearn_crfsuite) (3.5.0)\n",
            "Installing collected packages: python-crfsuite, sklearn_crfsuite\n",
            "Successfully installed python-crfsuite-0.9.10 sklearn_crfsuite-0.5.0\n",
            "Started training on Penn Treebank corpus!\n",
            "Finished training on Penn Treebank corpus!\n",
            "Started training on UD corpus!\n",
            "Finished training on UD corpus!\n"
          ]
        }
      ],
      "source": [
        "#Ignoring some warnings for the sake of readability.\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "#First, install sklearn_crfsuite, as it is not preloaded into Colab.\n",
        "!pip install sklearn_crfsuite\n",
        "from sklearn_crfsuite import CRF\n",
        "\n",
        "#This loads the model. Specifics are:\n",
        "#algorithm: methodology used to check if results are improving. Default is lbfgs (gradient descent).\n",
        "#c1 and c2:  coefficients used for regularization.\n",
        "#max_iterations: max number of iterations (DUH!)\n",
        "#all_possible_transitions: since crf creates a \"network\", of probability transition states,\n",
        "#this option allows it to map even \"connections\" not present in the data.\n",
        "penn_crf = CRF(\n",
        "    algorithm='lbfgs',\n",
        "    c1=0.01,\n",
        "    c2=0.1,\n",
        "    max_iterations=100,\n",
        "    all_possible_transitions=True\n",
        ")\n",
        "#The fit method is the default name used by Machine Learning algorithms to start training.\n",
        "print(\"Started training on Penn Treebank corpus!\")\n",
        "penn_crf.fit(X_penn_train, y_penn_train)\n",
        "print(\"Finished training on Penn Treebank corpus!\")\n",
        "\n",
        "#Same for UD\n",
        "ud_crf = CRF(\n",
        "    algorithm='lbfgs',\n",
        "    c1=0.01,\n",
        "    c2=0.1,\n",
        "    max_iterations=100,\n",
        "    all_possible_transitions=True\n",
        ")\n",
        "print(\"Started training on UD corpus!\")\n",
        "ud_crf.fit(X_ud_train, y_ud_train)\n",
        "print(\"Finished training on UD corpus!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQvig1nQBcbA"
      },
      "source": [
        "# Checking the Results\n",
        "\n",
        "For that, we'll use a score method named balanced f-score. This score takes into account *precision* and *recall*.\n",
        "\n",
        "* **precision**: Considering the universe of tagged words, how many were correctly tagged?\n",
        "* **recall**: Considering the universe of correct tags, how many words were really correctly tagged?\n",
        "\n",
        "The distinction is in the direction you look. Precision looks at all tagged words to find how many are ok; Recall looks at correct tags to find how many were able to be \"guessed\".\n",
        "\n",
        "F-score is then calculated using these two. I won't go into the maths of it.  If you want,\n",
        "* You can read the wikipedia article here: https://en.wikipedia.org/wiki/F1_score\n",
        "* Or watch a neat simple video here: https://www.youtube.com/watch?v=j-EB6RqqjGI&ab_channel=CodeEmporium\n",
        "\n",
        "Also, here's the wikipedia image to help you understand:\n",
        "<div>\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/2/26/Precisionrecall.svg/350px-Precisionrecall.svg.png\"/>\n",
        "</div>\n",
        "\n",
        "We won't go into the computations either. Let the package do its thing (after all, we're interested in NLP now, not in statistics):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uTlJwNkF_0zs",
        "outputId": "2e223752-fa04-4aa0-b77b-f4d3f4330624"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## Penn ##\n",
            "F1 score on Test Data\n",
            "0.9668646324625245\n",
            "F1 score on Training Data \n",
            "0.9936643188628935\n",
            "Class wise score:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         NNP      0.952     0.963     0.957      1213\n",
            "           ,      1.000     1.000     1.000       592\n",
            "          CD      1.000     0.999     0.999       683\n",
            "         NNS      0.964     0.986     0.975       740\n",
            "          JJ      0.879     0.912     0.895       731\n",
            "          MD      0.993     1.000     0.996       135\n",
            "          VB      0.980     0.946     0.963       313\n",
            "          DT      0.992     0.993     0.992      1062\n",
            "          NN      0.962     0.955     0.958      1899\n",
            "          IN      0.981     0.980     0.981      1285\n",
            "           .      1.000     1.000     1.000       509\n",
            "         VBZ      0.958     0.936     0.947       219\n",
            "         VBG      0.936     0.876     0.905       185\n",
            "          CC      1.000     0.997     0.998       287\n",
            "         VBD      0.965     0.945     0.955       492\n",
            "         VBN      0.917     0.907     0.912       279\n",
            "      -NONE-      0.998     1.000     0.999       871\n",
            "          RB      0.912     0.912     0.912       296\n",
            "          TO      1.000     1.000     1.000       298\n",
            "         PRP      1.000     1.000     1.000       150\n",
            "         RBR      0.375     0.231     0.286        13\n",
            "         WDT      0.954     1.000     0.976        62\n",
            "         VBP      0.878     0.902     0.890       112\n",
            "          RP      0.667     0.720     0.692        25\n",
            "        PRP$      1.000     1.000     1.000        74\n",
            "         JJS      0.960     1.000     0.980        24\n",
            "         POS      0.992     1.000     0.996       124\n",
            "          ``      1.000     1.000     1.000        55\n",
            "          EX      0.750     1.000     0.857         3\n",
            "          ''      1.000     1.000     1.000        52\n",
            "          WP      1.000     1.000     1.000        13\n",
            "           :      1.000     1.000     1.000        49\n",
            "         JJR      0.764     0.894     0.824        47\n",
            "         WRB      1.000     0.955     0.977        22\n",
            "           $      1.000     1.000     1.000       170\n",
            "        NNPS      0.739     0.459     0.567        37\n",
            "         WP$      1.000     1.000     1.000         4\n",
            "       -LRB-      1.000     1.000     1.000        16\n",
            "       -RRB-      1.000     1.000     1.000        16\n",
            "         PDT      0.000     0.000     0.000         4\n",
            "         RBS      1.000     1.000     1.000         1\n",
            "          FW      0.000     0.000     0.000         0\n",
            "          UH      0.000     0.000     0.000         0\n",
            "         SYM      0.000     0.000     0.000         0\n",
            "          LS      0.000     0.000     0.000         0\n",
            "           #      0.000     0.000     0.000         0\n",
            "\n",
            "   micro avg      0.967     0.967     0.967     13162\n",
            "   macro avg      0.814     0.814     0.813     13162\n",
            "weighted avg      0.967     0.967     0.967     13162\n",
            "\n",
            "## UD ##\n",
            "F1 score on Test Data \n",
            "0.9042422401050215\n",
            "F1 score on Training Data \n",
            "0.9889075355782778\n",
            "Class wise score:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ADJ      0.823     0.782     0.802       864\n",
            "        NOUN      0.878     0.898     0.888      2338\n",
            "       CCONJ      0.988     0.991     0.989       425\n",
            "       PUNCT      0.998     0.992     0.995      1552\n",
            "         ADP      0.907     0.932     0.919      1090\n",
            "       PROPN      0.370     0.944     0.531       144\n",
            "       SCONJ      0.841     0.782     0.810       385\n",
            "         AUX      0.959     0.963     0.961       675\n",
            "        VERB      0.910     0.786     0.843      1689\n",
            "         DET      0.979     0.983     0.981       996\n",
            "        PRON      0.970     0.965     0.967       990\n",
            "         NUM      0.909     0.974     0.940       154\n",
            "         ADV      0.826     0.880     0.852       560\n",
            "           X      1.000     0.067     0.125        45\n",
            "         SYM      0.545     0.600     0.571        10\n",
            "        PART      0.943     0.929     0.936       408\n",
            "        INTJ      0.800     0.500     0.615         8\n",
            "\n",
            "    accuracy                          0.903     12333\n",
            "   macro avg      0.861     0.822     0.808     12333\n",
            "weighted avg      0.913     0.903     0.904     12333\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#We'll use the sklearn_crfsuit own metrics to compute f1 score.\n",
        "from sklearn_crfsuite import metrics\n",
        "from sklearn_crfsuite import scorers\n",
        "print(\"## Penn ##\")\n",
        "\n",
        "#First calculate a prediction from test data, then we print the metrics for f-1 using the .flat_f1_score method.\n",
        "y_penn_pred=penn_crf.predict(X_penn_test)\n",
        "print(\"F1 score on Test Data\")\n",
        "print(metrics.flat_f1_score(y_penn_test, y_penn_pred,average='weighted',labels=penn_crf.classes_))\n",
        "#For the sake of clarification, we do the same for train data.\n",
        "y_penn_pred_train=penn_crf.predict(X_penn_train)\n",
        "print(\"F1 score on Training Data \")\n",
        "print(metrics.flat_f1_score(y_penn_train, y_penn_pred_train,average='weighted',labels=penn_crf.classes_))\n",
        "\n",
        "# This presents class wise score. Helps see which classes (tags) are the ones with most problems.\n",
        "print(\"Class wise score:\")\n",
        "print(metrics.flat_classification_report(\n",
        "    y_penn_test, y_penn_pred, labels=penn_crf.classes_, digits=3\n",
        "))\n",
        "\n",
        "#Same for UD\n",
        "print(\"## UD ##\")\n",
        "\n",
        "y_ud_pred=ud_crf.predict(X_ud_test)\n",
        "print(\"F1 score on Test Data \")\n",
        "print(metrics.flat_f1_score(y_ud_test, y_ud_pred,average='weighted',labels=ud_crf.classes_))\n",
        "y_ud_pred_train=ud_crf.predict(X_ud_train)\n",
        "print(\"F1 score on Training Data \")\n",
        "print(metrics.flat_f1_score(y_ud_train, y_ud_pred_train,average='weighted',labels=ud_crf.classes_))\n",
        "\n",
        "### Look at class wise score\n",
        "print(\"Class wise score:\")\n",
        "print(metrics.flat_classification_report(\n",
        "    y_ud_test, y_ud_pred, labels=ud_crf.classes_, digits=3\n",
        "))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJUcgYacWTaa"
      },
      "source": [
        "Not too shabby!\n",
        "\n",
        "Remember that State of the Art results for Penn Treebank are at 97% f1.\n",
        "\n",
        "Now, notice how UD is worse (90%)? Probably because there aren't many tags, so less variation and less classes for probability distribution.\n",
        "\n",
        "---\n",
        "\n",
        "But, wouldn't it be better if we could see it actually working?\n",
        "\n",
        "That's what the following cell does. It also helps us understand what we'll have to implement in our main algorithm for it to work.\n",
        "\n",
        "Feel free to play with the input phrase.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C4rcQq-ubbuT",
        "outputId": "8b29da80-abc8-4535-b886-d6c85107b1da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('The', 'DT'), ('tagger', 'NN'), ('produced', 'VBN'), ('good', 'JJ'), ('results', 'NNS')]\n",
            "[('The', 'DET'), ('tagger', 'NOUN'), ('produced', 'VERB'), ('good', 'ADJ'), ('results', 'NOUN')]\n"
          ]
        }
      ],
      "source": [
        "#First, we pass the sentence and \"quickly tokenize it\" - we've already done it in our code, so I'll just mock here with a split:\n",
        "sent = \"The tagger produced good results\"\n",
        "features = [extract_features(sent.split(), idx) for idx in range(len(sent.split()))]\n",
        "\n",
        "#Then we tell the algorithm to make a prediction on a single input (sentence). I'll do once for Penn Treebank and once for UD.\n",
        "penn_results = penn_crf.predict_single(features)\n",
        "ud_results = ud_crf.predict_single(features)\n",
        "\n",
        "#These line magics are just there to make it a neaty print, making a (word, POS) style print;\n",
        "penn_tups = [(sent.split()[idx], penn_results[idx]) for idx in range(len(sent.split()))]\n",
        "ud_tups = [(sent.split()[idx], ud_results[idx]) for idx in range(len(sent.split()))]\n",
        "\n",
        "#The results come out here! Notice the difference in tags.\n",
        "print(penn_tups)\n",
        "print(ud_tups)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8zGOqJzC2JR"
      },
      "source": [
        "### Top Most likely Transition Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jUkADK6wC2JR",
        "outputId": "f37582de-9bc5-4d38-e74b-e81dc5abf70c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Transition Features \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1567"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "print(\"Number of Transition Features \")\n",
        "len(penn_crf.transition_features_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FF3uMp6yC2JR",
        "outputId": "85d8a698-e259-4f7e-d1fe-a77206c9cfbc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(('MD', 'VB'), 6.024137),\n",
              " (('WDT', '-NONE-'), 4.2113),\n",
              " (('NNP', 'POS'), 3.740829),\n",
              " (('TO', 'VB'), 3.659402),\n",
              " (('PDT', 'DT'), 3.594341),\n",
              " (('NNS', 'POS'), 3.536176),\n",
              " (('PRP', 'VBD'), 3.350589),\n",
              " (('NNS', 'VBP'), 3.213575),\n",
              " (('NNP', 'NNP'), 3.15736),\n",
              " (('JJ', 'NN'), 3.022866),\n",
              " (('NN', 'POS'), 2.841618),\n",
              " (('VBP', 'RP'), 2.780188),\n",
              " (('WP', '-NONE-'), 2.759963),\n",
              " (('NNP', 'NNPS'), 2.746184),\n",
              " (('PRP', 'VBP'), 2.717152),\n",
              " (('VBN', '-NONE-'), 2.696489),\n",
              " (('NNPS', 'POS'), 2.694848),\n",
              " (('JJ', 'NNS'), 2.629192),\n",
              " (('PDT', 'PRP$'), 2.611903),\n",
              " (('PRP', 'VBZ'), 2.604405)]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "Counter(penn_crf.transition_features_).most_common(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mmzBTwXxC2JR",
        "outputId": "0392d0f6-5bcd-49e7-d7a2-dabb6a37675a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(('NNP', 'EX'), -2.112821),\n",
              " (('JJR', 'VBG'), -2.12289),\n",
              " (('-NONE-', 'POS'), -2.152712),\n",
              " (('PRP', 'NNS'), -2.172563),\n",
              " (('VBZ', '.'), -2.208415),\n",
              " (('PRP', 'VBN'), -2.215511),\n",
              " (('TO', 'VBP'), -2.217894),\n",
              " (('VBZ', 'MD'), -2.245735),\n",
              " (('RBR', 'NN'), -2.325953),\n",
              " (('CC', '-NONE-'), -2.396139),\n",
              " (('DT', 'VB'), -2.469255),\n",
              " (('PRP', 'POS'), -2.570586),\n",
              " (('-NONE-', 'NNS'), -2.571994),\n",
              " (('IN', 'RBS'), -2.638013),\n",
              " (('NNP', 'VBN'), -2.682921),\n",
              " (('RBR', 'NNS'), -2.743404),\n",
              " (('NNS', 'NNP'), -3.114224),\n",
              " ((',', 'RP'), -3.13517),\n",
              " (('VBZ', 'VBD'), -3.307498),\n",
              " (('VBD', 'VBD'), -3.640485)]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "Counter(penn_crf.transition_features_).most_common()[-20:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IeDuwSaIC2JR"
      },
      "source": [
        "### Top Most Likely State Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "izBa-TLNC2JR",
        "outputId": "54ef8560-d5ae-488d-9542-9de33549e154"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of State Features \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "36196"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "print(\"Number of State Features \")\n",
        "len(penn_crf.state_features_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r_jymIoPC2JR",
        "outputId": "7bc1acfa-2edf-4239-f37e-7b08264c9d80"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(('word:interest-rate', 'NN'), 6.227139),\n",
              " (('prefix-1:*', '-NONE-'), 6.138283),\n",
              " (('suffix-2:ly', 'RB'), 6.089414),\n",
              " (('suffix-1:s', 'NNS'), 5.825122),\n",
              " (('suffix-1:s', 'VBZ'), 5.788958),\n",
              " (('suffix-2:ss', 'NN'), 4.890271),\n",
              " (('is_capitalized', 'NNP'), 4.482083),\n",
              " (('suffix-2:ed', 'VBN'), 4.329963),\n",
              " (('prev_word:Raul', 'NN'), 4.307088),\n",
              " (('has_hyphen', 'JJ'), 4.294396),\n",
              " (('suffix-2:ng', 'VBG'), 4.281525),\n",
              " (('suffix-1:g', 'VBG'), 4.257789),\n",
              " (('prev_word:tune', 'RP'), 4.227194),\n",
              " ((\"prev_word:n't\", 'VB'), 4.150681),\n",
              " (('next_word:', 'NN'), 4.087802),\n",
              " (('prev_word:later', 'RP'), 4.080315),\n",
              " (('suffix-1:d', 'VBD'), 4.060264),\n",
              " (('suffix-2:st', 'JJS'), 4.009616),\n",
              " (('prev_word:*', 'VB'), 4.002673),\n",
              " (('suffix-2:rs', 'NNS'), 3.999978)]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "Counter(penn_crf.state_features_).most_common(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YyC0TJjTC2JR",
        "outputId": "3ba54b1a-1339-48b4-d617-3550b915086e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(('prefix-3:shor', 'NN'), -2.247673),\n",
              " (('suffix-3:lion', 'NN'), -2.263752),\n",
              " (('prev_word:paid', 'IN'), -2.297541),\n",
              " (('prev_word:by', 'VBG'), -2.393153),\n",
              " (('prev_word:leaving', 'NNS'), -2.416315),\n",
              " (('prev_word:Malcolm', 'NNP'), -2.461706),\n",
              " (('suffix-3:less', 'NN'), -2.464933),\n",
              " (('prev_word:moderate', 'NN'), -2.600659),\n",
              " (('prefix-3:unde', 'JJ'), -2.613726),\n",
              " (('has_hyphen', 'VBG'), -2.635978),\n",
              " (('is_capitalized', 'VBD'), -2.696345),\n",
              " (('suffix-1:s', 'NN'), -2.731046),\n",
              " (('suffix-2:ed', 'NN'), -2.81631),\n",
              " (('has_hyphen', 'VBN'), -3.006344),\n",
              " (('prev_word:*', 'NNP'), -3.029953),\n",
              " (('is_all_caps', 'JJ'), -3.100154),\n",
              " (('has_hyphen', 'VBD'), -3.153063),\n",
              " (('is_all_caps', 'NNP'), -3.263742),\n",
              " (('is_all_lower', 'NNPS'), -3.676713),\n",
              " (('is_all_lower', 'NNP'), -5.238513)]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "Counter(penn_crf.state_features_).most_common()[-20:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Mn831Zxbckt",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "# Saving the Weights\n",
        "\n",
        "We will want to load this to our NLPTools, right? So we have to save the weights. This means saving the classifier we trained to be able to classify our tokens.\n",
        "\n",
        "To do it, we use Pickle, which is a Python package to save a readable binary file extension called \"pickle\". We'll later open this in our tool.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ul2KlQu4c5-N"
      },
      "outputs": [],
      "source": [
        "#import the pickle module\n",
        "import pickle\n",
        "\n",
        "#Simply dump! Use 'wb' in open to write bytes.\n",
        "\n",
        "penn_filename = 'penn_treebank_crf_postagger.sav'\n",
        "pickle.dump(penn_crf, open(penn_filename, 'wb'))\n",
        "\n",
        "ud_filename = 'ud_crf_postagger.sav'\n",
        "pickle.dump(ud_crf, open(ud_filename,'wb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6O2F5HijfWc8"
      },
      "source": [
        "To open the file, we just have to import the module and read the file using:\n",
        "\n",
        "`model = pickle.load(open(filename, 'rb'))`\n",
        "\n",
        "Great, we now have pickle files that can be loaded in our tool. Just download them using the lefthand file explorer and we're good to go!\n",
        "See you back at the article!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}