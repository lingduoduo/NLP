{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8155e530",
      "metadata": {
        "id": "8155e530"
      },
      "source": [
        "# List the files in the directory"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "q1DafOHeYCj7"
      },
      "id": "q1DafOHeYCj7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7062cb2",
      "metadata": {
        "id": "f7062cb2",
        "outputId": "e031aa8b-fb8e-489c-e7dd-c6db5a3fbe59",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Arun Chauhan',\n",
              " 'Aruna Malapati',\n",
              " 'Animesh Chaturvedi',\n",
              " 'Amit Saxena',\n",
              " '.ipynb_checkpoints',\n",
              " 'Amita Jain',\n",
              " 'Ankita Jain']"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "# The PDF files for each author are stored in their respective folder\n",
        "import os\n",
        "os.listdir('/Dataset-IK/') #No. of authors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc4a38ef",
      "metadata": {
        "id": "fc4a38ef",
        "outputId": "e6c327ca-94e4-43d8-9c95-846f4dbd0d41",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total No. of PDF files (research papers): 64\n"
          ]
        }
      ],
      "source": [
        "c = 0 #No. of files\n",
        "for root, dirs, files in os.walk('/Dataset-IK/'):\n",
        "    for file in files:\n",
        "        c = c + 1\n",
        "        #print(os.path.join(root, file))\n",
        "\n",
        "print('Total No. of PDF files (research papers):',c)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2120f7a",
      "metadata": {
        "id": "c2120f7a"
      },
      "source": [
        "# Extracting text from PDFs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99fa0eb1",
      "metadata": {
        "id": "99fa0eb1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c748f1c-9b27-4ca5-f134-5715298544e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tika\n",
            "  Downloading tika-2.6.0.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tika) (67.7.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from tika) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->tika) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->tika) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->tika) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->tika) (2023.7.22)\n",
            "Building wheels for collected packages: tika\n",
            "  Building wheel for tika (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tika: filename=tika-2.6.0-py3-none-any.whl size=32621 sha256=45d221dc25cb08f74c80a96fdb3c22177bf6818efe9781ec1514609502ebd771\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/71/c7/b757709531121b1700cffda5b6b0d4aad095fb507ec84316d0\n",
            "Successfully built tika\n",
            "Installing collected packages: tika\n",
            "Successfully installed tika-2.6.0\n"
          ]
        }
      ],
      "source": [
        "!pip install tika"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4468825d",
      "metadata": {
        "id": "4468825d"
      },
      "outputs": [],
      "source": [
        "import tika #PDF Text Extractor\n",
        "from tika import parser\n",
        "\n",
        "# Set up tika\n",
        "tika.initVM()\n",
        "\n",
        "# Function to extract text from PDF\n",
        "def extract_text(filename):\n",
        "    # Parse PDF file\n",
        "    parsed = parser.from_file(filename)\n",
        "\n",
        "    # Extract text from parsed data\n",
        "    text = parsed[\"content\"]\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e454eee2",
      "metadata": {
        "id": "e454eee2",
        "outputId": "cdf5ae1f-f40b-44d7-a000-6a56b8514969",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSupervised Heterogeneous Domain Adaptation via Random Forests\\n\\n\\nSupervised Heterogeneous Domain Adaptation via Random Forests\\n\\nSanatan Sukhija1, Narayanan C Krishnan1, Gurkanwal Singh2⇤\\n\\n1Department of Computer Science and Engineering, Indian Institute of Technology Ropar,\\nPunjab, India sanatan@iitrpr.ac.in, ckn@iitrpr.ac.in\\n\\n2Department of Computer Science and Engineering, PEC University of Technology,\\nChandigarh, India gurkanwal.singh7@gmail.com\\n\\nAbstract\\nHeterogeneity of features and lack of correspon-\\ndence between data points of different domains are\\nthe two primary challenges while performing fea-\\nture transfer. In this paper, we present a novel su-\\npervised domain adaptation algorithm (SHDA-RF)\\nthat learns the mapping between heterogeneous\\nfeatures of different dimensions. Our algorithm\\nuses the shared label distributions present across\\nthe domains as pivots for learning a sparse fea-\\nture transformation. The shared label distributions\\nand the relationship between the feature spaces and\\nthe label distributions are estimated in a supervised\\nmanner using random forests. We conduct exten-\\nsive experiments on three diverse datasets of vary-\\ning dimensions and sparsity to verify the superi-\\nority of the proposed approach over other baseline\\nand state of the art transfer approaches.\\n\\n1 Introduction\\nThe key to success of many supervised learning algorithms\\nis the availability of abundant labeled training data. How-\\never, for many real-world problems, collecting labeled data\\nis often very expensive and cumbersome. Transfer learning\\nalgorithms help to overcome the scarcity of labeled data in\\na domain (often referred to as the target domain) by util-\\nising information about the task, and data from single or\\nmultiple auxiliary domains (referred to as source domains).\\nTransfer learning approaches have found success in many\\napplications including activity recognition [Hu et al., 2011;\\nCook et al., 2013c], sentiment classification [Zhou et al.,\\n2014], document analysis and indoor localization [Pan and\\nYang, 2010].\\n\\nA popular setting for performing transfer is when the\\nsource and the target domains are represented by the same\\nset of features. The goal in this setting is to minimise the dif-\\nferences in the data distribution of the source and target do-\\nmains. However, for applications such as sentiment analysis\\nacross different languages [Pan, 2010], and activity recogni-\\ntion across different domains [Cook et al., 2013b], the source\\n\\n⇤The author contributed to this work during his internship at IIT\\nRopar.\\n\\nand target data are represented using heterogeneous features\\nof different dimensions that may or may not overlap. Transfer\\nlearning for such heterogeneous domains can be performed\\nby first bridging the gap between the features characterising\\nthe different domains. This is the principle behind feature-\\nbased transfer learning approaches.\\n\\nThe feature-based transfer approach proposed in this pa-\\nper is motivated by the application of activity recognition in\\na smart home. Smart home based activity recognition deals\\nwith learning the daily activities of smart home resident(s),\\ncaptured through a series of sensor observations. Transfer\\nlearning algorithms can be used to overcome the scarcity of\\nlabeled data of a new target smart home by utilising the la-\\nbeled data of other source smart homes. However, different\\nlayouts and types of sensors deployed at different places lead\\nto heterogeneous feature spaces [Hu and Yang, 2011] neces-\\nsitating transfer methodologies. Figure 1 illustrates the layout\\nand sensor locations for three smart homes from the CASAS\\ndatasets [Cook et al., 2013c] used in this paper. Given only\\na few labeled instances in the target we leverage the common\\nlabels in the source and target domains to derive the relation-\\nship between the corresponding feature spaces. The key as-\\nsumption of our algorithm is that features in both source and\\ntarget domains that characterise data partitions with similar\\nlabel distribution, must be related to each other. The shared\\nlabel distributions across the two domains act as the pivot for\\nlearning the mapping between the feature spaces. The gen-\\nerated sparse mapping represents a target feature as a linear\\ncombination of source features. This mapping is estimated\\nwithout assuming any correspondence between source and\\ntarget data points.\\n\\n1.1 Problem Definition\\nLet {XS , YS}mi=1 and {XT , YT }nj=1 represent the set of la-\\nbeled instances in the source domain S and target domain T\\n\\nrespectively, where m o n. xS 2 RdS is a source data\\npoint with yS 2 Y the corresponding class label. Similarly,\\nxT 2 RdT is a target data point and yT 2 Y is its associated\\nlabel. The features that describe xS and xT are completely\\ndifferent and d\\n\\nS 6= d\\n\\nT . However, we assume that the source\\nand target domains share a common label space. Let the num-\\nber of shared labels be k. Our goal is to learn a mapping\\nf : RdS ! RdT such that the data from the source domain\\ncan be mapped to the target domain. This mapped source data\\n\\nProceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence (IJCAI-16)\\n\\n2039\\n\\n\\n\\nhh102 hh113 hh118\\n\\nFigure 1: Layouts of the three CASAS smart homes that differ in terms the layout, and count of the sensors deployed. The\\nblack squares represent the location of sensors.\\n\\ncan then be used in conjunction with the target data to learn\\nthe hypothesis h : RdT ! Y .\\n\\n1.2 Contributions\\nThe contributions of this paper can be summarised as follows:\\n\\n1. The proposed algorithm yields a heterogeneous feature-\\nspace class-invariant mapping, assuming no correspon-\\ndence between the data-points of the domains that share\\nno overlapping features.\\n\\n2. Our algorithm does not require the computation of an\\noptimal code matrix for error correcting output code,\\nwhich is a challenging task, that is a requirement for\\nthe current supervised state of the art feature transfer al-\\ngorithm [Zhou et al., 2014]. The proposed algorithm\\nutilises naturally occurring label distributions at leaf\\nnodes of a decision tree model as pivots to generate the\\nmapping PS 2 RdS⇥dT .\\n\\n3. The experiments conducted on diverse datasets indicate\\nthe effectiveness of the algorithm even if very few la-\\nbeled instances are available in the target domain.\\n\\n2 Related Work\\nBridging features across heterogeneous spaces for domain\\nadaptation is a challenging problem. The approaches for het-\\nerogeneous domain adaptation can be broadly split into two\\ncategories based on the type of mapping learned, namely,\\nFeature Remapping and Latent Space Transformation.\\n\\nFeature Remapping approaches determine the transforma-\\ntion for converting source features to target features or vice-\\nversa. It can in turn be of two types: one in which there is\\nan explicit correspondence between the individual features of\\nsource and target domain such as the i\\n\\nth source feature be-\\ning mapped to the j\\n\\nth target feature, and second in which a\\nsource or target feature is represented as a combination (of-\\nten linear) of features from the other domain. Approaches for\\none-to-one source to target feature remapping have used ge-\\nnetic algorithms and other greedy methods to obtain an op-\\ntimal mapping, using classification accuracy as the perfor-\\nmance measure [Feuz and Cook, 2014]. Alternate approaches\\nrely on domain independent features known as pivots that can\\nbe utilised to align the feature spaces [Blitzer et al., 2006;\\nHe et al., 2014]. These approaches partition the features of a\\ndomain into independent and dependent sets. The domain\\nindependent features are present across different domains,\\nwhile the dependent features are specific to a domain. The\\ngoal is to learn the mapping between the dependent features\\n\\nby using the independent features. Spectral clustering algo-\\nrithm can be used to obtain the feature-clusters from the co-\\naligned bipartite graph of domain independent and dependent\\nfeatures that acts as the common subspace [Pan, 2010]. In the\\nabsence of explicit domain independent features, statistical\\nproperties of domain specific features can be used to derive\\nmeta features to bridge the domains [Feuz, 2014]. A recent\\nwork on feature remapping for feature transfer constructs a\\nclass-invariant sparse transformation matrix by mapping the\\nweight vectors of SVM classifier trained on labeled data from\\nthe domains [Zhou et al., 2014]. Synthetically generated er-\\nror correcting output codes (ECOC) are used to train the SVM\\nmodel so as to estimate accurate transformations. We com-\\npare the performance of our algorithm against this supervised\\nheterogeneous feature remapping approach (SHFR-ECOC).\\n\\nLatent Space Transformation approaches determine trans-\\nformations to project the data of different domains onto a\\ncommon latent space. Specifically, these approaches compute\\ntwo projection matrices PS and PT for source and target do-\\nmains respectively, such that the difference between the pro-\\njected source space BS : PS ⇥XS and projected target space\\nBT : PT ⇥XT is minimised while trying to preserve the char-\\nacteristics of the original feature spaces. Approaches in this\\ncategory can be summarised as unsupervised Eigen transfer\\nframeworks. Manifold alignment based approaches [Wang\\nand Mahadevan, 2009] determine the transformed space by\\nmaking the manifold assumption, where the mapping brings\\ndata distributions of the domains closer to each other while\\npreserving the local geometric structure and maximising the\\nalignment. However, these approaches only work for data\\nthat exhibit strong manifold property and therefore are not\\napplicable to other scenarios. Heterogeneous spectral map-\\nping (HeMap) [Shi and Yu, 2012] optimises the difference in\\nthe latent space in a general setting by learning two transfor-\\nmation matrices using spectral embedding without using any\\nlabel information. The algorithm implicitly tries to discover\\nthe correspondence between the data points of source and tar-\\nget domain through an optimisation framework. Often, in sit-\\nuations where there is no explicit data correspondences, the\\nrecovered transformations are noisy. Since these approaches\\ndirectly estimate the projected data, estimating the projection\\nfor out-of-sample data is a challenging problem. We com-\\npare the performance of our algorithm against the HeMap ap-\\nproaches that compute linear and non-linear transformations.\\n\\nIn the context of smart home activity recognition, heteroge-\\nneous layouts can be manually unified by defining a common\\nmeta-feature space that is shared by all smart apartments.\\nThese meta-features can be manually specified by a domain\\nexpert or can be derived from structural, temporal, spatial or\\n\\n2040\\n\\n\\n\\nXS1<x1\\n\\nXS3<x3XS2<x2\\n\\nXS4<x4\\nXT2<y1\\n\\nXT1<y2\\n\\nXT3<y3\\n\\nXT4<y4\\n\\nWS\\nWT\\n\\nTarget DomainSource Domain\\n\\nShared label distribution\\n\\nLabels Features Features \\n\\nCo\\nnt\\n\\nrib\\nut\\n\\nio\\nn\\n\\nDi\\nstr\\n\\nib\\nut\\n\\nio\\nn\\n\\nFigure 2: Illustration of the relationship between the features of the two domains based on a single pivot label distribution.\\n\\nfunctional similarities of original features [Rashidi and Cook,\\n2010]. A manual mapping is not optimal and a poor map-\\nping can drastically hinder the recognition performance of the\\nmodel. The proposed algorithm intends to exploit label space\\ndistributions of data partitions to identify a mapping across\\nthe domains for addressing the problem of heterogeneous do-\\nmain adaptation.\\n\\n3 Proposed Methodology\\nThe task of determining ‘common’ features between hetero-\\ngeneous source and target feature spaces for knowledge trans-\\nfer is a challenging problem [Li et al., 2014]. Our novel so-\\nlution to this problem leverages the common label informa-\\ntion between the source and target domains as the pivot for\\nknowledge transfer. The proposed algorithm determines the\\nmapping PS between source and target features based on the\\nestimate of the contribution of the features towards creating\\ndata partitions having similar label distributions.\\n\\n3.1 Estimating Pivots Across the Domains\\nThe first step in our proposed approach is to derive the piv-\\nots that are used to construct the bridge across heterogeneous\\nfeature spaces. We define the pivots in terms of the shared\\nlabels between the source and target domains. In the sim-\\nplest scenario each shared label is a pivot. When the num-\\nber of shared labels between the domains is small, learning\\nthe feature mapping is a challenging problem [Zhou et al.,\\n2014]. The SHFR ECOC approach overcomes this limita-\\ntion by using synthetically generated error-correcting output\\ncodes (ECOC) for representing each shared label. It is desir-\\nable that the class labels are independent as the relationships\\nbetween the different labels are not effectively captured due\\nto the randomness of the ECOC generation process [Rajan\\nand Ghosh, 2004]. Thus selecting the optimal code matrix is\\na challenging problem for SHFR ECOC. Our approach over-\\ncomes this limitation by relying on naturally occurring label\\ndistributions in the complex data space.\\n\\nTo arrive at these label distributions, our approach looks at\\nthe leaf nodes of a decision tree modeled on the dataset. A\\ndecision tree follows a greedy strategy to recursively parti-\\ntion the data based on some feature value test. A leaf node\\nin a decision tree holds a distribution L of class labels that is\\nassociated with a set of data partition. A path in a decision\\ntree from the root to a leaf node contains a sequence of fea-\\ntures chosen as split functions. The candidate split at a node\\ninvolves a locally optimal partitioning based on some metric\\nlike Gini Impurity, Information Gain or Gain Ratio.\\n\\nTo ensure a sufficient number of pivotal label distributions\\nfor learning the mapping between the domains, we train a\\nrandom forest, which also helps to reduce overfitting. Every\\ntree in the forest is constructed using a random subset of fea-\\ntures [Breiman, 2001]. Our algorithm first constructs ns and\\nnt trees from S and T respectively. Every path in a decision\\ntree leading to a partition of data is associated with a certain\\nlabel distribution. Label distributions that appear both in the\\nsource and target random forest models are the pivots that are\\nused for bridging the two domains.\\n\\n3.2 Estimating Feature Relationships WS and WT\\n\\nThe next step in the algorithm computes the relationship ma-\\ntrices WS and WT between the domain dependent features\\nand the shared pivots. Since our pivots are label distributions,\\nwe define this relationship as the contribution of the domain\\ndependent features towards creating the pivot label distribu-\\ntion. This relationship can be easily extracted from the deci-\\nsion tree structure. The boundary of the data partition at the\\nleaf node can be identified with the feature splits along the\\npath to the leaf node from the root of the tree.\\n\\nA simple approach to compute WS and WT would be to\\ngive equal importance to all the features that were used as\\nsplit nodes in the path. Thus for a path, the i\\n\\nth entry in the\\ncorresponding feature relationship vector would contain the\\nfrequency of the i\\n\\nth feature getting selected as a split node.\\nAnother approach would be to give higher priority to a feature\\nused at parent node compared to the features chosen as split\\nnodes at its descendants. For every path, each entry in the\\nfeature contribution vector is given by\\n\\nPc\\ni=1(1/2)\\n\\nv(i) where\\nv(i) denotes the decision tree depth at which the split was\\nmade and c represents the frequency of the feature being used\\nas a candidate split in the path.\\n\\nIn practice, it is common to have duplicate label distribu-\\ntions at leaf nodes, i.e., different data partitions correspond-\\ning to the same label distribution. The feature contribution\\nvectors for these data partitions are averaged. Thus at the\\nend of this process, for each shared pivotal label distribution\\nbetween S and T, we also have the domain dependent fea-\\nture relationships to these pivots. Based on the similar source\\nand target class label distributions, the estimated feature con-\\ntribution matrices WS 2 RNp⇥dS and WT 2 RNp⇥dT are\\nmapped to yield the source projection matrix PS , where Np\\n\\nis the number of pivots. This process is illustrated in Figure\\n2. The advantage with using a random forest model is that\\nthe pivots and the relationship between the domain dependent\\nfeatures and pivots across source and target can be estimated\\nfrom a single model reducing the complexity of the transfer\\napproach.\\n\\n2041\\n\\n\\n\\nAlgorithm 1 Supervised HDA via Random Forests (SHDA-\\nRF)\\nInput: Source data: S 2 RM⇥dS and Target data: T 2 RN⇥dT\\n\\nOutput: PS 2 RdS⇥dT\\n\\n1. Build a random forest with ns trees from source features XS .\\n2. For every path from the root to a leaf node in a tree, the con-\\ntribution of a feature is estimated as W (xS(v)) = W (xS(v)) +\\n(1/2)v where v denotes the level at which the feature x 2 XS\\n\\nwas selected as a candidate split. The corresponding label distri-\\nbution L is acquired from the leaf node.\\n3. Similarly construct the target features contribution matrix WT\\n\\nusing nt trees created from T.\\n4. Remove duplicates from LS and LT . For every duplicate entry\\nin LS and LT , the corresponding feature vector entries in WS and\\nWT are averaged.\\n5. Return the corresponding WS and WT for the identical class\\nlabel distributions.\\n6. The mapping PS can be obtained by running LASSO dT times\\non obtained WS and WT from Step 5.\\n\\nTable 1: Summary of CASAS-HH datasets.\\nDataset Feature count Activity count\\nhh102 43 29\\nhh113 48 30\\nhh118 44 32\\n\\n3.3 Deriving the Feature Transformation\\n\\nThe last step in our algorithm derives a sparse transformation\\nPS between the two domains. Our objective is to represent\\neach target feature as a linear combination of a small set of\\nsource features. The Least Absolute Shrinkage and Selection\\nOperator (LASSO) is used to learn PS from WS and WT . It\\nis defined as:\\n\\nmin\\nPS\\n\\n1\\n\\nNp\\n\\nNpX\\n\\ni=1\\n\\nk WT �WSPS k22 +\\ndTX\\n\\ni\\n\\n�i k PSi k1,\\n\\ns.t. PSi � 0\\n\\nThe first part of the optimisation problem minimises the dif-\\nference between the projected source feature contribution ma-\\ntrix PS ⇥WS and target feature contribution matrix WT . The\\nsecond part is the L1 regularisation term to obtain a sparse\\ntransformation matrix. The regularisation parameter � con-\\ntrols the size of this subset. There are dT minimisation prob-\\nlems that are solved using Least Angle Regression (LARS)\\n[Hastie et al., 2001].\\n\\nThe proposed methodology is summarised in Algorithm\\n1. Once the mapping PS 2 RdS⇥dT is obtained, the tar-\\nget model is retrained along with the projected source data\\n(S ⇥ PS). The SHFR-ECOC approach does not retrain the\\nmodel after finding the transformation. It uses the source\\nmodel to predict the class labels of transformed target in-\\nstances. In contrast, our approach utilizes the benefits of ran-\\ndomization and implicit feature selection of RF to retrain the\\nmodel attuned for target domain.\\n\\n4 Experiments\\nWe compare the performance of the proposed algorithm\\nagainst other baseline classifiers and approaches that perform\\ntransfer. Random forests (BRF) and SVM that uses ECOC\\n(SVM ECOC) were chosen as the baseline classifiers. Trans-\\nfer approaches include SHFR ECOC [Zhou et al., 2014] and\\nHeMAP (linear and non-linear) [Shi and Yu, 2012]. The num-\\nber of trees in the random forest was set to 100. The number\\nof bagged features for learning in a tree in the forest was set\\nto\\n\\np\\nd+ 5, where d is the total number of features.\\n\\nThe parameters for the SVM model with RBF kernel were\\nfine-tuned using grid search. Based on cross validation ex-\\nperiments, the length of ECOC was set to 35, beyond which\\nthe performance plateaued. We choose three diverse datasets,\\nvarying in the size and sparsity of the features, for investigat-\\ning the performance of the different algorithms.\\n\\nThe CASAS dataset [Cook et al., 2013a] is a collection of\\nsmart home datasets that are widely used for investigating ac-\\ntivity recognition algorithms. We use the horizon house (HH)\\ndatasets from this collection, which are records of sensor data\\nfrom single resident smart homes. Sensor data from one smart\\nhome serves as the source and another acts as the target. A\\nsliding window of 20 sensor events is used to build the fea-\\nture vector that consists of counts of sensor events within the\\nsliding window, along with temporal features such as time\\nof the day and day of the week [Cook and Krishnan, 2015;\\nFeuz, 2014]. The feature vector is annotated with the activ-\\nity label associated with the last sensor event in the sliding\\nwindow. The number of features and activity labels in each\\ndataset are presented in Table 1. The feature values of the sen-\\nsors in close vicinity appear to be mutually related. This mo-\\ntivates learning a sparse feature mapping instead of a dense\\nmapping. The target training set consists of approximately\\n7000 samples that preserve the original class distribution. 16\\nsuch random subsets are used for evaluating the performance\\nof the different algorithms.\\n\\nThe 20 Newsgroups [Lang, 1995] text collection is a\\nsparse dataset of approximately 19000 documents belonging\\nto 20 classes that follow a label hierarchy. The transfer ex-\\nperiments were performed on two datasets each containing\\nthe subcategories falling under rec and talk, and rec and sci\\nrespectively. There are a total of 8 classes in each dataset\\nwith a vocabulary spanning over 26000 words. We consid-\\nered only the first 10000 features that contributed the most\\ntowards the classification task. For each dataset, two transfer\\nsettings were created. In the first setting, the source and target\\nconsisted of random and mutually exclusive partition of 5000\\nfeatures. Target training data is created by randomly selecting\\n10 samples per class. In the second setting, the roles of the\\nsource and target dataset were reversed. Since the baseline\\nSVM ECOC model was unable to handle the high dimen-\\nsional features, PCA was performed while preserving 75%\\nvariance on the TF-IDF feature values. Dimensionality re-\\nduction is not performed as a pre-processing step for the other\\napproaches. The predefined test partitions of the dataset are\\nused for testing the approaches.\\n\\nThe Statlog (Landsat Satellite) [Lichman, 2013] image\\ndataset comprises of 6 classes and 36 real-valued features. It\\n\\n2042\\n\\n\\n\\nTable 2: Performance comparison is depicted in terms of mean error(%). Statistically significant SHDA-RF results against BRF\\nand SHFR-RF are highlighted in bold and indicated by ⇤ respectively.\\n\\nCASAS HH datasets\\nBaseline Results Transfer Results\\n\\nS!T BRF SVM-\\nECOC\\n\\nSHFR-\\nECOC\\n\\nHeMap\\nLinear\\n\\nHeMap\\nNon-Linear\\n\\nFA SHFR-RF SHDA-RF\\n\\nhh102!hh113 30.49±2.58 47.14±1.00 39.22±1.63 51.06±1.53 52.97±0.97 34.71±1.55 28.68±1.14 27.93±2.54\\nhh102!hh118 28.6±1.07 57.74±1.84 43.52±1.18 59.6±0.89 61.8±0.87 37.7±2.38 27.89±0.95 26.97±1.15*\\nhh113!hh102 28.44±1.54 37.54±1.60 38.70±1.50 41.41±1.92 43.47±2.53 38.64±2.68 25.97±1.69 25.97±1.00\\nhh113!hh118 21.6±0.45 54.97±1.13 36.7±1.41 58.4±1.26 63±1.39 31±2.7 19.47±1.07 18.38±1.29*\\nhh118!hh102 29.6±1.86 39.99±1.59 39.28±1.88 43±0.99 45.7±0.9 37.4±2.67 29.54±1.88 27.83±2.64*\\nhh118!hh113 23.5±1.21 36.3±0.67 32.35±1.1 40.3±0.72 41.4±0.53 31±7.38 21.69±0.68 21.54±1.47\\n\\n20 Newsgroups dataset\\nBaseline results Transfer Results\\n\\nS ! T BRF SVM-ECOC\\n(PCA)\\n\\nSHFR-ECOC\\n(PCA)\\n\\nHeMap\\nLinear\\n\\nHeMap\\nNon-Linear\\n\\nSHFR-RF SHDA-RF\\n\\nrec v/s sci\\nF1:F5000!F5001:F10000 51.91±2.3 50.49±4.1 48.01±3.5 63.6±3.62 63.22±4.1 46.61±1.36 40.06±2.9*\\nF5001:F10000!F1:F5000 68.41±3.6 67.09±4.0 60.23±6.6 73.1±3.9 72.8±4.6 58.12±2.13 56.81±4.1*\\n\\nrec v/s talk\\nF1:F5000!F5001:F10000 55.79±1.1 56.12±1.6 51.55±2.5 66.2±3.9 66.0±3.55 49.99±0.12 48.82±3.3*\\nF5001:F10000!F1:F5000 68.63±2.4 66.16±3.8 52.92±3.1 70.44±3.0 70.2± 6.11 44.67±0.23 35.51±5.2*\\n\\nSatellite Statlog dataset\\nS!T BRF SVM ECOC SHFR-ECOC HeMap-\\n\\nLinear\\nHeMap\\n\\nNon-Linear\\nSHFR-RF SHDA-RF\\n\\nF1:F18!F19:F36 19.30±0.9 21.45±1.3 22.20±2.13 33.31±5.8 33.18±5.1 19.42±1.65 18.58±1.6\\nF19:F36!F1:F18 20.05±1.00 21.50±1.67 22.45±1.1 31.57±6.1 32.16±4.2 19.73±1.45 18.66±0.78*\\n\\nTable 3: Performance of SVM and SHFR ECOC models on\\noriginal features of 20 Newsgroups dataset.\\n\\nrec v/s sci\\nS ! T SVM ECOC SHFR ECOC\\n\\nF1:F5000!F5001:F10000 79.48±3.39 87.09±4.1\\nF5001:F10000!F1:F5000 85.01±3.76 88.22±3.12\\n\\nrec v/s talk\\nF1:F5000!F5001:F10000 83.12±3.6 85.16±4.23\\nF5001:F10000!F1:F5000 89.55±4.1 86.92±3.75\\n\\nconsists of 4435 examples in the training set and 2000 exam-\\nples in the test set. The 36 features were split randomly into\\ntwo equal groups for creating the source and target domains.\\nTo evaluate different algorithms, we used multiple sets of 10\\nlabeled samples per class to create target training data.\\n\\n5 Results and Discussion\\nThe performance of different classifiers on the datasets is re-\\nported in Table 2. The common observation across all the\\ndatasets is the superior performance of baseline random forest\\n(BRF) model over other baseline and some transfer learning\\napproaches. This is another motivation behind adopting ran-\\ndom forest model for performing transfer. The performance\\nof the SHDA-RF algorithm on the CASAS-HH dataset is sig-\\nnificantly better than all the other approaches by about 2-3%\\n(p-value< 0.05). Among the baseline classifiers, it is evi-\\ndent that the BRF models perform better than SVM ECOC.\\nThis can be explained by considering that the activity labels in\\nthe dataset are annotated by humans using rule based heuris-\\ntics. It can be also noted that SHFR ECOC, a transfer strat-\\negy based on SVM ECOC, performs better than SVM ECOC\\n\\nTable 4: Performance of HeMap (Linear and Non-Linear) on\\n20 Newsgroups dataset without explicit correspondence be-\\ntween source and target data\\n\\nrec v/s sci\\nS ! T HeMap\\n\\nLinear\\nHeMap\\n\\nNon-Linear\\nF1:F5000!F5001:F10000 83.75±4.23 93.75± 4.79\\nF5001:F10000!F1:F5000 85.01±3.76 88.22±3.12\\n\\nrec v/s talk\\nF1:F5000!F5001:F10000 87.5±6.45 86.44±6.52\\nF5001:F10000!F1:F5000 83.75±4.79 85.0± 7.07\\n\\nsignificantly. This suggests that the possibility of knowledge\\ntransfer between the two domains, which is further reinforced\\nby the performance improvement obtained by SHDA-RF over\\nBRF model. Another common strategy that is used for per-\\nforming transfer on activity recognition datasets is by defin-\\ning a mapping that aggregates sensors to form layout inde-\\npendent functional areas (FA) [van Kasteren et al., 2010] as\\nan explicit meta-feature space. For example, individual sen-\\nsors in the ‘bedroom’ are all clubbed together under a single\\nfeature. It can be observed from Table 2 that this approach\\nperforms worse than the BRF model. This suggests poten-\\ntial loss in information due to aggregation of different sen-\\nsor events that is critical for differentiating activities happen-\\ning in the same functional area. The FA approach, an unsu-\\npervised transfer approach, on the other hand performs sig-\\nnificantly better than other unsupervised transfer approaches\\nnamely HeMAP (linear and non-linear).\\n\\nOn the high dimensional 20 Newsgroups dataset, SHDA-\\nRF results in superior performance as compared to all the\\nother approaches. The difference in the performance of\\n\\n2043\\n\\n\\n\\n7000 samples 1 week 2 weeks 3 weeks\\n0\\n\\n10\\n\\n20\\n\\n30\\n\\nTarget Data Size −−−−>\\n\\nM\\nea\\n\\nn \\nEr\\n\\nro\\nr \\n−−\\n−>\\n\\n \\n\\n \\n\\nBRF\\nSHDA−RF\\n\\nhh102−−>hh113\\n\\n7000 samples 1 week 2 weeks 3 weeks\\n0\\n\\n5\\n\\n10\\n\\n15\\n\\n20\\n\\n25\\n\\nTarget Data Size −−−−>\\n\\nM\\nea\\n\\nn \\nEr\\n\\nro\\nr \\n−−\\n−>\\n\\n \\n\\n \\n\\nBRF\\nSHDA−RF\\n\\nhh113−−>hh118\\n\\n7000 samples 1 week 2 weeks 3 weeks\\n0\\n\\n10\\n\\n20\\n\\n30\\n\\nTarget Data Size −−−−>\\n\\nM\\nea\\n\\nn \\nEr\\n\\nro\\nr \\n−−\\n−>\\n\\n \\n\\n \\n\\nBRF\\nSHDA−RF\\n\\nhh118−−>hh102\\n\\nFigure 3: Availability of labeled target data helps to reduce mean error by learning a better mapping\\n\\nSHDA-RF and the next best classifier SHFR-ECOC is on\\nan average 7-8% (p-value< 0.05). Handling high dimen-\\nsional sparse data with only a few samples available per class\\nnecessitated the use of dimensionality reduction techniques\\nfor SVM ECOC and SHFR ECOC approaches. This can\\nbe observed by comparing the results of the SVM ECOC\\nand SHFR ECOC models trained and tested on the origi-\\nnal features (Table 3) and on transformed features (Table 2).\\nHowever, the proposed approach does not require such a pre-\\nprocessing step and is able to learn well in the original high\\ndimensional space. The HeMAP approaches attempt to es-\\ntimate a direct mapping between the source and target data.\\nLearning this mapping in the presence of explicit correspon-\\ndence between source and target data is easier than in the gen-\\neral case. As depicted in Table 4, the performance of HeMap\\nsuffers without explicit correspondence between source and\\ntarget data. Even with explicit correspondence between the\\ndata points, the performance of the unsupervised transfer ap-\\nproaches are not at par with the other techniques. SHDA-RF\\n\\nIdentical >=95% >=90% >=85%\\n16\\n\\n18\\n\\n20\\n\\n22\\n\\n24\\n\\n26\\n\\n28\\n\\n30\\n\\n32\\n\\nSimilarity of label distributions−−−−>\\n\\nM\\nea\\n\\nn \\nEr\\n\\nro\\nr−\\n−−\\n\\n>\\n\\n \\n\\n \\n\\nhh102−−>hh113\\nhh113−−>hh118\\nhh118−−>hh102\\n\\nFigure 4: Effect of the number of pivots on the mean error\\n\\nperforms marginally better than BRF on the Statlog dataset.\\nIt is able to significantly outperform the BRF model only in\\none out of the two settings. Though the difference in the per-\\nformances of SVM ECOC and SHFR ECOC is not signifi-\\ncant, however if considered, it performs better than the SHFR\\nECOC transfer model. The smaller margin of improvement\\ncould be attributed to the property of the dataset, which is\\ndense and real-valued.\\n\\nRandom Forest is used as the final model for comparing the\\ndifferent transfer mappings. The performance of the transfer\\nmapping learned from SHFR-ECOC that uses random for-\\nest as the final model (SHFR-RF) is significantly poorer than\\nSHDA-RF on the 20 Newsgroups dataset. On the CASAS-\\nHH datasets, the results are only marginally poorer. Thus\\n\\noverall the results do seem to suggest that the transfer map-\\nping learned through SHDA-RF is better than SHFR-ECOC.\\n\\nFigure 3 presents the results for BRF and SHDA-RF mod-\\nels on a few CASAS-HH datasets with increasing target train-\\ning data. It can be observed that the mean error for both the\\napproaches reduces with increase in the number of labeled\\ntarget domain data. However, the transfer approach performs\\nmarginally better than the baseline when number of target\\ntraining examples is close to 50%.\\n\\nThe SHDA-RF algorithm uses only identical label distri-\\nbutions across the domains as pivots. We conducted experi-\\nments to study the effect of increasing the shared label dis-\\ntributions between the domains by relaxing the similarity be-\\ntween the distributions. We used Jensen-Shannon divergence\\n[Lin, 1991] to determine the similarity between two label dis-\\ntributions. Figure 4 reports the mean error for models with in-\\ncreasing similarity relaxation on three CASAS-HH datasets.\\nIt can be observed that the mean error reduces only till about\\n90% relaxation beyond which the error increases marginally.\\n\\n6 Summary and Future Work\\nIn this paper we present a novel supervised heterogeneous do-\\nmain adaptation technique that learns the mapping between\\nheterogeneous feature spaces of different dimensions. Our\\nalgorithm uses the shared label distributions across the do-\\nmains as the pivots for learning the feature transformation.\\nWe estimate the pivots using random forest models trained\\nboth on source and a small part of target labeled data. The\\nexperiments conducted on diverse datasets suggest the supe-\\nriority of the proposed algorithm over other baseline and fea-\\nture transfer approaches.\\n\\nThe SHDA-RF algorithm establishes the mapping between\\nthe feature spaces. A natural extension will be to consider in-\\nstance transfer approaches to reduce the marginal and condi-\\ntional probability distributions between the target and trans-\\nformed source data. The proposed algorithm utilises a sin-\\ngle source domain for knowledge transfer. Another direction\\nthat we would like to explore is how to effectively combine\\nlabeled data from multiple source domains to make an im-\\nproved final prediction on the target. Finally, variants of ran-\\ndom forests and sampling techniques can be used to improve\\nupon on the random forest model that is the foundation of the\\nSHDA-RF approach.\\n\\nAcknowledgments\\nThe authors are grateful to the anonymous reviewers for their\\nvaluable comments. This research is partially supported by\\nthe ISIRD grant from IIT Ropar.\\n\\n2044\\n\\n\\n\\nReferences\\n[Blitzer et al., 2006] John Blitzer, Ryan McDonald, and Fer-\\n\\nnando Pereira. Domain adaptation with structural corre-\\nspondence learning. In Proceedings of the 2006 Confer-\\nence on Empirical Methods in Natural Language Process-\\ning, pages 120–128, 2006.\\n\\n[Breiman, 2001] Leo Breiman. Random forests. Machine\\nlearning, pages 5–32, 2001.\\n\\n[Cook and Krishnan, 2015] Diane J. Cook and Narayanan C.\\nKrishnan. Activity Learning:Discovering, Recognizing,\\nand Predicting Human Behavior from Sensor Data. John\\nWiley and Sons Inc., 2015.\\n\\n[Cook et al., 2013a] Diane J. Cook, Aaron S. Crandall,\\nBrian L. Thomas, and Narayanan C. Krishnan. Casas: A\\nsmart home in a box. Computer, 46(7):62–69, 2013.\\n\\n[Cook et al., 2013b] Diane J. Cook, Kyle Dillon Feuz, and\\nNarayanan C. Krishnan. Transfer learning for activity\\nrecognition: a survey. Journal of Knowledge and Infor-\\nmation Systems, pages 537–556, 2013.\\n\\n[Cook et al., 2013c] Diane J. Cook, Narayanan C. Krishnan,\\nand Parisa Rashidi. Activity discovery and activity recog-\\nnition: A new partnership. IEEE Transactions on Systems\\nMan and Cybernetics, pages 820–828, 2013.\\n\\n[Feuz and Cook, 2014] Kyle Dillon Feuz and Diane J. Cook.\\nHeterogeneous transfer learning for activity recognition\\nusing heuristic search techniques. International Journal\\nof Pervasive Computing and Communications, pages 393–\\n418, 2014.\\n\\n[Feuz, 2014] Kyle Dillon Feuz. Preparing smart environ-\\nments for life in the wild: Feature-space and Multi-view\\nhetrogeneous learning. PhD thesis, Washington State Uni-\\nversity, 2014.\\n\\n[Hastie et al., 2001] Trevor Hastie, Robert Tibshirani, and\\nJerome Friedman. The Elements of Statistical Learning.\\nSpringer Series in Statistics. 2001.\\n\\n[He et al., 2014] Jingrui He, Yan Liu, and Qiang Yang. Link-\\ning heterogeneous input spaces with pivots for multi-task\\nlearning. In Proceedings of the SIAM International Con-\\nference on Data Mining, pages 181–189, 2014.\\n\\n[Hu and Yang, 2011] Derek Hao Hu and Qiang Yang. Trans-\\nfer learning for activity recognition via sensor mapping. In\\nProceedings of the 22nd International Joint Conference on\\nArtificial Intelligence, pages 1962–1967, 2011.\\n\\n[Hu et al., 2011] Derek Hao Hu, Vincent Wenchen Zheng,\\nand Qiang Yang. Cross-domain activity recognition via\\ntransfer learning. Journal of Pervasive and Mobile Com-\\nputing, pages 344–358, 2011.\\n\\n[Lang, 1995] Ken Lang. Newsweeder: Learning to filter net-\\nnews. In Proceedings of the Twelfth International Confer-\\nence on Machine Learning, pages 331–339, 1995.\\n\\n[Li et al., 2014] Wen Li, Lixin Duan, Dong Xu, and Ivor W.\\nTsang. Learning with augmented features for supervised\\nand semi-supervised heterogeneous domain adaptation.\\nIEEE Transactions on Pattern Analysis and Machine In-\\ntelligence, pages 1134–1148, 2014.\\n\\n[Lichman, 2013] M. Lichman. UCI machine learning repos-\\nitory, 2013.\\n\\n[Lin, 1991] Jianhua Lin. Divergence measures based on the\\nshannon entropy. IEEE Transactions on Information The-\\nory, pages 145–151, 1991.\\n\\n[Pan and Yang, 2010] Sinno Jialin Pan and Qiang Yang. A\\nsurvey on transfer learning. IEEE Transactions on Knowl-\\nedge and Data Engineering, pages 1345–1359, 2010.\\n\\n[Pan, 2010] Jialin Pan. Feature based transfer learning with\\nreal-world applications. PhD thesis, Hong Kong Univer-\\nsity of Science and Technology, 2010.\\n\\n[Rajan and Ghosh, 2004] Suju Rajan and Joydeep Ghosh.\\nAn empirical comparison of hierarchical vs. two-level ap-\\nproaches to multiclass problems. In Multiple Classifier\\nSystems, Lecture Notes in Computer Science, pages 283–\\n292. 2004.\\n\\n[Rashidi and Cook, 2010] Parisa Rashidi and Diane J. Cook.\\nMulti home transfer learning for resident activity discov-\\nery and recognition. In Proceedings of the International\\nWorkshop on Knowledge Discovery from Sensor Data,\\npages 56–63, 2010.\\n\\n[Shi and Yu, 2012] Xiaoxiao Shi and Philip Yu. Dimension-\\nality reduction on heterogeneous feature space. In Pro-\\nceedings of the 12th IEEE International Conference on\\nData Mining, pages 635–644, 2012.\\n\\n[van Kasteren et al., 2010] T. L. M. van Kasteren, G. En-\\nglebienne, and B. J. A. Kröse. Transferring knowledge\\nof activity recognition across sensor networks. In Pro-\\nceedings of the 8th International Conference on Pervasive\\nComputing, pages 283–300, 2010.\\n\\n[Wang and Mahadevan, 2009] Chang Wang and Sridhar Ma-\\nhadevan. Manifold alignment without correspondence. In\\nProceedings of the 21st International Joint Conference on\\nArtificial Intelligence, pages 1273–1278, 2009.\\n\\n[Zhou et al., 2014] Joey Tianyi Zhou, Ivor W. Tsang,\\nSinno Jialin Pan, and Mingkui Tan. Heterogeneous do-\\nmain adaptation for multiple classes. In Proceedings of the\\n17th International Conference on Artificial Intelligence\\nand Statistics, pages 1095–1103, 2014.\\n\\n2045\\n\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "#Testing on a sample PDF file\n",
        "filename = '/Supervised Heterogeneous Domain Adaptation via Random Forests.pdf'\n",
        "extract_text(filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb15d444",
      "metadata": {
        "id": "cb15d444"
      },
      "source": [
        "## If you only want to extract the 'Abstract' from a research paper\n",
        "### Please note that this function might not work correctly for some PDFs.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abf8c8e1",
      "metadata": {
        "id": "abf8c8e1"
      },
      "outputs": [],
      "source": [
        "# Function to extract abstract from PDF\n",
        "def extract_abstract(filename):\n",
        "    # Parse PDF file\n",
        "    parsed = parser.from_file(filename)\n",
        "\n",
        "    # Extract text from parsed data\n",
        "    text = parsed[\"content\"]\n",
        "\n",
        "    # Find introduction section by looking for keywords\n",
        "    abstract_start = text.find(\"Abstract\")\n",
        "    abstract_end_options = [text.find(\"\\n1 \", abstract_start), text.find(\"\\ni \", abstract_start), text.find(\"\\nI \", abstract_start)]\n",
        "    abstract_end = min(pos for pos in abstract_end_options if pos != -1)\n",
        "\n",
        "    # Extract introduction section\n",
        "    abstract = text[abstract_start:abstract_end]\n",
        "\n",
        "    return abstract"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(extract_abstract(filename))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4SwEWYfORiUf",
        "outputId": "18ce4af7-a6b6-4cbf-fa00-7eaad018827f"
      },
      "id": "4SwEWYfORiUf",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Abstract\n",
            "Heterogeneity of features and lack of correspon-\n",
            "dence between data points of different domains are\n",
            "the two primary challenges while performing fea-\n",
            "ture transfer. In this paper, we present a novel su-\n",
            "pervised domain adaptation algorithm (SHDA-RF)\n",
            "that learns the mapping between heterogeneous\n",
            "features of different dimensions. Our algorithm\n",
            "uses the shared label distributions present across\n",
            "the domains as pivots for learning a sparse fea-\n",
            "ture transformation. The shared label distributions\n",
            "and the relationship between the feature spaces and\n",
            "the label distributions are estimated in a supervised\n",
            "manner using random forests. We conduct exten-\n",
            "sive experiments on three diverse datasets of vary-\n",
            "ing dimensions and sparsity to verify the superi-\n",
            "ority of the proposed approach over other baseline\n",
            "and state of the art transfer approaches.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dcd681d9",
      "metadata": {
        "id": "dcd681d9"
      },
      "source": [
        "# Extracting Introduction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1f5c8de",
      "metadata": {
        "id": "e1f5c8de"
      },
      "outputs": [],
      "source": [
        "def extract_intro(filename):\n",
        "    # Parse PDF file\n",
        "    parsed = parser.from_file(filename)\n",
        "\n",
        "    # Extract text from parsed data\n",
        "    text = parsed[\"content\"]\n",
        "\n",
        "    # Find introduction section by looking for the \"1 Introduction\" heading\n",
        "    intro_start = text.find(\"Introduction\")\n",
        "\n",
        "    # Find the next heading that starts with \"2 \", \"ii \", or \"II \"\n",
        "    intro_end_options = [text.find(\"\\n2 \", intro_start), text.find(\"\\nii \", intro_start), text.find(\"\\nII \", intro_start)]\n",
        "    intro_end = min(pos for pos in intro_end_options if pos != -1)\n",
        "\n",
        "    # Extract the introduction section\n",
        "    intro = text[intro_start:intro_end]\n",
        "\n",
        "    return intro"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(extract_abstract(filename))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tyV_F1vGR1KI",
        "outputId": "5b8374c5-f603-4601-f84a-9f1527922822"
      },
      "id": "tyV_F1vGR1KI",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Abstract\n",
            "Heterogeneity of features and lack of correspon-\n",
            "dence between data points of different domains are\n",
            "the two primary challenges while performing fea-\n",
            "ture transfer. In this paper, we present a novel su-\n",
            "pervised domain adaptation algorithm (SHDA-RF)\n",
            "that learns the mapping between heterogeneous\n",
            "features of different dimensions. Our algorithm\n",
            "uses the shared label distributions present across\n",
            "the domains as pivots for learning a sparse fea-\n",
            "ture transformation. The shared label distributions\n",
            "and the relationship between the feature spaces and\n",
            "the label distributions are estimated in a supervised\n",
            "manner using random forests. We conduct exten-\n",
            "sive experiments on three diverse datasets of vary-\n",
            "ing dimensions and sparsity to verify the superi-\n",
            "ority of the proposed approach over other baseline\n",
            "and state of the art transfer approaches.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9fc9167f",
      "metadata": {
        "id": "9fc9167f"
      },
      "source": [
        "# Using NER to extract author names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf35040d",
      "metadata": {
        "id": "bf35040d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a022d34d-1a15-4679-ac96-8c44edf941a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-23.2.1-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (67.7.2)\n",
            "Collecting setuptools\n",
            "  Downloading setuptools-68.2.2-py3-none-any.whl (807 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m807.9/807.9 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (0.41.2)\n",
            "Installing collected packages: setuptools, pip\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 67.7.2\n",
            "    Uninstalling setuptools-67.7.2:\n",
            "      Successfully uninstalled setuptools-67.7.2\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 23.1.2\n",
            "    Uninstalling pip-23.1.2:\n",
            "      Successfully uninstalled pip-23.1.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pip-23.2.1 setuptools-68.2.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "_distutils_hack",
                  "pkg_resources",
                  "setuptools"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.6.1)\n",
            "Collecting spacy\n",
            "  Obtaining dependency information for spacy from https://files.pythonhosted.org/packages/fa/7c/8518799f3fc85e4c2538c0b4fc2306de90696f00c1a8286c943f9414292a/spacy-3.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
            "  Downloading spacy-3.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Collecting weasel<0.4.0,>=0.1.0 (from spacy)\n",
            "  Obtaining dependency information for weasel<0.4.0,>=0.1.0 from https://files.pythonhosted.org/packages/de/f5/6786a5fd1ab6a38511f3772c9002f312a2d509c1237ae514631adf145ad4/weasel-0.3.2-py3-none-any.whl.metadata\n",
            "  Downloading weasel-0.3.2-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (68.2.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.23.5)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.1.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
            "Collecting cloudpathlib<0.16.0,>=0.7.0 (from weasel<0.4.0,>=0.1.0->spacy)\n",
            "  Obtaining dependency information for cloudpathlib<0.16.0,>=0.7.0 from https://files.pythonhosted.org/packages/97/a2/e9a5bd762cccefc92a98c87354a65a8b75c280ab187a05e6d5851adbdae6/cloudpathlib-0.15.1-py3-none-any.whl.metadata\n",
            "  Downloading cloudpathlib-0.15.1-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.3)\n",
            "Downloading spacy-3.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m63.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading weasel-0.3.2-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.0/50.0 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cloudpathlib-0.15.1-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: cloudpathlib, weasel, spacy\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.6.1\n",
            "    Uninstalling spacy-3.6.1:\n",
            "      Successfully uninstalled spacy-3.6.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "en-core-web-sm 3.6.0 requires spacy<3.7.0,>=3.6.0, but you have spacy 3.7.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed cloudpathlib-0.15.1 spacy-3.7.1 weasel-0.3.2\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m2023-10-12 21:22:57.732588: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-10-12 21:22:58.881697: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Collecting en-core-web-sm==3.7.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.0/en_core_web_sm-3.7.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.0) (3.7.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (0.3.2)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (4.66.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (68.2.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (1.23.5)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (0.1.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.16.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (0.15.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (2.1.3)\n",
            "Installing collected packages: en-core-web-sm\n",
            "  Attempting uninstall: en-core-web-sm\n",
            "    Found existing installation: en-core-web-sm 3.6.0\n",
            "    Uninstalling en-core-web-sm-3.6.0:\n",
            "      Successfully uninstalled en-core-web-sm-3.6.0\n",
            "Successfully installed en-core-web-sm-3.7.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ],
      "source": [
        "!pip install -U pip setuptools wheel\n",
        "!pip install -U spacy\n",
        "!python3 -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "beb30d3e",
      "metadata": {
        "id": "beb30d3e"
      },
      "source": [
        "### Via 'spacy'\n",
        "spaCy is a free open-source library for Natural Language Processing in Python. It features NER, POS tagging, dependency parsing, word vectors and more."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8269eeb",
      "metadata": {
        "id": "b8269eeb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9541662f-bbc8-4708-c7b9-bb0b17913b76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install PyPDF2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efe3176c",
      "metadata": {
        "id": "efe3176c"
      },
      "outputs": [],
      "source": [
        "import PyPDF2 #Another text extractor. You can use it when tika doesn't work.\n",
        "import spacy\n",
        "\n",
        "def extract_author_names(filename):\n",
        "\n",
        "    # Load the spaCy NER model\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "    # Open the PDF file\n",
        "    pdf_file = open(filename, 'rb')\n",
        "\n",
        "    # Create a PDF reader object\n",
        "    pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
        "\n",
        "    # Extract the text from the first page\n",
        "    first_page = pdf_reader.pages[0]\n",
        "    first_page_text = first_page.extract_text()\n",
        "\n",
        "    # Close the PDF file\n",
        "    pdf_file.close()\n",
        "\n",
        "    # Apply the spaCy NER model to the text\n",
        "    doc = nlp(first_page_text)\n",
        "    author_names = []\n",
        "    for entity in doc.ents:\n",
        "        if entity.label_ == 'PERSON':\n",
        "            author_names.append(entity.text)\n",
        "\n",
        "    # Return the list of author names\n",
        "    return author_names"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a77b384",
      "metadata": {
        "id": "3a77b384"
      },
      "source": [
        "## Extract 'Abstract', 'Introduction', and 'Author_Names' together"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86a14361",
      "metadata": {
        "id": "86a14361",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d59679b-079d-496e-d29f-60f4f0a3cad2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-10-12 21:23:43.217140: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Collecting en-core-web-sm==3.7.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.0/en_core_web_sm-3.7.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.0) (3.7.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (0.3.2)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (4.66.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (68.2.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (1.23.5)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (0.1.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.16.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (0.15.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (2.1.3)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "!python3 -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7dc42b01",
      "metadata": {
        "id": "7dc42b01",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61006f92-02d9-4d9f-9b2c-9c7607c34c26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-10-12 21:24:02.880283: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Collecting en-core-web-lg==3.7.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.7.0/en_core_web_lg-3.7.0-py3-none-any.whl (587.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.7/587.7 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-lg==3.7.0) (3.7.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-lg==3.7.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-lg==3.7.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-lg==3.7.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-lg==3.7.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-lg==3.7.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-lg==3.7.0) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-lg==3.7.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-lg==3.7.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-lg==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-lg==3.7.0) (0.3.2)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-lg==3.7.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-lg==3.7.0) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-lg==3.7.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-lg==3.7.0) (4.66.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-lg==3.7.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-lg==3.7.0) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-lg==3.7.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-lg==3.7.0) (68.2.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-lg==3.7.0) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-lg==3.7.0) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-lg==3.7.0) (1.23.5)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->en-core-web-lg==3.7.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->en-core-web-lg==3.7.0) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->en-core-web-lg==3.7.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->en-core-web-lg==3.7.0) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->en-core-web-lg==3.7.0) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.0->en-core-web-lg==3.7.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.0->en-core-web-lg==3.7.0) (0.1.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.0->en-core-web-lg==3.7.0) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.16.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.0->en-core-web-lg==3.7.0) (0.15.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->en-core-web-lg==3.7.0) (2.1.3)\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-3.7.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n"
          ]
        }
      ],
      "source": [
        "!python3 -m spacy download en_core_web_lg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04e1e5d2",
      "metadata": {
        "id": "04e1e5d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a09495f-73b7-4eba-939e-e4a1f3210e74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-10-12 21:24:59.888146: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'en' are deprecated. Please use the\n",
            "full pipeline package name 'en_core_web_sm' instead.\u001b[0m\n",
            "Collecting en-core-web-sm==3.7.0\n",
            "  Using cached https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.0/en_core_web_sm-3.7.0-py3-none-any.whl (12.8 MB)\n",
            "Requirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.0) (3.7.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (0.3.2)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (4.66.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (68.2.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (1.23.5)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (0.1.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.16.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (0.15.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (2.1.3)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ],
      "source": [
        "!python3 -m spacy download en"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d69bded",
      "metadata": {
        "id": "3d69bded"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tika\n",
        "from tika import parser\n",
        "\n",
        "#tqdm --> Can help you with the progress bar\n",
        "\n",
        "# Set up Tika\n",
        "tika.initVM()\n",
        "\n",
        "#Testing on a sample PDF file\n",
        "filename = '/Supervised Heterogeneous Domain Adaptation via Random Forests.pdf'\n",
        "\n",
        "# Extract the introduction from the PDF file\n",
        "text = extract_text(filename)\n",
        "intro = extract_intro(filename)\n",
        "abstract = extract_abstract(filename)\n",
        "author_names = extract_author_names(filename)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(abstract)\n",
        "print('-----')\n",
        "print(intro)\n",
        "print('-----')\n",
        "print(author_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KtbngQoLSlSo",
        "outputId": "a8f7fc26-b977-429d-c7c4-fab87a10ef3a"
      },
      "id": "KtbngQoLSlSo",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Abstract\n",
            "Heterogeneity of features and lack of correspon-\n",
            "dence between data points of different domains are\n",
            "the two primary challenges while performing fea-\n",
            "ture transfer. In this paper, we present a novel su-\n",
            "pervised domain adaptation algorithm (SHDA-RF)\n",
            "that learns the mapping between heterogeneous\n",
            "features of different dimensions. Our algorithm\n",
            "uses the shared label distributions present across\n",
            "the domains as pivots for learning a sparse fea-\n",
            "ture transformation. The shared label distributions\n",
            "and the relationship between the feature spaces and\n",
            "the label distributions are estimated in a supervised\n",
            "manner using random forests. We conduct exten-\n",
            "sive experiments on three diverse datasets of vary-\n",
            "ing dimensions and sparsity to verify the superi-\n",
            "ority of the proposed approach over other baseline\n",
            "and state of the art transfer approaches.\n",
            "\n",
            "-----\n",
            "Introduction\n",
            "The key to success of many supervised learning algorithms\n",
            "is the availability of abundant labeled training data. How-\n",
            "ever, for many real-world problems, collecting labeled data\n",
            "is often very expensive and cumbersome. Transfer learning\n",
            "algorithms help to overcome the scarcity of labeled data in\n",
            "a domain (often referred to as the target domain) by util-\n",
            "ising information about the task, and data from single or\n",
            "multiple auxiliary domains (referred to as source domains).\n",
            "Transfer learning approaches have found success in many\n",
            "applications including activity recognition [Hu et al., 2011;\n",
            "Cook et al., 2013c], sentiment classification [Zhou et al.,\n",
            "2014], document analysis and indoor localization [Pan and\n",
            "Yang, 2010].\n",
            "\n",
            "A popular setting for performing transfer is when the\n",
            "source and the target domains are represented by the same\n",
            "set of features. The goal in this setting is to minimise the dif-\n",
            "ferences in the data distribution of the source and target do-\n",
            "mains. However, for applications such as sentiment analysis\n",
            "across different languages [Pan, 2010], and activity recogni-\n",
            "tion across different domains [Cook et al., 2013b], the source\n",
            "\n",
            "⇤The author contributed to this work during his internship at IIT\n",
            "Ropar.\n",
            "\n",
            "and target data are represented using heterogeneous features\n",
            "of different dimensions that may or may not overlap. Transfer\n",
            "learning for such heterogeneous domains can be performed\n",
            "by first bridging the gap between the features characterising\n",
            "the different domains. This is the principle behind feature-\n",
            "based transfer learning approaches.\n",
            "\n",
            "The feature-based transfer approach proposed in this pa-\n",
            "per is motivated by the application of activity recognition in\n",
            "a smart home. Smart home based activity recognition deals\n",
            "with learning the daily activities of smart home resident(s),\n",
            "captured through a series of sensor observations. Transfer\n",
            "learning algorithms can be used to overcome the scarcity of\n",
            "labeled data of a new target smart home by utilising the la-\n",
            "beled data of other source smart homes. However, different\n",
            "layouts and types of sensors deployed at different places lead\n",
            "to heterogeneous feature spaces [Hu and Yang, 2011] neces-\n",
            "sitating transfer methodologies. Figure 1 illustrates the layout\n",
            "and sensor locations for three smart homes from the CASAS\n",
            "datasets [Cook et al., 2013c] used in this paper. Given only\n",
            "a few labeled instances in the target we leverage the common\n",
            "labels in the source and target domains to derive the relation-\n",
            "ship between the corresponding feature spaces. The key as-\n",
            "sumption of our algorithm is that features in both source and\n",
            "target domains that characterise data partitions with similar\n",
            "label distribution, must be related to each other. The shared\n",
            "label distributions across the two domains act as the pivot for\n",
            "learning the mapping between the feature spaces. The gen-\n",
            "erated sparse mapping represents a target feature as a linear\n",
            "combination of source features. This mapping is estimated\n",
            "without assuming any correspondence between source and\n",
            "target data points.\n",
            "\n",
            "1.1 Problem Definition\n",
            "Let {XS , YS}mi=1 and {XT , YT }nj=1 represent the set of la-\n",
            "beled instances in the source domain S and target domain T\n",
            "\n",
            "respectively, where m o n. xS 2 RdS is a source data\n",
            "point with yS 2 Y the corresponding class label. Similarly,\n",
            "xT 2 RdT is a target data point and yT 2 Y is its associated\n",
            "label. The features that describe xS and xT are completely\n",
            "different and d\n",
            "\n",
            "S 6= d\n",
            "\n",
            "T . However, we assume that the source\n",
            "and target domains share a common label space. Let the num-\n",
            "ber of shared labels be k. Our goal is to learn a mapping\n",
            "f : RdS ! RdT such that the data from the source domain\n",
            "can be mapped to the target domain. This mapped source data\n",
            "\n",
            "Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence (IJCAI-16)\n",
            "\n",
            "2039\n",
            "\n",
            "\n",
            "\n",
            "hh102 hh113 hh118\n",
            "\n",
            "Figure 1: Layouts of the three CASAS smart homes that differ in terms the layout, and count of the sensors deployed. The\n",
            "black squares represent the location of sensors.\n",
            "\n",
            "can then be used in conjunction with the target data to learn\n",
            "the hypothesis h : RdT ! Y .\n",
            "\n",
            "1.2 Contributions\n",
            "The contributions of this paper can be summarised as follows:\n",
            "\n",
            "1. The proposed algorithm yields a heterogeneous feature-\n",
            "space class-invariant mapping, assuming no correspon-\n",
            "dence between the data-points of the domains that share\n",
            "no overlapping features.\n",
            "\n",
            "2. Our algorithm does not require the computation of an\n",
            "optimal code matrix for error correcting output code,\n",
            "which is a challenging task, that is a requirement for\n",
            "the current supervised state of the art feature transfer al-\n",
            "gorithm [Zhou et al., 2014]. The proposed algorithm\n",
            "utilises naturally occurring label distributions at leaf\n",
            "nodes of a decision tree model as pivots to generate the\n",
            "mapping PS 2 RdS⇥dT .\n",
            "\n",
            "3. The experiments conducted on diverse datasets indicate\n",
            "the effectiveness of the algorithm even if very few la-\n",
            "beled instances are available in the target domain.\n",
            "\n",
            "-----\n",
            "['Random Forests', 'Narayanan C Krishnan1', 'Huet al.', 'Cook', 'Zhou et al.', 'Yang', 'Pan', 'Cook', 'Hu', 'Yang', 'Cook']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "688e2355",
      "metadata": {
        "id": "688e2355"
      },
      "source": [
        "# Complete Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22c1a894",
      "metadata": {
        "id": "22c1a894"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tika\n",
        "from tika import parser\n",
        "\n",
        "#tqdm --> Can help you with the progress bar\n",
        "\n",
        "# Set up Tika\n",
        "tika.initVM()\n",
        "\n",
        "folder_names = []\n",
        "texts = []\n",
        "filenames = []\n",
        "introduction_vector = []\n",
        "abstract_vector = []\n",
        "author_names_vector = []\n",
        "\n",
        "c = 0 # For printing indexes of the PDF files on which tika couldn't extract the text successfully\n",
        "\n",
        "for root, dirs, files in os.walk('/Dataset-IK/'):\n",
        "     for file in files:\n",
        "        #print(os.path.join(root, file))\n",
        "\n",
        "        c = c + 1\n",
        "\n",
        "        try:\n",
        "            # Extract the introduction from the PDF file\n",
        "            text = extract_text(os.path.join(root, file))\n",
        "            intro = extract_intro(os.path.join(root, file))\n",
        "            abstract = extract_abstract(os.path.join(root, file))\n",
        "            author_names = extract_author_names(os.path.join(root, file))\n",
        "\n",
        "            folder_names.append(root.split('/')[-1])\n",
        "            texts.append(text)\n",
        "            introduction_vector.append(intro)\n",
        "            abstract_vector.append(abstract)\n",
        "            author_names_vector.append(author_names)\n",
        "            filenames.append(file)\n",
        "\n",
        "        except:\n",
        "            print(\"Index for the failed files:\", c)\n",
        "            #introduction_vector.append([])\n",
        "            #abstract_vector.append([])\n",
        "            #author_names_vector.append([])\n",
        "            texts.append([])\n",
        "            filenames.append([])\n",
        "            folder_names.append([])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83de3e35",
      "metadata": {
        "id": "83de3e35"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "df = pd.DataFrame(filenames,columns = ['FileName'])\n",
        "df['Author'] = folder_names\n",
        "df['Text'] = texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "211fedc3",
      "metadata": {
        "id": "211fedc3",
        "outputId": "af981297-63f2-41c2-8150-de9b07eb5f6c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(64, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27fddce3",
      "metadata": {
        "id": "27fddce3",
        "outputId": "33e430d0-165a-4068-f0c0-ebb01f17c80e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                   FileName          Author  \\\n",
              "0               Combating the infodemic.pdf    Arun Chauhan   \n",
              "1             Fighting hate speech from.pdf    Arun Chauhan   \n",
              "2                      Hate or Non-hate.pdf    Arun Chauhan   \n",
              "3  mBERT based model for identification.pdf    Arun Chauhan   \n",
              "4         Towards Next-Generation Alert.pdf  Aruna Malapati   \n",
              "\n",
              "                                                Text  \n",
              "0  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...  \n",
              "1  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...  \n",
              "2  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...  \n",
              "3  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...  \n",
              "4  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9d7158d9-e718-4bda-bd77-7db986f10d55\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>FileName</th>\n",
              "      <th>Author</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Combating the infodemic.pdf</td>\n",
              "      <td>Arun Chauhan</td>\n",
              "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Fighting hate speech from.pdf</td>\n",
              "      <td>Arun Chauhan</td>\n",
              "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Hate or Non-hate.pdf</td>\n",
              "      <td>Arun Chauhan</td>\n",
              "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>mBERT based model for identification.pdf</td>\n",
              "      <td>Arun Chauhan</td>\n",
              "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Towards Next-Generation Alert.pdf</td>\n",
              "      <td>Aruna Malapati</td>\n",
              "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9d7158d9-e718-4bda-bd77-7db986f10d55')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-9d7158d9-e718-4bda-bd77-7db986f10d55 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-9d7158d9-e718-4bda-bd77-7db986f10d55');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-e7847e6a-e48a-4e37-b9b7-8fb3b96e9cc8\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e7847e6a-e48a-4e37-b9b7-8fb3b96e9cc8')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-e7847e6a-e48a-4e37-b9b7-8fb3b96e9cc8 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "508bddea",
      "metadata": {
        "id": "508bddea",
        "outputId": "b1f404ec-f0d6-4dbd-adf4-4105566802f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCombating the infodemic: COVID-19 induced fake news recognition in social media networks\\n\\n\\nComplex & Intelligent Systems\\nhttps://doi.org/10.1007/s40747-022-00672-2\\n\\nORIG INAL ART ICLE\\n\\nCombating the infodemic: COVID-19 induced fake news recognition in\\nsocial media networks\\n\\nShankar Biradar1 · Sunil Saumya1 · Arun Chauhan2\\n\\nReceived: 3 June 2021 / Accepted: 28 January 2022\\n© The Author(s) 2022\\n\\nAbstract\\nCOVID-19 has caused havoc globally due to its transmission pace among the inhabitants and prolific rise in the number of\\npeople contracting the disease worldwide. As a result, the number of people seeking information about the epidemic via\\nInternet media has increased. The impact of the hysteria that has prevailed makes people believe and share everything related\\nto illness without questioning its truthfulness. As a result, it has amplified the misinformation spread on social media networks\\nabout the disease. Today, there is an immediate need to restrict disseminating false news, even more than ever before. This\\npaper presents an early fusion-based method for combining key features extracted from context-based embeddings such\\nas BERT, XLNet, and ELMo to enhance context and semantic information collection from social media posts and achieve\\nhigher accuracy for false news identification. From the observation, we found that the proposed early fusion-based method\\noutperforms models that work on single embeddings. We also conducted detailed studies using several machine learning and\\ndeep learning models to classify misinformation on social media platforms relevant to COVID-19. To facilitate our work, we\\nhave utilized the dataset of “CONSTRAINT shared task 2021”. Our research has shown that language and ensemble models\\nare well adapted to this role, with a 97% accuracy.\\n\\nKeywords Social networks · Fake news · COVID-19 · Machine learning · Voting classifier · Contextual embedding\\n\\nIntroduction\\n\\nBecause of digitization, the Internet has become an integral\\npart of our daily lives in recent years. The latest survey reveals\\nthat more than 3.8 million people use social media; this is\\nmore than half of the world population1. These social media\\nnetworks have given us several benefits, such as enable indi-\\nviduals to produce and share their content, quick and easy\\nconnectivity, brand ads, customer reviews, etc. However, it\\n\\nB Shankar Biradar\\nShankar@iiitdwd.ac.in\\n\\nSunil Saumya\\nsunil.saumya@iiitdwd.ac.in\\n\\nArun Chauhan\\naruntakhur@gmail.com\\n\\n1 Department of Computer Science and Engineering, Indian\\nInstitute of Information Technology Dharwad, Dharwad,\\nKarnataka, India\\n\\n2 Department of Computer Science and Engineering, Graphic\\nEra University Dehradun, Dehradun, India\\n\\n1 https://www.broadbandsearch.net/.\\n\\nstill hasmanydrawbacks, one ofwhich being fake news. Fake\\n(or false) news is defined as news reports that are intention-\\nally, verifiably inaccurate and have the potential to deceive\\nreaders [1]. The impact of the false news on society is defini-\\ntive, direct proof of this being the U.S. election in 2016.\\nDuring the critical months of the U.S. presidential election\\ncampaign, a total of 8,711,000 Facebook posts were discov-\\nered from the top 20 often argued fake election reports, which\\nwas more than 7,367,000 posts from the top 20 most debated\\nelections shared on 19 major news websites [41]. Fake news\\nalso involves violent events in the real world, which jeop-\\nardize community peace [15]. Conspiracy theories mainly\\ndevelop during crises in public health, and cause tremendous\\neffects on life. During the 2019 Ebola outbreak in the Demo-\\ncratic Republic of the Congo (DR Congo), misinformation\\nwas led to violence, suspicion, social upheavals, and targeted\\nattacks on healthcare providers [9]. During the 2002–2003\\nSARS epidemic, uncertainty and anxiety about the disease\\ntriggered social stigma toward Asians. Even during the cur-\\nrent COVID-19 outbreak, research in the American Journal\\nof Tropical Medicine and Hygiene found that approximately\\n5,800 patients have been hospitalized due to inaccurate social\\n\\n123\\n\\nhttp://crossmark.crossref.org/dialog/?doi=10.1007/s40747-022-00672-2&domain=pdf\\nhttps://orcid.org/0000-0003-4869-5860\\nhttps://orcid.org/0000-0002-9555-9434\\nhttps://orcid.org/0000-0002-0327-7254\\nhttps://www.broadbandsearch.net/\\n\\n\\nComplex & Intelligent Systems\\n\\nmedia information. In addition, many people died of drink-\\ning methanol or cleaning drugs dependent on alcohol. They\\nwere misinformed that these products are the treatment for\\nthe virus [14].\\n\\nThere is a sense of urgency within the scientific com-\\nmunity to address fake news on social media. Traditionally,\\nfake news was identified by experts such as journalists2.\\nTheir primary job is to review allegations against facts based\\non publicly discussed or published evidence. However, it is\\ntime-consuming and costly; furthermore, fake news spreads\\nrapidly, and traditional approaches of dealing with it are\\nineffective. This is why, the artificial intelligence research\\ncommunity is interested in automating the identification of\\nfake news on the Internet. The aim of automatic fake news\\nidentification is to save humans’ time and effort. However,\\nit faces numerous challenges, which we have summarized\\nbelow.\\n\\n1. Data from social media are abundant and growing, but its\\nnon-structural, incomplete, and noisy nature makes pro-\\ncessing and understanding extremely difficult [5].\\n\\n2. Because fake news items are meant to deceive read-\\ners rather than provide objective claims, state-of-the-art\\nmethods for identifying false content fall short of better\\naccuracy [5].\\n\\n3. Fake news may cover a variety of issues such as politics,\\nfinance, religion, and health. It can also use various lin-\\nguistic styles; thus, features that distinguish one type of\\nfake news may not work well for another.\\n\\nWith the automatic false news identification problem inmind,\\nsignificant efforts have been undertaken to address these dif-\\nficulties from various viewpoints. Some of these tactics are\\ndiscussed in the next section.\\n\\nOur research on social media material is primarily lim-\\nited to the subject of COVID-19, since the pandemic created\\nan immediate need to build resources to protect against\\ndisinformation dissemination. This paper performs detailed\\nstudies utilizing machine learning models, such as tradi-\\ntional machine learning models, neural network models, and\\nspecific state-of-the-art transfer learning models for false\\ninformation classification in the COVID-19 data set.\\n\\nThe main contribution of the paper are:\\n\\n1. To develop an early fusion-based approach for combining\\nkey features extracted from context-based embeddings to\\nimprove the efficacy of fake news identification.\\n\\n2. To develop various Recurrent Neural Network models,\\nsuch as LSTM, BiLSTM, GRU, and BiGRU, with pre-\\ntrained BERT embeddings for fake news identification.\\nIn addition, their ensemble settings are also explored.\\n\\n2 https://www.politifact.com/.\\n\\n3. To develop a hybrid voting classifier by integrating the\\nresults of conventional machine learning algorithms and\\nlanguage models.\\n\\n4. To develop amulti-level bit-wise operator-based classifier\\nfor fake news identification.\\n\\nThe rest of the article is arranged as follows. The specifics\\nof past work done are given in Sect. 2. In Sect. 3, the imple-\\nmentation specifics are outlined.Wepresented our results and\\ndiscussions in detail in Sects. 4 and 5. Subsequently, Sect. 6\\noffers the conclusion and scope for the future.\\n\\nRelated works\\n\\nThe ever-increasing attraction and beauty of using social\\nnetworks directly or indirectly influence our everyday lives.\\nTherefore, it is not shocking that social networking has been\\na platform to exploit emotions through disseminating mis-\\ninformation according to patterns. The adverse use of these\\nchannels was primarily used to spread inaccurate or unclear,\\ncommunal, financial, and political information3. For exam-\\nple, false news research on Twitter in a Boston attack reveals\\nthat scare peddlers successfully manipulate social media to\\ncausemass hysteria and panic [11]. Furthermore, distributing\\ndisinformation may have a detrimental effect on individuals\\nand the communities they work in; it can instill fear and\\naffect their emotional reactions to elections and natural dis-\\nasters [3,18]. The spread of misleading vaccine information\\nhas been one such example, indicating that many parents\\nhave spread fake news concerning the vaccine’s safety. Con-\\nsequently, some young children’s parents have criticized the\\npediatric advice and declined to vaccinate or immunize their\\nchildren [20]. As a result, there has been an alarming increase\\nin sickness that could have been avoided.\\n\\nIn recent years, the research community has been explor-\\ning the driving force behind the spread of false information\\non incidents such as the pandemic COVID-19 or policy sce-\\nnarios [26,36] has uncovered early social media campaigns\\non political, religious, and economic propaganda. Results\\nrevealed that hacked identities (using someone’s personal\\ninformation and pictures to create fake profiles and use them\\nto spread fake information [30,31]) are used to promote mis-\\ninformation and can also be used to propagate propaganda\\n[7] attempted to track the dissemination of scientific misin-\\nformation, which concluded that most people found details\\nreliable, because their friends also tweeted.Another potential\\ncause is that people want to share new information [38].\\n\\n3 http://www3.weforum.org/docs/GRR17_Report_web.pdf.\\n\\n123\\n\\nhttps://www.politifact.com/\\nhttp://www3.weforum.org/docs/GRR17_Report_web.pdf\\n\\n\\nComplex & Intelligent Systems\\n\\nFake content detection in various scenarios\\n\\nAvoiding the spread of fake news has been a serious concern\\nfor the scientific community. Recently, a significant amount\\nof research has been conducted to identify fake information\\non social media. Fake news identification methods are typ-\\nically classified into four classes: (1) news content-based;\\n(2) social background-based [34,43]; (3) propagation-based;\\nand (4) information-based.\\n\\nSocial background-based methods incorporate elements\\nfrom social media user accounts and message content [32]\\nattempted to construct interaction networks that represent the\\ninterconnections between various entities such as publish-\\ners (person who post the news), news articles, and users to\\nperform fake news detection [6] proven that attributes gath-\\nered from user posting and re-posting behaviors will help in\\ndetermining the reliability of the information [35] tried to\\ncombine post-content with social context information such\\nas relationships among the publishers and users.\\n\\nPropagation-based models study the fake news spreading\\npattern in Social Media Networks. [19] have extracted infor-\\nmation from social networks (structural properties of fake\\nnews spreading) to identify fake news; they built a friendship\\nnetwork for this purpose. [28] built an RNN-based frame-\\nwork consisting of three models: the first model captures the\\ntemporal patterns of user activity on a given post. The second\\nmodule learns the source characteristic from user behavior,\\nand the third module integrates the two [33] have developed\\nhierarchical dissemination networks for false news and real\\nnews and then conducted a comparative review of structural,\\ntemporal, and linguistic networking features for the perspec-\\ntive on recognizing fake news. The majority of these models\\ncan be grouped as positions or propagation-based. However,\\nthe issue with these approaches is the early identification and\\nprevention of the dissemination of fake news.\\n\\nSome analysis experiments have been conducted to estab-\\nlish the basis for false news studies with information base\\nand knowledge diagrams, such as DBpedia and Knowledge\\nVault [46]. They depend on an established information base\\nthat includes “common knowledge” to equate news articles\\nwith the truthfulness of the news. However, recently emerg-\\ning methods often produce new and unforeseen information,\\nwhich is not contained in the current knowledge bases or\\nknowledge diagrams. Therefore, such approaches are often\\nincapable of dealing with news stories [46]. One more alter-\\nnative is to use a credibility-based approach; these methods\\nrequire external content, such as source information and news\\ncomments, to determine false news, which may not always\\nbe available, especially if the item is spread via social media\\n[46].\\n\\nContent-based approaches extract various features from\\nnews content and are better adapted for early fake news detec-\\ntion. [21] used a content-based machine learning approach;\\n\\nthe author employed two types of machine learning mod-\\nels. The first is the word-bag model, made more robust by\\nstacking two layers of ensemble learning models. Second,\\nneural networks that use pre-trained GloVe Word embed-\\ndings, including (a) a one-dimensional convolutional neural\\nnetwork (CNN) and (b) a bi-directional long short-termmem-\\nory network (BiLSTM); [27] tried to detect fake news by\\nconstructing deep semi-supervised two-way CNN.Machine-\\ngenerated fake news detection was explored by [45]. More\\nrecently, [40,44] have tried to combine user comment emo-\\ntion with post-content to improve the accuracy of prediction.\\nFinally, [17] proposed amulti-layer dense neural network for\\nidentifying fake news in Urdu news articles.\\n\\nFake content detection on COVID-19 data\\n\\nIn the previous research, the primary subject was politi-\\ncal and communal fake news spread through social media.\\nHowever, very few studies have concentrated on spread of\\ndisinformation, linked mainly to COVID-19. [10] tried to\\ncombine topical Dirichlet Allocation (LDA) with contextu-\\nalized representations from XLNet for the task of fake news\\nidentification related to COVID-19. [8] achieved 95% accu-\\nracy by applying standard machine learning algorithms on\\nmany linguistic features such as n-grams, readability, emo-\\ntional tone, and punctuation for the COVID data set. [24]\\npresented CTF, a large-scale COVID-19 Twitter dataset with\\ntagged real and fake tweets. Cross-SEAN, a semi-supervised\\nend-to-end neural attention model based on cross-stitching,\\nwas also proposed. They obtained an F1 score of 0.95% per-\\ncent on the CTF data set. The usefulness and effectiveness of\\npre-trained Transformer-based language models in retriev-\\ning and classifying fake news in a specialized domain of\\nCOVID-19were demonstrated byVijjali et al. [37]. They also\\nconcluded that the suggested two-stage model outperforms\\nother baseline NLP techniques. Gupta et al. [12] developed\\na model to detect fake news about COVID-19 in English\\ntweets. They attain a 0.93 F1 score on the English fake news\\ndetection challenge by combining entity information taken\\nfrom tweets with textual representations learned through\\nword embeddings. Finding fake news related to COVID-19,\\non the other hand, is a “need of the hour” during this pan-\\ndemic, and a lot of research needs to be conducted on this\\ntopic.\\n\\nMethodology\\n\\nThis section explains the different learningmodels developed\\nand experimented for COVID-19 fake news identification.\\nWefirst discuss the data set used in the study, and later, its pre-\\ntreatment and different fake news classifiers are explained in\\ndetail.\\n\\n123\\n\\n\\n\\nComplex & Intelligent Systems\\n\\nDataset description\\n\\nWe have used the dataset released in the “CONSTRAINT\\nshared task-2021” for the current study [25]. The dataset\\nincluded real news as well as fake news regarding the\\nCOVID-19 issue in social media. The total number of real\\nnews samples are 5600, and fake news samples are 5100.\\nThe data set has two fields: tweet and label, and it is balanced\\noverall. Fake news were gathered from numerous web pages,\\ne.g., Politifact1, NewsChecker2, Boomlive3, etc., using tools\\nsuch as IFCN chatbot5 and Google Reality Search Explore4.\\nIn addition, the real news were gathered from Twitter using\\nverified Twitter handles [25]. The detailed distribution of the\\ndataset is shown in Table 1, while sample sentences from the\\ndataset are shown in Table 2.\\n\\nData pre-treatment\\n\\nA few data pre-treatment steps were taken on both text and\\nlabel fields to process the data for model training. The textual\\ncorpus included URLs, hyperlinks, figures, stop words, and\\ncapital letters. Various pre-processing exercises were per-\\nformed to simplify details, such as replacing punctuation\\nwith white spaces, removing the URLs, and Twitter user han-\\ndles that did not help to identify fake news. Converting texts\\n\\nTable 1 Dataset distribution\\n\\nSplit Real Fake Total\\n\\nTrain 3360 3060 6420\\n\\nValidation 1120 1020 2140\\n\\nTest 1120 1020 2140\\n\\nTotal 5600 5100 10700\\n\\nTable 2 Sample sentences from dataset\\n\\nSentence Label\\n\\nThe CDC currently reports 99031 deaths. Real\\n\\nIn general, the discrepancies in death counts between\\ndifferent\\n\\nsources are small and explicable. The death toll stands at\\nroughly\\n\\n100000 people today.\\n\\nCDC Recommends Mothers Stop Breastfeeding To Fake\\n\\nBoost Vaccine Efficacy\\n\\nThe WHO confirmed asymptomatic persons can’t Fake\\n\\ntransmit the coronavirus and are not infectious\\n\\nThe confirmation earlier today of a second death Real\\n\\nlinked to COVID-19 in the last two days means the number\\nof\\n\\nCOVID-19-related deaths in New Zealand are now 24.\\n\\ninto the lower case was also done to remove the redundant\\ntokens. Furthermore, the data lemmatization was carried out\\nto translate the term into its useful basic form, such as the\\nwords runs, running, and ran are all mapped to their root\\nword run. The tokenizer was then fed with pre-processed\\ntext to convert each tweet into the number of tokens where\\neach word in the tweet was assumed to be a single token. In\\ncombination with tokenization, padding and masking for the\\nvariable-length phrases were also performed. The padding\\nensures that all the input sequence data have the same length\\nby padding or truncating the input data points. The masking\\nremoves the importance of padded tokens during model con-\\nstruction by setting the padded value to zero. Our model used\\nBi-directional EncoderRepresentations (BERT) and general-\\nized autoregressive pre-training method (XLNet) tokenizers.\\n\\nEmbedding\\n\\nWe used various contextual embedding techniques such\\nas DistilBERT, a generalized autoregressive pre-training\\nmethod (XLNet), and Embeddings from Language Models\\n(ELMo) to preserve the sentence context. Three different\\nembedding were used, because we wanted to test the per-\\nformance of our proposed models on different context-based\\nembeddings. The DistilBERT is pre-trained as a smaller\\ngeneral-purpose language model, reducing the complexity\\nof the BERT model by 40% while maintaining 97% of their\\nlanguage comprehension and 60% faster [29]. DistillBERT’s\\ndesign is similar to the original BERT and comprises 12\\nself-attention heads and 12 transformer blocks with a hid-\\nden layer size of 768. We only drew the special [CLS] token\\npresent at the start of the DistilBERT model for classifica-\\ntion, which provides full-sentence embedding. The XLNet\\nis another model based on a transformer that uses autore-\\ngression and autoencoding techniques. Like BERT, it has\\n12 layers, 12 attention heads with 768 hidden layers, and\\nis trained on 110 million parameters. However, the [CLS]\\ntoken is present at last in this model, which gives the full-\\nsentence embedding [42]. ELMo is one more state-of-the-art\\npre-trained contextual-based embedding technique that uses\\na deep, bi-directional LSTM model to create word repre-\\nsentations. It generates embeddings of dimension 1024 [23].\\nThe embedding representations used by ELMo are character-\\nistics of the complete input sentence. They are computed as\\na linear function of the internal network states on top of two-\\nlayer BiLSTM with character convolutions. ELMo employs\\nthe semi-supervised learning principle. The most fundamen-\\ntal advantage of using pre-trained embeddings like BERT,\\nXLNet, and ELMo in our proposed model is that they are\\ntrained on a massive corpus and works well for smaller train-\\ning data set. Therefore, we tried to evaluate the performance\\nof differentmachine learning and deep learningmodels using\\npre-trained embeddings.\\n\\n123\\n\\n\\n\\nComplex & Intelligent Systems\\n\\nClassification\\n\\nThemain objective of thiswork is to construct a classifier that\\nseparates our corpus into a real or fake class. Severalmachine\\nlearning models, neural network models, and a few language\\nmodels were used for this purpose. We have broadly cate-\\ngorized our proposed models into four different categories:\\n(1) Early fusion-based DNN model, (2) RNN-based ensem-\\nble model, (3) Voting classifier model, and (4) Multi-level\\nbit-wise operator model. The reason behind using the afore-\\nmentioned model in our proposed approach is that in our\\nexperiments, these models performed better than the other\\nmodels, such as traditionalmachine learning and simple feed-\\nforward neural network models, and the results of those are\\nshown in Table 4.\\n\\nIn Model 1, we used an early fusion-based approach to\\ncombine key features from input sentence embeddings. This\\nmodel used three parallel sub-models to extract features from\\nembeddings such as BERT, XLNet, and ELMo. Initially,\\ninput from the embedding layer is passed through Dense lay-\\ners of sizes 1000, 500, and 100 for BERT, XLNet, and 2000,\\n1000, and 100 for ELMo to extract key features from the\\nembedding vector. Then, batch normalization and a dropout\\nrate of 0.4 are added to prevent overfitting issues. The grid\\nsearch is used to find the optimal hyperparameter value of\\ndropout that results in the most accurate prediction. We per-\\nformed a grid search with a dropout rate range of 0.1–0.8 and\\ndiscovered that a dropout rate of 0.4 produces better results in\\nour suggestedmodel. Next, the output of these dropout layers\\nis concatenated using the concatenation layer. The concate-\\nnated output is then passed through several dense layers with\\ndropout before being used for classification in the sigmoid\\nlayer. The detailed architecture ofModel 1 is presented inFig.\\n1, and the pseudocode for the same is given in Algorithm 1.\\nAll these models were built using Python’s sci-kit-learn4 and\\ntransformer library5.\\n\\nAlgorithm 1 : Early fusion-based approach (Model 1) for\\ncombining embeddings to predict fake news and real news.\\nInput: embedding vectors ’ E’ from BERT, XLNet, and ELMo.\\nOutput: Decision in the form of value.\\n‘E ′ ← embeddingvectors.\\n‘Er ′ ← dimensinali t y − reduced − vectors(‘E ′).\\n‘Econcat ′ ← concatenated − vectors(‘Er ′).\\n‘Featues′ ← concatenated − vectors − Dense(Econcat).\\n‘Value′ ← predict − using − sigmoid(Featues).\\nif sigmoid(’Value’) ≥ 0.5 then\\n\\n‘Pred ′ ← “ f ake − news − content ′′\\nelse\\n\\n‘Pred ′ ← “real − news − content ′′\\nend if\\n\\n4 https://scikit-learn.org/stable/.\\n5 https://huggingface.co/docs/transformers/index.\\n\\nBERT ELMO XLNET\\n\\nDense(1000)\\n\\nConcatenation Layer\\n\\nSigmoid\\n\\nFake\\n\\nData Preprocessing\\n\\n786 * 10700 1024 * 10700 786 * 10700\\n\\nDense(500)\\n\\nDense(100)\\n\\nDropout(0.4)\\n\\nDense(2000)\\n\\nDense(1000)\\n\\nDense(100)\\n\\nDropout(0.4)\\n\\nDense(1000)\\n\\nDense(500)\\n\\nDense(100)\\n\\nDropout(0.4)\\n\\nDense(200)\\n\\nDense(100)\\n\\nDense(50)\\n\\nDropout(0.4)\\n\\nReal\\n\\nE\\nm\\n\\nbe\\ndd\\n\\nin\\ng\\n\\nC\\nla\\n\\nss\\nifi\\n\\nca\\ntio\\n\\nn\\n\\nCovid-19\\nnews\\n\\nFig. 1 Early fusion-based DNN model architecture\\n\\nIn Model 2, multiple RNN variants such as LSTM, BiL-\\nSTM, GRU, and BiGRU with two stacked layers of 60 and\\n30 neurons were developed on the features extracted from\\nBERT embeddings. The LSTM model is used in the pro-\\nposed approach to resolve the vanishing gradient problem of\\nRNNmodels.Wealso experimentedwith bi-directionalmod-\\nels such as BiLSTM and BiGRU in our suggested approach\\nto explore information in both directions, as the prediction\\nof the current word is sometimes dependent on the presence\\nof next and previous words. Furthermore, GRU was chosen,\\nbecause it has a lower computational cost than LSTM. We\\nused the Adam optimizer with a loss function of ’binary-\\ncross entropy’ and a dropout value of 0.2 in our experiment.\\nConstant values of 60 and30neurons are selected fromexper-\\nimental trials, whereas dropout 0.2 is determined using a grid\\nsearch on a dropout rate range of 0.1–0.8. In our experiment,\\nwe used the Adam optimizer, because it converges faster than\\nother optimizers and performs better on noisy social media\\ndata. We also experimented with the ensembled setting of\\n\\n123\\n\\nhttps://scikit-learn.org/stable/\\nhttps://huggingface.co/docs/transformers/index\\n\\n\\nComplex & Intelligent Systems\\n\\nbase learner models, wherein the probability values of all the\\nbase learner’s predictions are stacked into a list. The prob-\\nability values of base learners’ predictions were then added\\nup and divided by the number of base Learners to get the\\naverage prediction value. The final prediction is obtained by\\ncomparing the average prediction to a threshold value of 0.5.\\nFigure 2 illustrates the Model 2 architecture in detail. The\\nmodel was developed using Python’s sci-kit-learn library.\\n\\nIn Model 3, We attempted to combine the results of tra-\\nditional machine learning algorithms with state-of-the-art\\nlanguage model classifiers such as BERT and ULMFit by\\nemploying the voting classifier. To build BERT and ULM-\\nFit model, we have used simple classification API provided\\nby the developers. Bert-base-uncased and ASGD Weight-\\nDropped LSTM (AWD-LSTM) is our underlying option\\nmodel for BERT andULMFit, respectively. AWD-LSTM is a\\nstate-of-the-art language model composed of regular LSTM\\nwith no attention. AWD-LSTM consists of three stages: LM\\npre-training in which a languagemodel is trainedwith a large\\nwikitext-103 corpus to capture general features of language.\\nLM fine-tuning in which the model is fine-tuned to the tar-\\nget task data. Finally, the classifier is fine-tuned using the\\nBPT3C language model [13]. In our proposed model ini-\\ntially, the base learners’ binary prediction values are stacked\\ninto a single list, and the final outcome is selected based on\\nthe majority prediction value. In this approach, we employed\\n“hard voting” to divide the corpus into Real and Fake cate-\\ngories. For example, if predictions from base learners are [1\\n1 0], then the final prediction based on majority count will\\nbe 1. The architecture of model 3 is depicted in Fig. 3, and\\npseudocode for the voting classifier model is given in Algo-\\nrithm 2. The huggingface6 library in Python is used to build\\nthe BERT model, while the fastai7 library is used to create\\nthe ULMFit model.\\n\\nAlgorithm 2 : Voting classifier-based approach (Model 3)\\nfor predicting fake news and real news.\\nInput: List of predictions from the classifier.\\nOutput: Decision in the form of value.\\nif count-of-real(Pred-list)> count-of-fake(Pred-list) then\\n\\n‘Majori ty − pred ′ ← 0\\nelse\\n\\n‘Majori ty − pred ′ ← 1\\nend if\\nif ‘Majori ty − pred ′== 1 then\\n\\n‘Pred ′ ← “ f alse − content ′′\\nelse\\n\\n‘Pred ′ ← “Real − content ′′\\nend if\\n\\n6 https://huggingface.co/docs/transformers/index.\\n7 https://docs.fast.ai/tutorial.text.html.\\n\\nData Preprocessing\\n\\nBERT Embedding\\n\\nLSTM(60) BILSTM(60) GRU(60)\\n\\nLSTM(30) BILSTM(30) GRU(30)\\n\\nDense Dense Dense Dense\\n\\nSigmoid Sigmoid Sigmoid Sigmoid\\n\\nBIGRU(30)\\n\\nAvg(Pred)\\n<\\n\\n0.5\\n\\nReal Fake\\n\\nPred-1 Pred-2 Pred-3 Pred-4\\nStacking Layer\\n\\nBIGRU(60)\\n\\nCovid 19\\nnews\\n\\nFig. 2 RNNs’ ensemble model architecture\\n\\nCovid-19\\nnews\\n\\nDATA\\nPREPROCESSING\\n\\nULMFit \\nClassifier\\n\\nBERT \\nClassifier\\n\\nEMBEDDING\\n(BERT)\\n\\nConventional\\nClassifier\\n\\nStacking Layer\\n\\nReal count > \\nFake count\\n\\nReal Fake\\n\\nFig. 3 Voting classifier architecture\\n\\nFinally, inModel 4, we experimentedwith a bit-wise ’OR’\\noperator to combine the results of traditional ML models\\nand language models. We used a multi-level ’OR’ing opera-\\ntion in this approach. Initial findings from base learners LR,\\nBERT, SVM, and KNN are binary predictions like as 1s and\\n0s, which are then merged using the ’OR’ operator to pro-\\n\\n123\\n\\nhttps://huggingface.co/docs/transformers/index\\nhttps://docs.fast.ai/tutorial.text.html\\n\\n\\nComplex & Intelligent Systems\\n\\nvide intermediate results ’R1’ and ’R2’. These intermediate\\noutcomes are later combined to obtain the final prediction\\nusing one more ’OR’ operator. For example, if base learner\\npredictions are (1 0 0 1), the final prediction after passing\\nthrough the ’OR’ operator is 1, wherein 1 represents fake\\nand 0 means real (Fig. 4). The reason for adopting the afore-\\nmentioned models as base learners is because BERT is the\\nhighest performing model among language models, and LR,\\nSVM, and KNN perform better among traditional machine\\nlearning models, as illustrated in Tables 4 and 7.\\n\\nAlgorithm 3 : Algorithm for multi-level bit-wise operator\\nmodel (Model 4).\\nInput: List of predictions[P1,P2,P3,P4] from the classifier.\\nOutput: Decision in the form of value.\\n‘R1′ ←P1‘OR′P2.\\n‘R2′ ←P3‘OR′P4.\\n‘Value′ ←R1‘OR′R2.\\nif ‘Value′== 1 then\\n\\n‘Pred ′ ← “ f alse − content ′′\\nelse\\n\\n‘Pred ′ ← “Real − content ′′\\nend if\\n\\nFig. 4 Multi-level bit-wise OR model architecture\\n\\nTable 3 Parameters for ML models\\n\\nClassifier Hyperparameter\\n\\nLogistic regression C=1, max-iter=500\\n\\nRandom forest no-of-estimators=200\\n\\nNaive bayes var-smoothing=1e-09\\n\\nSupport vector machine c=1, solver=‘lbfgs’\\n\\nK nearest neighbors n-neighbors=24\\n\\nOptimizers and loss functions\\n\\nWe used Adam optimizer in our all proposed neural network\\nmodels as it incorporates the sparse gradient benefits of Ada-\\nGrad with the ability of RMSProp to solve non-stationary\\nobjectives. In addition, Adam is stable and well suited to\\nvarious problems [16]. Since the current study deals with a\\nbinary classification problem, we used binary cross-entropy\\nas a loss function.\\n\\nResults\\n\\nLet ‘S’ be the set of social media posts. For each social media\\npost ‘P’, (∀P ∈ S), we have two independent sets: training\\nset {Sx,Sy} and testing set {Sp,Sq}.We aim to apply informa-\\ntion gained from the training set {Sx,Sy} to conduct effective\\nfake news detection on the corresponding test set {Sp,Sq}.\\nWe tested our models with two matrices: Accuracy and F1\\nscore. Accuracy can provide a measure of all accurate clas-\\nsifications, but we also interested to know the measure of\\nincorrect classifications to analyze the performance of our\\nmodel on both fake and non-fake data; hence, we used both\\nF1 score and accuracy as evaluationmatrices. Themathemat-\\nical expression for aforementioned matrices are as follows:\\n\\nPrecision (P) = W/W+X\\nRecall (R) = W/W+Y\\nAccuracy = W+Z/N, where N=W+X+Y+Z > 0\\nF-1 Score = 2*P*R/P+R,\\nwhere ‘W’ represents the true-positive score (TP), ‘X’\\n\\nrepresents false-positive score (FP), ‘Y’ represents false-\\nnegative score (FN), and ‘Z’ represents true-negative score\\n(TN) respectively.\\n\\nInitial experimental results\\n\\nInitial experiments were performed using conventional\\nmachine learning algorithms, such as Logistic Regression,\\nRandom Forest, Naive Bayes, Support Vector Machine, and\\nK nearest neighbors (tested with 24 different values for n-\\nneighbors and then developed our grid search according\\nto that value). Table 3 presents the parameter used during\\nthe training for the aforementioned classifiers. These classi-\\n\\n123\\n\\n\\n\\nComplex & Intelligent Systems\\n\\nFig. 5 Comparative analysis of\\nML algorithms on different\\nembedding\\n\\nTable 4 Test set results of\\ntraditional algorithms on\\ndifferent embedding\\n\\nBERT Embedding XLNet Embedding ELMo Embedding\\n\\nClassifier Acc F1-score Acc F1-score Acc F1-score\\n\\n0.92-Fake 0.82-Fake 0.90-Fake\\n\\nLR 0.9257 0.8313 0.9038\\n\\n0.93-Real 0.84-Real 0.90-Real\\n\\n0.87-Fake 0.78-Fake 0.71-Fake\\n\\nNB 0.8759 0.7886 0.70166\\n\\n0.88-Real 0.79-Real 0.69-Real\\n\\n0.92-Fake 0.79-Fake 0.90-Fake\\n\\nSVM 0.9241 0.8053 0.9044\\n\\n0.93-Real 0.83-Real 0.90-Real\\n\\n0.91-Fake 0.82-Fake 0.87-Fake\\n\\nRF 0.9123 0.8273 0.8694\\n\\n0.91-Real 0.84-Real 0.86-Real\\n\\n0.91-Fake 0.79-Fake 0.86-Fake\\n\\nKNN 0.9143 0.8012 0.88\\n\\n0.91-Real 0.81-Real 0.87-Real\\n\\n0.93-Fake 0.84-Fake 0.87-Fake\\n\\nENSEMBLE 0.93 0.85 0.88\\n\\n0.93-Real 0.85-Real 0.88-Real\\n\\nfiers have been investigated using BERT, XLNet, and ELMo\\nembeddings. The results obtained from these models are\\nillustrated, as shown in Fig. 5. These results are also sum-\\nmarized in Table 4. It can be observed from Table 4 that\\nLR achieves the highest accuracy of 92.57% and 83.13%\\nfor BERT, and XLNet embeddings and SVM outperformed\\nother models with 90.44% accuracy for ELMO embeddings.\\nFinally, we used ensemble learning to combine predictions\\nfrom all classifiers for all three embeddings, and the results\\nare shown in the last row of Table 4. As can be seen, the over-\\nall accuracy is improved to 93% for BERT, 85% for XLNet,\\nand 88% for ELMo embeddings.\\n\\nNext, the attempt was made to examine the efficiency of a\\nbasic feed-forward neural network on a separate embedding\\n\\nwith two hidden dense layers, L1=60 and L2=50 neurons.\\nWe further explored the influence of the various activation\\nfunctions on the outcomes. The findings are indicated in Fig.\\n6. It is observed from Fig. 6 that RELU activation function\\nperforms better than the remaining for all three embeddings.\\n\\nProposedmodel results\\n\\nIn Model 1, we assessed our model’s performance on var-\\nious context-based embeddings, such as BERT, XLNet,\\nand ELMo. We later incorporated key features from these\\nthree embeddings into a single vector by employing sev-\\neral dense layers with a concatenation layer and classified\\nthe resulting vector. Table 5 represents the outcomes of\\n\\n123\\n\\n\\n\\nComplex & Intelligent Systems\\n\\nFig. 6 Performance analyses using different activation function\\n\\nTable 5 Test set results of early fusion-based DNN models (Model 1)\\n\\nClassifier Acc (%) F1-score (%)\\n\\nBERT embedding+DNN 91.58 92-Fake, 91-Real\\n\\nELMO embedding+DNN 92.16 92-Fake, 92-Real\\n\\nXLNet embedding+DNN 83.13 83-Fake, 84-Real\\n\\nBERT+ELMo embeddings+DNN 92.61 93-Fake, 92-Real\\n\\nXLNet+BERT embeddings+DNN 90.66 90-Fake, 89-Real\\n\\nXLNet+ELMo embeddings+DNN 91.93 91-Fake, 92-Real\\n\\nBERT+XLNet+ELMo embeddings+DNN 93 93-Fake, 92-Real\\n\\nModel 1 on the test data set. From the table, classifiers such\\nas BERT embedding+DNN, ELMo embedding+DNN, and\\nXLNet embedding+DNN achieved an accuracy of 91.58%,\\n92.16%, and 83.13%, respectively. The combined model\\nof BERT+XLNet+ELMo embeddings + DNN achieved an\\naccuracy of 93%. The outcome in Table 5 clearly shows that\\nintegrating the key features from all three embeddings will\\nincrease the classifier’s efficiency on our test data set.\\n\\nIn Model 2, the efficiency of RNN-based models for the\\nclassification of the COVID-19 dataset was explored. Ini-\\ntially, we performed individual experiments with different\\nvariants of RNN. Later, to increase model performance, we\\nexperimentedwith their ensemble setup,which calculated the\\naverage prediction value using the probability values of base\\nlearner predictions, as described in “Classification”. Thefinal\\nresult is determined by comparing the average prediction to\\nthe threshold value. The results tabulated in Table 6 showed\\nthat the ensemble model outperformed the remainingmodels\\nwith an accuracy of 92%, and the next best performingmodel\\nis BIGRU with an accuracy of 91.978%.\\n\\nIn Model 3, for our task of classifying the COVID-19 data\\ncollection, first, we worked with language model classifiers\\nsuch as BERT and ULMFit. The BERT and ULMFit devel-\\nopers provide a simple classification API. Bert-base-uncased\\nand ASGD Weight-Dropped LSTM (AWD-LSTM) [22] is\\nour underlying option model for BERT and ULMFit, respec-\\ntively. From the initial findings, we found that the BERT\\n\\nTable 6 Test set results of RNN-based models (Model 2)\\n\\nClassifier Acc (%) F1-score (%)\\n\\nLSTM 91.74 91-Fake,92-Real\\n\\nBILSTM 91.121 91-Fake,91-Real\\n\\nGRU 90.23 89-Fake,91-Real\\n\\nBIGRU 91.978 91-Fake,93-Real\\n\\nEnsemble 92 92-Fake,93-Real\\n\\nTable 7 Test set result of language and voting classifier model models\\n(Model 3)\\n\\nClassifier Acc (%) F1-score (%)\\n\\nBERT classifier 97 97-Fake,98-Real\\n\\nULMFit classifier 96 96-Fake,96-Real\\n\\nLR,ULMFit classifier, BERT classifier 98 98-Fake,98-Real\\n\\nLR,KNN,BERT classifier 96 96-Fake,96-Real\\n\\nLR,SVM,RF,KNN AND BERT classifier 95 95-Fake,95-Real\\n\\nTable 8 Test set result for bit-wise operator models (Model 4)\\n\\nModel Acc(%) F1-score(%)\\n\\nLR OR BERT 95 94-Fake, 95-Real\\n\\nULMFit OR BERT 96 96-Fake, 96-Real\\n\\n( LR OR BERT) OR (SVM OR KNN) 92 92-Fake,93-Real\\n\\nclassifier outperforms theULMFit classifierwith an accuracy\\nvalue of 97%, as shown in Table 7. Next, we attempted to\\ncombine the outcomes of a language model with output from\\nconventional machine learning algorithms using the voting\\nclassifier model, as illustrated in Fig. 3. From Table 7, it can\\nbe seen that the ensemble setting of LR, ULMFit classifier,\\nand BERT classifier has achieved the highest accuracy of\\n98%.\\n\\nIn Model 4, to further improve the efficiency of our pro-\\nposed models, we examined the effect of the bit-wise ‘OR’\\noperator on the outcomes. The results are tabulated in Table\\n8. As shown in Table 8, bit-wise ’OR’ing between ULM-\\nFit and BERT classifiers achieved the maximum accuracy\\nof 96%. We also tried multi-level ‘OR’ing between the out-\\ncomes of traditional machine learning algorithm with those\\nof the BERT classifier, and the model achieved an accuracy\\nof 92% as shown in the last row of Table 8.\\n\\nAdditional experiments\\n\\nTo assess the performance of the proposed model, we con-\\nducted experiments on two additional social media data sets,\\nsuch as liar [39] and Kaggle fake news data set8, using our\\n\\n8 https://www.kaggle.com/jruvika/fake-news-detection.\\n\\n123\\n\\nhttps://www.kaggle.com/jruvika/fake-news-detection\\n\\n\\nComplex & Intelligent Systems\\n\\nFig. 7 Performance analyses using different data set\\n\\nbest-performing model, and the results are summarized in\\nFig. 7. The experimental results show that the proposed\\nmodel performs better on both data sets, with an accuracy\\nvalue of 87% on the Kaggle dataset and 61% on the Liar\\ndataset, allowing it to be used as a general solution for iden-\\ntifying fake news in any domain.\\n\\nWe also investigated the behavior of our proposed models\\non sample sentences from the test data set and discovered that\\nthe majority of proposed models were able to classify it cor-\\nrectly. Table 9 displays the results of the test cases. As shown\\nin Table 9, all proposed models accurately classified the first\\ntest sample. Still, for test samples 2 and 3,Model 1 andModel\\n2 could not predict the actual target value. However, Models\\n3 and 4 accurately classified the target value in all three cases,\\nowing to their superior performance. Some sample sentences\\ncontaining emotions (test samples 4 and5)were also included\\nin the study, and it was discovered that the majority of the\\nmodels correctly classified those sentences. Finally, to take\\nadvantage of domain-specific embeddings, we tested tradi-\\ntional machine learning classifiers with embeddings from\\nClinicalBERT [2], which was trained on electronic health\\nrecords of ICU patients, and the results are summarized in\\nTable 10. The architecture of ClinicalBERT is similar to that\\nof the original BERT, as mentioned in Sect. 3. However, due\\nto a lack of medical terminology in our training data set,\\ndomain-specific embeddings failed to outperform conven-\\ntional BERT in our initial trials. Therefore, we do not proceed\\nwith the proposed models.\\n\\nDiscussion\\n\\nFrom the results in the previous section, we have found that\\nthe logistic regression (LR) and Support Vector Machine\\n(SVM) are the best-performing models among the con-\\nventional machine learning algorithms. Among the word\\nembeddings, BERT performed better than XLNet and ELMo\\nfrom Table 4, indicating that BERT handles our tweeter data\\nbetter. The performance ofmachine learningmodels utilizing\\n\\nXLNet word embedding was unexpected. The performance\\nof XLNet was poor, which could be attributed to the fact\\nthat our dataset contained short sentences, whereas XLNet\\nis trained to handle long sentences. In addition, our early\\nfusion-based model for combining the key features from\\nthree different embedding vectors(Model 1) enhanced the\\nclassifier’s efficiency on our test data set, as shown in Table\\n5. In Model 2, among the different variants of RNN mod-\\nels, BiGRU performed better than other models. However,\\nthe best performance was reported by their Ensemble setting\\nwith 92% accuracy (shown in Table 6).\\n\\nOur benchmark findings on various machine learning\\nmodels show that the Voting Classifier model comprised LR,\\nBERT, and ULMFit were the best-performing models for\\ndetecting false news in our COVID-19 dataset (Model 3).\\nThey achieved an accuracy of 98% with the F-1 score of\\n98% for fake class and 98% for real class. BERT and ULM-\\nFit Classifiers followed it with an accuracy of 97% and 96%,\\nrespectively, as shown inTable 7.As a result, the overall study\\nconcluded that state-of-the-art language models and ensem-\\nble models outperform other machine learning techniques in\\nrecognizing fake news in the COVID-19 dataset. Our find-\\nings show that languagemodels likeBERTandULMFit fared\\nbetter on our data set, because they were trained on massive\\nWikipedia corpora. We also compared our work with some\\nexisting models on the same data set, and the result is indi-\\ncated in Table 11. As shown in Table 11, [8] have used SVM\\nclassifier with linguistic features extracted from a tweet and\\nreported an accuracy of 95%. [4] used a Layer Differentiated\\ntraining approach for ULMFit, and [10] used XLNet embed-\\ndings with Topic Distributions to achieve accuracy of 96.7%\\nand 96.8%, respectively. According to Table 11, our model\\nis the best-performing model among the existing models on\\nthe same dataset.\\n\\nImplications and limitations of our model\\n\\nFake news is now a regular phenomenon on social media\\nsites, especially amid health crises; the size of the fake news-\\nrelated pandemic is increasing at an alarming rate. Therefore,\\nit is critical to stop the spread of fake news. The models\\nprovided in this article can help to improve the performance\\nof existing methods for dealing with the propagation of fake\\nnews through social media sites. However, even though our\\nmodels considerably improved the performance of existing\\nmethods, none of the models are perfect, and there is always\\nroom for improvement. Furthermore, even our model has\\nsome limits that can be addressed in future research. Some\\nof our model’s limitations include:\\n\\n1. Due to computational constraints, our model is trained on\\na limited data set. Future studies could broaden this to a\\nbigger corpus.\\n\\n123\\n\\n\\n\\nComplex & Intelligent Systems\\n\\nTable 9 Test cases for fake news\\n\\nSample text Model-1 Model-2 Model-3 Model-4 Target\\n\\nBill Gates said that the COVID-19 vaccine will perma-\\nnently change your DNA\\n\\nFake Fake Fake Fake Fake\\n\\nCOVID-19 is caused by a bacterium, not virus and can be\\ntreated with aspirin\\n\\nFake Real Fake Fake Fake\\n\\nEMA endorses the use of dexamethasone for COVID-19 Fake Fake Real Real Fake\\n\\nThankGod! newCOVID-19 clustersmostly affecting low\\npaid workers\\n\\nFake Fake Fake Fake Fake\\n\\nA video of a television presenter where she says “thank\\nGodwhere she says “thank God things get complicated”\\nreferring to the coronavirus inGermany to the coronavirus\\nin Germany\\n\\nReal Fake Fake Fake Fake\\n\\nTable 10 Results on\\ndomain-specific embeddings\\n\\nEmbedding Accuracy(%) F1-score fake(%) F1-score real(%)\\n\\nLR ClinicalBERT 88 88 89\\n\\nNB ClinicalBERT 80 80 80\\n\\nSVM ClinicalBERT 90 90 90\\n\\nRF ClinicalBERT 85 85 86\\n\\nKNN ClinicalBERT 85 85 86\\n\\nEnsemble ClinicalBERT 86 85 86\\n\\nTable 11 Comparative analysis\\nour model with some existing\\nmodels\\n\\nSource Model Acc (%) F1-score (%)\\n\\n[8] SVM+linguistic features 95.19% 95.70\\n\\n[4] ladiff ULMFit 96.72 96.73\\n\\n[10] XLNet with topic distributions 96.8 96.7\\n\\nproposed model1 early fusion-based model 93 93\\n\\nproposed model2 RNN-based model 92 92\\n\\nproposed model3 LR,ULMFit,BERT Classifier 97 98\\n\\nproposed model4 bit-wise operator-based model 96 96\\n\\n2. Our study did not account for code-mixed and native\\nlanguage statements. Therefore, there is always the pos-\\nsibility of expanding it to include a low resource regional\\nand code-mixed language data set.\\n\\nConclusion and future work\\n\\nThe present COVID-19 pandemic is a threat to people.\\nMuch of which will rely on the precision and credibility of\\nshared knowledge to control and prevent COVID-19 among\\nthe inhabitants. This article has performed comprehensive\\nresearch on various machine learning models to classify\\n‘fake’ news in the COVID-19 dataset and summarized the\\nbenchmark findings. Our findings indicated that ensemble\\nand language models are doing better than the other mod-\\nels to classify fake news in our corpus with accuracy value\\nof 98%. Furthermore, among all the embedding discussed,\\n\\nBERT embedding performed better than XLNet and ELMo\\nby a larger margin with the available short text data extracted\\nfrom Twitter. And also, combining features extracted from\\ndifferent embeddings into a single vector for classification\\nwill increase the performance by a small margin. Further-\\nmore, future work can extend our study into multimodal data\\ncontaining both picture and text pieces. future research can\\nalso include mixed language code, and regional languages;\\nthis is critical, because many people over the globe rely on\\nthe knowledge they exchange in their native language. Addi-\\ntionally, performance of existing methods can be improved\\nby training them on a larger corpus of health-related fake\\nnews.\\n\\nDeclarations\\n\\nConflict of interest Authors state that there is no conflict of interest.\\n\\n123\\n\\n\\n\\nComplex & Intelligent Systems\\n\\nOpen Access This article is licensed under a Creative Commons\\nAttribution 4.0 International License, which permits use, sharing, adap-\\ntation, distribution and reproduction in any medium or format, as\\nlong as you give appropriate credit to the original author(s) and the\\nsource, provide a link to the Creative Commons licence, and indi-\\ncate if changes were made. The images or other third party material\\nin this article are included in the article’s Creative Commons licence,\\nunless indicated otherwise in a credit line to the material. If material\\nis not included in the article’s Creative Commons licence and your\\nintended use is not permitted by statutory regulation or exceeds the\\npermitted use, youwill need to obtain permission directly from the copy-\\nright holder. To view a copy of this licence, visit http://creativecomm\\nons.org/licenses/by/4.0/.\\n\\nReferences\\n\\n1. Allcott H, Gentzkow M (2017) Social media and fake news in the\\n2016 election. J Econ Perspect 31(2):211–36\\n\\n2. Alsentzer E, Murphy JR, Boag W, Weng W-H, Jin D, Nau-\\nmann T, McDermott M (2019) Publicly available clinical BERT\\nembeddings. In: Proceedings of the 2nd clinical natural language\\nprocessing workshop, pp 72–78\\n\\n3. Anderson J, Rainie L (2017) The future of truth andmisinformation\\nonline. Pew Research Center, pp 1–224\\n\\n4. AzhanM, AhmadM (2021) LaDiff ULMFiT: a layer differentiated\\ntraining approach for ULMFiT. In: Chakraborty T, Shu K, Bernard\\nHR, Liu H, Akhtar MS (eds) Combating online hostile posts in\\nregional languages during emergency situation, vol 1402. Springer,\\nCham, pp 54-61. https://doi.org/10.1007/978-3-030-73696-5_6\\n\\n5. Cao J, Guo J, Li X, Jin Z, Guo H, Li J (2018) Automatic rumor\\ndetectiononmicroblogs: a survey. arXiv e-prints arXiv:1807.03505\\n\\n6. Castillo C, Mendoza M, Poblete B (2011) Information credibility\\non twitter. In Proceedings of the 20th International Conference on\\nWorld Wide Web, pp 675–684\\n\\n7. De Domenico M, Lima A, Mougel P, Musolesi M (2013) The\\nanatomy of a scientific rumor. Sci Rep 3:2980\\n\\n8. Felber T (2021) Constraint 2021: machine learning models\\nfor COVID-19 fake news detection shared task. arXiv e-prints\\narXiv:2101.03717\\n\\n9. Fung IC-H, FuK-W,ChanC-H,ChanBSB,CheungC-N,Abraham\\nT, Tse ZTH (2016) Social media’s initial reaction to information\\nand misinformation on Ebola, august 2014: facts and rumors. Pub-\\nlic Health Rep 131(3):461–473\\n\\n10. Gautam A, Venktesh V, Masud S (2021) Fake news detec-\\ntion system using XLNet model with topic distributions: CON-\\nSTRAINT@AAAI2021 shared task. In: Chakraborty T, Shu K,\\nBernard HR, Liu H, Akhtar MS (eds) Combating online hostile\\nposts in regional languages during emergency situation, vol 1402.\\nSpringer, Cham, pp 189–200. https://doi.org/10.1007/978-3-030-\\n73696-5_18\\n\\n11. Gupta A, Lamba H, Kumaraguru P (2013) $1.00 per rt# boston-\\nmarathon# prayforboston: analyzing fake content on twitter. In\\n2013 APWG eCrime Researchers Summit, IEEE, pp 1–12\\n\\n12. Gupta A, Sukumaran R, John K, Teki S (2021) Hostility detection\\nand Covid-19 fake news detection in social media. arXiv e-prints\\narXiv:2101.05953\\n\\n13. Howard J, Ruder S (2018) Universal language model fine-tuning\\nfor text classification. In: Proceedings of the 56th annual meeting\\nof the association for computational linguistics, vol 1: Long Papers,\\npp 328–339\\n\\n14. IslamMS, Sarkar T, Khan SH, Kamal A-HM, Hasan SM, Kabir A,\\nYeasmin D, Islam MA, Chowdhury KIA, Anwar KS et al (2020)\\nCOVID-19-related infodemic and its impact on public health: a\\n\\nglobal social media analysis. Am J Trop Med Hyg 103(4):1621–\\n1629\\n\\n15. Kang C, Goldman A (2016) In washington pizzeria attack, fake\\nnews brought real guns. New York Times\\n\\n16. Kingma DP, Ba J (2015) Adam: a method for stochastic optimiza-\\ntion. In: ICLR (Poster)\\n\\n17. Kumar A, Saumya S, Singh JP (2020) NITP-AI-NLP@UrduFake-\\nFIRE2020: multi-layer dense neural network for fake news detec-\\ntion in urdu news articles. In FIRE (Working Notes), pp 458–463\\n\\n18. Kumar A, Singh JP, Saumya S (2019) A comparative analysis\\nof machine learning techniques for disaster-related tweet classi-\\nfication. In 2019 IEEE R10 Humanitarian Technology Conference\\n(R10-HTC)(47129), IEEE, pp 222–227\\n\\n19. Kwon S, Cha M, Jung K, Chen W, Wang Y (2013) Prominent\\nfeatures of rumor propagation in online social media. In 2013 IEEE\\n13th International Conference on Data Mining, IEEE, pp 1103–\\n1108\\n\\n20. Lewandowsky S, EckerUK, Seifert CM, SchwarzN,Cook J (2012)\\nMisinformation and its correction: continued influence and suc-\\ncessful debiasing. Psychol Sci Public Interest 13(3):106–131\\n\\n21. Liu H (2019) A location independent machine learning approach\\nfor early fake news detection. In 2019 IEEE International Confer-\\nence on Big Data (Big Data), IEEE, pp 4740–4746\\n\\n22. Merity S,KeskarNS, SocherR (2018)Regularizing and optimizing\\nLSTM language models. In: International conference on learning\\nrepresentations\\n\\n23. Merity S, Shirish Keskar N,Socher R (2018) AnAnalysis of Neural\\nLanguage Modeling at Multiple Scales. arXiv e-prints\\n\\n24. PakaWS, Bansal R, Kaushik A, Sengupta S, Chakraborty T (2021)\\nCross-sean: a cross-stitch semi-supervised neural attention model\\nfor COVID-19 fake news detection. Appl Soft Comput 107:107393\\n\\n25. Patwa P, Sharma S, Pykl S, Guptha V, Kumari G, Akhtar S,\\nEkbal A, Das A, Chakraborty T (2021) Fighting an infodemic:\\nCOVID-19 fake news dataset. In: Chakraborty T, Shu K, Bernard\\nHR, Liu H, Akhtar MS (eds) Combating online hostile posts in\\nregional languages during emergency situation. CONSTRAINT\\n2021. Communications in Computer and Information Science, vol\\n1402. Springer, Cham, pp 21-29. https://doi.org/10.1007/978-3-\\n030-73696-5_3\\n\\n26. Pennycook G, McPhetres J, Zhang Y, Lu JG, Rand DG (2020)\\nFighting COVID-19misinformation on social media: experimental\\nevidence for a scalable accuracy-nudge intervention. Psychol Sci\\n31(7):770–780\\n\\n27. Reis JC, Correia A, Murai F, Veloso A, Benevenuto F (2019)\\nSupervised learning for fake news detection. IEEE Intell Syst\\n34(2):76–81\\n\\n28. RuchanskyN,SeoS,LiuY (2017)Csi: a hybrid deepmodel for fake\\nnews detection. In: Proceedings of the 2017 ACM on Conference\\non Information and Knowledge Management, pp 797–806\\n\\n29. Sanh V, Debut L, Chaumond J, Wolf T (2019) Distilbert, a distilled\\nversion of bert: smaller, faster, cheaper and lighter. arXiv e-prints\\narXiv:1910.01108\\n\\n30. Saumya S, Singh JP (2018) Detection of spam reviews: a sentiment\\nanalysis approach. CSI Trans ICT 6(2):137–148\\n\\n31. Saumya S, Singh JP (2020) Spam review detection using LSTM\\nautoencoder: an unsupervised approach. Electron Commer Res.\\nhttps://doi.org/10.1007/s10660-020-09413-4\\n\\n32. Shu K, Awadallah AH, Dumais S, Liu H (2020) Detecting fake\\nnews with weak social supervision. IEEE Intell Syst 36:96–103\\n\\n33. Shu K, Mahudeswaran D, Wang S, Liu H (2020) Hierarchical\\npropagation networks for fake news detection: investigation and\\nexploitation. Proc Int AAAI ConfWeb and SocMedia 14:626–637\\n\\n34. Shu K, Sliva A, Wang S, Tang J, Liu H (2017) Fake news detection\\non social media: a data mining perspective. ACM SIGKDD Explor\\nNewsl 19(1):22–36\\n\\n123\\n\\nhttp://creativecommons.org/licenses/by/4.0/\\nhttp://creativecommons.org/licenses/by/4.0/\\nhttps://doi.org/10.1007/978-3-030-73696-5_6\\nhttp://arxiv.org/abs/1807.03505\\nhttp://arxiv.org/abs/2101.03717\\nhttps://doi.org/10.1007/978-3-030-73696-5_18 \\nhttps://doi.org/10.1007/978-3-030-73696-5_18 \\nhttp://arxiv.org/abs/2101.05953\\nhttps://doi.org/10.1007/978-3-030-73696-5_3\\nhttps://doi.org/10.1007/978-3-030-73696-5_3\\nhttp://arxiv.org/abs/1910.01108\\nhttps://doi.org/10.1007/s10660-020-09413-4\\n\\n\\nComplex & Intelligent Systems\\n\\n35. Shu K, Wang S, Liu H (2019) Beyond news contents: The role\\nof social context for fake news detection. In: Proceedings of the\\ntwelfth ACM International Conference on Web Search and Data\\nMining, pp 312–320\\n\\n36. Varol O, Ferrara E, Menczer F, Flammini A (2017) Early detection\\nof promoted campaigns on social media. EPJ Data Sci 6(1):13\\n\\n37. Vijjali R, Potluri P, Kumar S, Teki S (2020) Two stage transformer\\nmodel for COVID-19 fake news detection and fact checking. In:\\nProceedings of the 3rd NLP4IFworkshop onNLP for internet free-\\ndom: censorship, disinformation, and propaganda, pp 1–10\\n\\n38. Vosoughi S, Roy D, Aral S (2018) The spread of true and false\\nnews online. Science 359(6380):1146–1151\\n\\n39. Wang WY (2017) “Liar, Liar Pants on Fire”: a new benchmark\\ndataset for fake news detection. In: Proceedings of the 55th annual\\nmeeting of the association for computational linguistics, vol 2:\\nShort Papers, pp 422–426\\n\\n40. WangY,YangW,MaF,Xu J, ZhongB,DengQ,Gao J (2020)Weak\\nsupervision for fake nws detection via reinforcement learning. Proc\\nAAAI Conf Artif Intell 34:516–523\\n\\n41. Willmore A (2016) This analysis shows how viral fake election\\nnews stories outperformed real news on facebook\\n\\n42. Yang Z, Dai Z, Yang Y, Carbell J, Salakhutdinov RR, Le QV\\n(2019) Xlnet: generalized autoregressive pretraining for language\\nunderstanding. Advances in neural information processing sys-\\ntems. Springer, Berlin, pp 5753–5763\\n\\n43. Zafarani R, Zhou X, Shu K, Liu H (2019) Fake news research:\\nTheories, detection strategies, and open problems. In: Proceedings\\nof the 25thACMSIGKDDInternationalConference onKnowledge\\nDiscovery and Data Mining, pp 3207–3208\\n\\n44. Zhang X, Cao J, Li X, Sheng Q, Zhong L, Shu K (2021) Mining\\ndual emotion for fake news detection\\n\\n45. Zellers R, Holtzman A, Rashkin H, Bisk Y, Farhadi A, Roesner F,\\nChoi Y (2019) Defending against neural fake news. Adv Neural\\nInform Process Syst 32:1–12\\n\\n46. Zhou X, Zafarani R (2018) Fake news: a survey of research, detec-\\ntion methods, and opportunities. arXiv preprint arXiv:1812.00315\\n\\nPublisher’s Note Springer Nature remains neutral with regard to juris-\\ndictional claims in published maps and institutional affiliations.\\n\\n123\\n\\nhttp://arxiv.org/abs/1812.00315\\n\\n\\tCombating the infodemic: COVID-19 induced fake news recognition in social media networks\\n\\tAbstract\\n\\tIntroduction\\n\\tRelated works\\n\\tFake content detection in various scenarios\\n\\tFake content detection on COVID-19 data\\n\\n\\tMethodology\\n\\tDataset description\\n\\tData pre-treatment\\n\\tEmbedding\\n\\tClassification\\n\\tOptimizers and loss functions\\n\\n\\tResults\\n\\tInitial experimental results\\n\\tProposed model results\\n\\tAdditional experiments\\n\\n\\tDiscussion\\n\\tImplications and limitations of our model\\n\\n\\tConclusion and future work\\n\\tReferences\\n\\n\\n\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "df.loc[0,'Text']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17e4b472",
      "metadata": {
        "id": "17e4b472"
      },
      "outputs": [],
      "source": [
        "df.to_csv('IK_rr_DataFrame.csv', index = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e93d448f",
      "metadata": {
        "id": "e93d448f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3559f506-5cd7-4727-f0ea-f73f52fa47dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IK_rr_DataFrame.csv  sample_data\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}