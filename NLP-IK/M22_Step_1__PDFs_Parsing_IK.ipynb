{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8155e530",
      "metadata": {
        "id": "8155e530"
      },
      "source": [
        "# List the files in the directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e56d4bc",
      "metadata": {
        "id": "8e56d4bc",
        "outputId": "ac85fd66-d33c-4c50-cda4-2eaec9649404",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'Amit Saxena'\n"
          ]
        }
      ],
      "source": [
        "!ls /Dataset-IK/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7062cb2",
      "metadata": {
        "id": "f7062cb2",
        "outputId": "bb5a508d-d579-484e-9f7b-4cfd6af6a41e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# The PDF files for each author are stored in their respective folder\n",
        "import os\n",
        "len(os.listdir('/Dataset-IK/')) #No. of authors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc4a38ef",
      "metadata": {
        "id": "fc4a38ef",
        "outputId": "a5d9e965-5d04-4e4a-c63b-edb5c93527ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total No. of PDF files (research papers): 16\n"
          ]
        }
      ],
      "source": [
        "c = 0 #No. of files\n",
        "for root, dirs, files in os.walk('/Dataset-IK/'):\n",
        "    for file in files:\n",
        "        c = c + 1\n",
        "        #print(os.path.join(root, file))\n",
        "\n",
        "print('Total No. of PDF files (research papers):',c)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2120f7a",
      "metadata": {
        "id": "c2120f7a"
      },
      "source": [
        "# Extracting text from PDFs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99fa0eb1",
      "metadata": {
        "id": "99fa0eb1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a277eb60-5314-4681-ffa0-290cc8295417"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tika\n",
            "  Downloading tika-2.6.0.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tika) (67.7.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from tika) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->tika) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->tika) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->tika) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->tika) (2023.7.22)\n",
            "Building wheels for collected packages: tika\n",
            "  Building wheel for tika (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tika: filename=tika-2.6.0-py3-none-any.whl size=32621 sha256=9c695e7d810b7d9477592ff9351ea96328b031b29a2d4751809af02961fd0a5c\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/71/c7/b757709531121b1700cffda5b6b0d4aad095fb507ec84316d0\n",
            "Successfully built tika\n",
            "Installing collected packages: tika\n",
            "Successfully installed tika-2.6.0\n"
          ]
        }
      ],
      "source": [
        "!pip install tika"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4468825d",
      "metadata": {
        "id": "4468825d"
      },
      "outputs": [],
      "source": [
        "import tika #PDF Text Extractor\n",
        "from tika import parser\n",
        "\n",
        "# Set up tika\n",
        "tika.initVM()\n",
        "\n",
        "# Function to extract text from PDF\n",
        "def extract_text(filename):\n",
        "    # Parse PDF file\n",
        "    parsed = parser.from_file(filename)\n",
        "\n",
        "    # Extract text from parsed data\n",
        "    text = parsed[\"content\"]\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e454eee2",
      "metadata": {
        "id": "e454eee2",
        "outputId": "4aaafac6-62c5-4b28-858c-4eeed86b483b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-09-29 01:39:00,323 [MainThread  ] [INFO ]  Retrieving http://search.maven.org/remotecontent?filepath=org/apache/tika/tika-server-standard/2.6.0/tika-server-standard-2.6.0.jar to /tmp/tika-server.jar.\n",
            "INFO:tika.tika:Retrieving http://search.maven.org/remotecontent?filepath=org/apache/tika/tika-server-standard/2.6.0/tika-server-standard-2.6.0.jar to /tmp/tika-server.jar.\n",
            "2023-09-29 01:39:01,008 [MainThread  ] [INFO ]  Retrieving http://search.maven.org/remotecontent?filepath=org/apache/tika/tika-server-standard/2.6.0/tika-server-standard-2.6.0.jar.md5 to /tmp/tika-server.jar.md5.\n",
            "INFO:tika.tika:Retrieving http://search.maven.org/remotecontent?filepath=org/apache/tika/tika-server-standard/2.6.0/tika-server-standard-2.6.0.jar.md5 to /tmp/tika-server.jar.md5.\n",
            "2023-09-29 01:39:01,469 [MainThread  ] [WARNI]  Failed to see startup log message; retrying...\n",
            "WARNING:tika.tika:Failed to see startup log message; retrying...\n",
            "2023-09-29 01:39:06,493 [MainThread  ] [WARNI]  Failed to see startup log message; retrying...\n",
            "WARNING:tika.tika:Failed to see startup log message; retrying...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTitle:\\n\\n\\nA Review of Clustering Techniques and Developments \\n\\nAmit Saxena1, Mukesh Prasad2, Akshansh Gupta3, Neha Bharill4, Om Prakash Patel4, Aruna Tiwari4, \\n\\nMeng Joo Er5, Weiping Ding6, Chin-Teng Lin2  \\n\\n1Department of Computer Science & IT, Guru Ghasidas Vishwavidyalaya, Bilaspur, India \\n\\n2Centre for Artificial Intelligence, University of Technology Sydney, Sydney, Australia \\n\\n3School of Computational and Integrative Sciences, Jawaharlal Nehru University, New Delhi, India \\n\\n4Department of Computer Science and Engineering, Indian Institute of Technology Indore, India \\n\\n5School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore \\n\\n6School of Computer and Technology, Nantong University, Nantong, China \\n\\n \\n\\n \\n\\nAbstract \\n\\nThis paper presents a comprehensive study on clustering: exiting methods and developments made at various \\n\\ntimes. Clustering is defined as an unsupervised learning where the objects are grouped on the basis of some \\n\\nsimilarity inherent among them. There are different methods for clustering the objects such as hierarchical, \\n\\npartitional, grid, density based and model based. The approaches used in these methods are discussed with their \\n\\nrespective states of art and applicability. The measures of similarity as well as the evaluation criteria, which are \\n\\nthe central components of clustering are also presented in the paper. The applications of clustering in some \\n\\nfields like image segmentation, object and character recognition and data mining are highlighted. \\n\\nKeywords: Unsupervised learning, Clustering, Data mining, Pattern recognition, Similarity measures \\n\\n1 Introduction \\n\\nGrouping of objects is required for various purposes in different areas of engineering, science and \\n\\ntechnology, humanities, medical science and our daily life. Take for an instance, people suffering from \\n\\na particular disease have some symptoms in common and are placed in a group tagged with some label \\n\\nusually the name of the disease. Evidently, the people not possessing those symptoms (and hence the \\n\\ndisease) will not be placed in that group. The patients grouped for that disease will be treated \\n\\naccordingly while patients not belonging to that group should be handled differently. It is therefore so \\n\\nessential for a medical expert to diagnose the symptoms of a patient correctly such that he/she is not \\n\\nplaced in a wrong group. Whenever we find a labeled object, we will place it into the group with same \\n\\nlabel. It is rather a trivial task as the labels are given in advance. However, on many occasions, no \\n\\nsuch labeling information is provided in advance and we group objects on the basis of some similarity.  \\n\\nBoth of these instances represent a wide range of problems occurring in analysis of data. In generic \\n\\nterms, these cases are dealt under the scope of classification [1]. Precisely, the first case when the class \\n\\n(label) of an object is given in advance is termed as supervised classification whereas the other case \\n\\nwhen the class label is not tagged to an object in advance is termed as unsupervised classification. \\n\\nThere has been a tremendous amount of work in supervised classification and evidently has been \\n\\nreported in the literature widely [2-9]. The main purpose behind the study of classification is to \\n\\ndevelop a tool or an algorithm, which can be used to predict the class of an unknown object, which is \\n\\nnot labeled. This tool or algorithm is called a classifier. The objects in the classification process are \\n\\nmore commonly represented by instances or patterns. A pattern consists of a number of features (also \\n\\ncalled attributes). The classification accuracy of a classifier is judged by the fact as how many testing \\n\\npatterns it has classified correctly. There has been a rich amount of work in supervised classification, \\n\\nsome of the pioneer supervised classification algorithms  can be found in neural networks [10, 11], \\n\\nfuzzy sets [12, 13], PSO [14, 15], rough sets [16-18] , decision tree [19], Bayes classifiers [20] etc. \\n\\nContrary to supervised classification, where we are given labeled patterns; the unsupervised \\n\\nclassification differs in the manner that there is no label assigned to any pattern. The unsupervised \\n\\nclassification is commonly known as clustering. As learning operation is central to the process of \\n\\n\\n\\nclassification (supervised or unsupervised), it is used in this paper interchangeably with the same \\n\\nspirit. Clustering is a very essential component of various data analysis or machine learning based \\n\\napplications like, regression,  prediction, data mining [21] etc. According to Rokach [22] clustering \\n\\ndivides data patterns into subsets in such a way that similar patterns are clustered together. The \\n\\npatterns are thereby managed into a well-formed evaluation that designates the population being \\n\\nsampled. Formally and conventionally, the clustering structure can be represented as a set S of subsets \\n\\nS1, S2, …, Sk , such that: \\n\\n1 2 3...... kS S S S \\uf066\\uf03d\\n                                                              (1) \\n\\nThis means obviously that any instance in S (S1... Sk) belongs to exactly one subset and does not \\n\\nbelong to any other subset. Clustering of objects is also applicable for charactering the key features of \\n\\npeople in recognizing them on the basis of some similarity. In general, we may divide people in \\n\\ndifferent clusters on the basis of gender, height, weight, color, vocal and some other physical \\n\\nappearances. Hence, clustering embraces several interdisciplinary areas such as: from mathematics \\n\\nand statistics to biology and genetics, where all of these use various terminology to explain the \\n\\ntopologies formed using this clustering analysis technique. For example, from biological \\n\\n“taxonomies”, to medical “syndromes” and genetic “genotypes” to manufacturing” group technology”, \\n\\neach of these topics has same identical problem: create groups of instances and assign each instance to \\n\\nthe appropriate groups. \\n\\nClustering is considered to be more difficult than supervised classification as there is no label \\n\\nattached to the patterns in clustering. The given label in the case of supervised classification becomes \\n\\na clue to grouping data objects as a whole. Whereas in the case of clustering, it becomes difficult to \\n\\ndecide, to which group a pattern will belong to, in the absence of a label. There can be several \\n\\nparameters or features which could be considered fit for clustering. The curse of dimensionality can \\n\\nadd to the crisis. High dimensionality not only leads to high computational cost but also affects the \\n\\nconsistency of algorithms. There are although feature selection methods reported as a solution [23]. \\n\\nThe sizes of the databases (e.g. small, large or very large) can also guide the clustering criteria.  \\n\\nJain [24] illustrated that the main aim of data clustering is to search the real grouping(s) of a set of \\n\\ninstances, points, or objects. Webster (Merriam-Webster Online Dictionary) [25] explains clustering as \\n\\n‘‘a statistical classification method for finding whether each of patterns comes into various groups by \\n\\nmaking quantitative comparisons of different features\". It is evident from the above discussion that \\n\\nsimilarity is the central factor to a cluster and hence clustering process. The natural grouping of data \\n\\nbased on some inherent similarity is to be discovered in clustering. In most of the cases, the number of \\n\\nclusters to be formed is specified by the user. As there is only numeric type data available to represent \\n\\nfeatures of the patterns in a group, the only way to extract any information pertaining to the \\n\\nrelationship among patterns is to make use of numeric arithmetic. The features of the objects are \\n\\nrepresented by numeric values. The most common approach to define similarity is taken as a measure \\n\\nof distance among the patterns, lower the distance (e.g. Euclidean distance) between the two objects, \\n\\nhigher the similarity and vice versa.  \\n\\nThe overall paper is organized as follows. Various clustering techniques will be discussed in \\n\\nSection 2. Section 3 presents measures of similarity for differentiating the patterns. In Section 4, the \\n\\nvariants of clustering methods have been presented. The evaluation criteria of the clustering \\n\\ntechniques applied for different problems are provided in Section 5. Section 6 highlights some \\n\\nemerging applications of clustering. Section 7 describes which clustering method to select under \\n\\ndifferent applications followed by conclusions in Section 8. Due to a wide range of topics in the \\n\\nsubject, the omission or the unbalancing of certain topics presented in the paper cannot be denied. The \\n\\nobjective of the paper is however to present a comprehensive timeline study of clustering with its \\n\\nconcepts, comparisons, existing techniques and few important applications. \\n\\n\\n\\n2 Clustering Techniques \\n\\nIn this section, we will discuss various clustering approaches with inherent techniques. The reason \\n\\nfor having different clustering approaches towards various techniques is due to the fact that there is no \\n\\nsuch precise definition to the notion of “cluster” [22, 26]. That is why, different clustering approaches \\n\\nhave been proposed, each of which uses a different inclusion principle. Fraley and Raftery [27] \\n\\nsuggested dividing the clustering approaches into two different groups: hierarchical and partitioning \\n\\ntechniques. Han and Kamber [21] suggested the following three additional categories for applying \\n\\nclustering techniques: density-based methods, model-based methods and grid-based methods. An \\n\\nalternative categorization based on the induction principle of different clustering approaches is \\n\\npresented in Castro et al [26]. However, the number of clusters into which available dataset to be \\n\\ndivided, is decided by the users judiciously by using some of the approaches including heuristic, trial \\n\\nand error or evolutionary. If the user decides suitable number, the accuracy judged by intra-cluster \\n\\ndistance will be high otherwise the accuracy can become low. Fig. 1 shows the taxonomy of clustering \\n\\napproaches [27].  \\n\\n \\n\\nFig. 1 Taxonomy of clustering approaches [27] \\n\\n2.1 Hierarchical Clustering (HC) Methods \\n\\nIn hierarchical clustering methods, clusters are formed by iteratively dividing the patterns using \\n\\ntop-down or bottom up approach. There are two forms of hierarchical method namely agglomerative \\n\\nand divisive hierarchical clustering [32]. The agglomerative follows the bottom-up approach, which \\n\\nbuilds up clusters starting with single object and then merging these atomic clusters into larger and \\n\\nlarger clusters, until all of the objects are finally lying in a single cluster or otherwise until certain \\n\\ntermination conditions are satisfied. The divisive hierarchical clustering follows the top-down \\n\\napproach, which breaks up cluster containing all objects into smaller clusters, until each object forms a \\n\\ncluster on its own or until it satisfies certain termination conditions. The hierarchical methods usually \\n\\nlead to formation of dendrograms as shown in Fig. 2 below.  \\n\\n\\n\\n \\n\\nFig. 2 Hierarchical clustering dendrogram \\n\\nThe hierarchical clustering methods could be further grouped in three categories based on similarity \\n\\nmeasures or linkages [28] as summarized in following sections. \\n\\n2.1.1 Single-linkage Clustering \\n\\nThis type of clustering is often called as the connectedness, the minimum method or the nearest \\n\\nneighbour method. In single-linkage clustering, the link between two clusters is made by a single \\n\\nelement pair, namely those two elements (one in each cluster) that are closest to each other. In this \\n\\nclustering, the distance between two clusters is determined by nearest distance from any member of \\n\\none cluster to any member of the other cluster, this also defines similarity. If the data is equipped with \\n\\nsimilarities, the similarity between a pair of clusters is considered to be equal to the greatest similarity \\n\\nfrom any member of one cluster to any member of the other cluster [29]. Fig. 3 shows the mapping of \\n\\nsingle linkage clustering. The criteria between two sets of clusters A and B is as follow: \\n\\n\\uf07b \\uf07dmin ( , ) : ,d a b a A b B\\uf0ce \\uf0ce\\n                                                      (2) \\n\\n \\n\\nFig. 3 Mapping of single linkage clustering \\n\\n2.1.2 Complete-linkage Clustering   \\n\\nIn complete-linkage clustering also called the diameter, the maximum method or the furthest \\n\\nneighbour method; the distance between two clusters is determined by longest distance from any \\n\\nmember of one cluster to any member of the other cluster [30]. Fig. 4 shows the mapping of complete \\n\\nlinkage clustering. The criteria between two sets of clusters A and B is as follow: \\n\\n\\uf07b \\uf07dmax ( , ) : ,d a b a A b B\\uf0ce \\uf0ce\\n                                                (3) \\n\\n\\n\\n \\n\\nFig. 4 Mapping of complete linkage clustering \\n\\n2.1.3 Average-linkage Clustering  \\n\\nIn average linkage clustering also known as minimum variance method; the distance between two \\n\\nclusters is determined by the average distance from any member of one cluster to any member of the \\n\\nother cluster [31]. Fig. 5 shows the mapping of average linkage clustering. The criteria between two \\n\\nsets of clusters A and B is as follow: \\n\\n1\\n( , )\\n\\n| || |a A b B\\n\\nd a b\\nA B \\uf0ce \\uf0ce\\n\\n\\uf0e5\\uf0e5\\n                                                             (4) \\n\\n \\n\\nFig. 5 Mapping of average linkage clustering \\n\\n2.1.4 Steps of Agglomerative and Divisive Clustering \\n\\n(i) Steps of agglomerative clustering \\n\\n \\n\\n \\n\\n  \\n\\n \\n\\n(ii) Steps of divisive clustering \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n1. Make each point a separate cluster \\n\\n2. Until the clustering is satisfactory \\n\\n3. Merge the two clusters with the smallest inter-cluster distance \\n\\n4. End \\n\\n \\n\\n1. Construct a single cluster containing all points \\n\\n2. Until the clustering is satisfactory \\n\\n3. Split the cluster that yields the two components with the largest inter-cluster distance \\n\\n4. End \\n\\n \\n\\n\\n\\nThe common criticism for classical HC algorithms is that they lack robustness and are, hence, \\n\\nsensitive to noise and outliers. Once an object is assigned to a cluster, it will not be considered again, \\n\\nwhich means that HC algorithms are not capable of correcting possible previous misclassification. The \\n\\ncomputational complexity for most of HC algorithms is at least O(N2) and this high cost limits their \\n\\napplication in large-scale data sets. Other disadvantages of HC include the tendency to form spherical \\n\\nshapes and reversal phenomenon, in which the normal hierarchical structure is, distorted [50]. With \\n\\nthe requirement of large-scale datasets in recent years, the HC algorithms are also enriched with some \\n\\nnew techniques as modifications to classical HC methods presented in following section.  \\n\\n2.1.5 Enhanced Hierarchical Clustering \\n\\nThe main deficiency of hierarchical clustering [33] is that after the two points of the clusters are \\n\\nlinked to each other, they cannot move in other clusters in a hierarchy. Few algorithms, which use \\n\\nhierarchical clustering with some enhancements, are given below:  \\n\\n(i) Balanced Iterative Reducing and Clustering Using Hierarchies (BIRCH) \\n\\nBIRCH [131] contains the idea of cluster features (CF). CF is the triple (n, LS, SS) where n is \\n\\nthe number of data objects in the cluster, LS is the linear sum of the attribute values of the \\n\\nobjects in the cluster and SS is the sum of squares of the attribute values of the objects in the \\n\\ncluster. These are stored in a CF-tree form, so no need to keep all tuples or all clusters in main \\n\\nmemory, but only, their tuples [34]. The main motivations of BIRCH lie in two aspects, the \\n\\nability to deal with large data sets and the robustness to outliers [131]. Also the BIRCH can \\n\\nachieve a comutational complexity of O(N). \\n\\n(ii) Clustering Using Representatives (CURE) \\n\\nCURE [35] is a clustering technique for dealing with large-scale databases, which is robust \\n\\ntowards outliers and accepts clusters of various shapes and sizes. Its performance is good with \\n\\n2-D data sets. BIRCH and CURE both handle outliers well but CURE clustering quality is \\n\\nbetter than that of BIRCH [35]. On the reverse, in terms of time complexity, BIRCH is better \\n\\nthan CURE as it attains computational complexity of O(N) compared to CURE O(N2logN).  \\n\\n(iii) ROCK \\n\\nROCK [130] is applied for categorical data sets which follows the agglomerative hierarchical \\n\\nclustering algorithm. It is based on the number of links between two records; links capture the \\n\\nnumber of other records, which are very similar to each other. This algorithm does not use \\n\\nany distance function. CURE [35] also proposed ROCK, which uses a random sample \\n\\nstrategy to handle large datasets. \\n\\n(iv) CHAMELEON \\n\\nCHAMELEON [36] is a hierarchical clustering algorithm, where clusters are merged only if \\n\\nthe interconnectivity and closeness (proximity) between two clusters are high relative to the \\n\\ninternal interconnectivity of the clusters and closeness of items within the clusters. One \\n\\nlimitation of CHAMELEON is that it is known for low dimensional spaces, and was not \\n\\napplied to high dimensions.  \\n\\nTable1 Features of hierarchical clustering-based enhanced methods \\n\\nName Type of data Complexity Ability to handle high \\n\\ndimensional data \\n\\nBIRCH Numerical O(N) No \\n\\nCURE Numerical O(N2logN) Yes \\n\\nROCK Categorical O(N2+Nmmma+N2logN)* No \\n\\nCHEMELEON Numerical/ Categorical O(Nm + NlogN + m2logN)** No \\n\\n*mm is the maximum number of neighbours for a point ma is the average number of \\n\\nneighbours for a point. \\n\\n**m is the number of initial sub-clusters produced by the graph partitioning algorithm. \\n\\n\\n\\n2.2 Partition Clustering Methods \\n\\nPartitional clustering is opposite to hierarchical clustering; here data are assigned into K clusters \\n\\nwithout any hierarchical structure by optimizing some criterion function [37]. The most commonly \\n\\nused criterion is the Euclidean distance, which finds the minimum distance between points with each \\n\\nof the available clusters and assigning the point to the cluster. The algorithms [33] studied in this \\n\\ncategory include: k-means [38], PAM [173], CLARA [173], CLARANS [174], Fuzzy C-means, \\n\\nDBSCAN etc. Fig. 6 shows the partitional clustering approach.  \\n\\n \\n\\nData points Partitional clusters \\n\\nFig. 6 Partitional clustering approaches \\n\\n2.2.1 K-means Clustering \\n\\nK-means algorithm is one of the best-known, bench marked and simplest clustering algorithms \\n\\n[37, 38], which is mostly applied to solve the clustering problems. In this procedure the given data set \\n\\nis classified through a user defined number of clusters, k. The main idea is to define k centroids, one \\n\\nfor each cluster. The objective function J is given as follows: \\n\\nMinimize \\n\\n2\\n( )\\n\\n1 1\\n\\nk n\\nj\\n\\ni j\\n\\nj i\\n\\nJ x c\\n\\uf03d \\uf03d\\n\\n\\uf03d \\uf02d\\uf0e5\\uf0e5\\n                                                          (5) \\n\\nwhere \\n\\n2\\n( )j\\n\\ni jx c\\uf02d\\n is a chosen distance measure between a data point \\n\\n( )j\\n\\nix\\nand the cluster centre jc\\n\\n, Fig. \\n\\n7 shows the flow diagram of K-means algorithm. \\n\\nAn algorithm similar to k-means, known as the Linde-Buzo-Gray (LBG) algorithm, was \\n\\nsuggested for vector quantization (VQ) [39] for signal compression. In this context, prototype vectors \\n\\nare called code words, which constitute a code book. VQ aims to represent the data with a reduced \\n\\nnumber of elements while minimizing information loss. Although K- Means clustering is still one of \\n\\nthe most popular clustering algorithms yet few limitation are associated with K Means clustering \\n\\ninclude: (a) There is no efficient and universal method for identifying the initial partitions and the \\n\\nnumber of clusters K and (b) K-means is sensitive to outliers and noise. Even if an object is quite far \\n\\naway from the cluster centroid, it is still forced into a cluster and, thus, distorts the cluster shapes [50]. \\n\\n \\n\\nFig. 7 Flow diagram of K -means algorithm \\n\\n\\n\\nThe procedure of K-means algorithm is composed of the following steps: \\n\\n1. Initialization: Suppose we decide to form K clusters of the given dataset. Now take K \\n\\ndistinct points (patterns) randomly. These points represent initial group centroids. As \\n\\nthese centroids will be changing after each iteration before clusters are fixed, there is \\n\\nno need to spend time in decision of choosing the centroids.  \\n\\n2. Assign each object to the group that has the closest centroid. \\n\\n3. When all objects have been assigned, recalculate the positions of the K centroids. \\n\\n4. Repeat Steps 2 and 3 until the centroids no longer move. This produces a separation \\n\\nof the objects into groups from which the metric to be minimized can be calculated.  \\n\\n2.2.2 Fuzzy C-means Clustering \\n\\nFuzzy c-means (FCM) is a clustering method which allows one point to belong to two or more \\n\\nclusters unlike K-means where only one cluster is assigned to each point. This method was developed \\n\\nby Dunn in 1973 [40] and improved by Bezdek in 1981 [41]. The procedure of fuzzy c-means [50] is \\n\\nsimilar to that of K-means. It is based on minimization of the following objective function: \\n2\\n\\n1 1\\n\\n|| || ;1\\nN c\\n\\nm\\n\\nm ij i j\\n\\ni j\\n\\nJ u x v m\\n\\uf03d \\uf03d\\n\\n\\uf03d \\uf02d \\uf03c \\uf03c \\uf0a5\\uf0e5 \\uf0e5\\n                                                     (6)  \\n\\nwhere m is fuzzy partition matrix exponent for controlling the degree of fuzzy overlap, with m > 1. \\n\\nFuzzy overlap refers to how fuzzy the boundaries between clusters are, that is the number of data \\n\\npoints that have significant membership in more than one cluster, uij is the degree of membership of xi \\n\\nin the cluster j, xi is the i-th pattern of d-dimension data, vj is j-th cluster center of the d-dimension and \\n*\\n\\n  is any norm expressing the similarity between any measured data and the center. \\n\\nProcedure for FCM \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\nFCM suffers from initial partition dependence, as well as noise and outliers like k-means. Yager \\n\\nand Filev [42] proposed the mountain method to estimate the cluster centers as an initial partition. \\n\\nGath and Geva [43] addressed the initialization problem by dynamically adding cluster prototypes, \\n\\nwhich are located in the space that is not represented well by the previously generated centers. \\n\\n1. Set up a value of c (number of cluster); \\n\\n2. Select initial cluster prototype 1 2, , , cV V V  from iX  , 1,2, ,i N\\uf03d  ; \\n\\n3. Compute the distance i jX V\\uf02d\\n between objects and prototypes; \\n\\n4. Computer the elements of the fuzzy partition matrix \\n( 1,2, ,i N\\uf03d ; 1,2, ,j c\\uf03d \\uf0bc\\uf0bc ) 1\\n\\n1\\n\\nc i j\\n\\nij l\\ni l\\n\\nx v\\nu\\n\\nx v\\n\\n\\uf02d\\n\\n\\uf03d\\n\\n\\uf0e9 \\uf0f9\\uf0e6 \\uf0f6\\uf02d\\n\\uf0ea \\uf0fa\\uf0e7 \\uf0f7\\uf03d\\n\\n\\uf0e7 \\uf0f7\\uf02d\\uf0ea \\uf0fa\\n\\uf0e8 \\uf0f8\\uf0eb \\uf0fb\\n\\n\\uf0e5\\n \\n\\n5. Compute the cluster prototypes ( 1,2, ,j c\\uf03d \\uf0bc\\uf0bc ) \\n2\\n\\n1\\n\\n2\\n\\n1\\n\\nN\\n\\nij ii\\n\\nj N\\n\\niji\\n\\nu x\\nV\\n\\nu\\n\\n\\uf03d\\n\\n\\uf03d\\n\\n\\uf03d\\n\\uf0e5\\n\\n\\uf0e5  \\n\\n6. Stop if the convergence is attained or the number of iterations exceeds a \\ngiven limit. Otherwise, go to step 3. \\n\\n \\n\\nhttp://home.dei.polimi.it/matteucc/Clustering/tutorial_html/cmeans.html#dunn#dunn\\nhttp://home.dei.polimi.it/matteucc/Clustering/tutorial_html/cmeans.html#bezdek#bezdek\\n\\n\\nChanging the proximity distance can improve the performance of FCM in relation to outliers [44]. In \\n\\nanother approach for reducing the effect of noise and outliers, Keller [45] interpreted memberships as \\n\\n“the compatibility of the points with the class prototype” rather than as the degree of membership. \\n\\nThis relaxes uij = 1 to uij > 0 and results in a possibilistic K-means clustering algorithm. \\n\\nThe conditions for a possibilistic fuzzy partition matrix are: \\n\\n\\uf05b \\uf05d0,1 ,1 , 1iju i N j C\\uf0ce \\uf0a3 \\uf0a3 \\uf0a3 \\uf0a3\\n                                                  (7) \\n\\n, 0,j iju i\\uf024 \\uf03e \\uf022\\n                                                                    (8) \\n\\n1\\n\\n0 , 1\\nN\\n\\nij\\n\\ni\\n\\nu N j C\\n\\uf03d\\n\\n\\uf03c \\uf03c \\uf0a3 \\uf0a3\\uf0e5\\n                                                          (9) \\n\\nTable 2 Features of partition clustering based techniques \\n\\nName Type of data complexity Ability to handle high \\n\\ndimensional data \\n\\nK-Mean Numerical O(N) No \\n\\nPAM Numerical O(K(N-K)2)* No \\n\\nCLARA Numerical O(K(40+K)2+K(N-K)) No \\n\\nCLARANS Numerical O(KN2) No \\n\\nFuzzy C-Means Numerical O(N) No \\n\\n*N is the number of points in the dataset and K is the number of clusters defined.  \\n\\nThe k-means algorithms have problems like defining the number of clusters initially, susceptibility to \\n\\nlocal optima, and sensitivity to outliers, memory space and unknown number of iteration steps that are \\n\\nrequired to cluster.  The fuzzy C means clustering are really suitable for handling the issues related to \\n\\nunderstand ability of patterns, incomplete/noisy data, mixed media information, human interaction and \\n\\nit can provide approximate solutions faster. They have been mainly used for discovering association \\n\\nrules and functional dependencies as well as image retrieval. However the time complexity of K \\n\\nmeans is much less than that of FCM thus K means works faster than FCM [191]. \\n\\nSome of the advantages of partition based algorithms includes that they are (i) relatively scalable and \\n\\nsimple and (ii) suitable for datasets with compact spherical clusters that are well-separated. However, \\n\\ndisadvantages with these algorithms include poor (i) cluster descriptors (ii) reliance on the user to \\n\\nspecify the number of clusters in advance (iii) high sensitivity to initialization phase, noise and outliers \\n\\nand (iv) inability to deal with non-convex clusters of varying size and density [175]. \\n\\n3 Measures of Similarities \\n\\nSimilarity of objects within a cluster plays the most important role in clustering process. A good \\n\\ncluster finds maximum similarity among its objects. The measure of similarity in cluster is mainly \\n\\ndecided by the distance among its members. In a conventional cluster (non-fuzzy), a member either \\n\\nbelongs to a cluster wholly or not at all. Many clustering methods use distance measures to determine \\n\\nthe similarity or dissimilarity between any pair of objects [22]. It is useful to denote the distance \\n\\nbetween two instances xi and xj as: d(xi, xj). A valid distance measure should be symmetric i.e d(xi, xj) \\n\\n= d(xj, xi) and obtain its minimum value (ideally zero) in case of identical vectors. The distance \\n\\nmeasure is called a metric distance measure if it also satisfies the following properties: \\n\\nTriangle inequality   \\n( , ) ( , ) ( , ) , ,i k i j j k i j kd x x d x x d x x x x x S\\uf0a3 \\uf02b \\uf022 \\uf0ce\\n\\n                              (10) \\n\\n( , ) 0 ,i j i j i jd x x x x x x S\\uf03d \\uf0de \\uf03d \\uf022 \\uf0ce\\n                                                (11) \\n\\n\\n\\n3.1 Minkowski: Distance Measures for Numeric Attributes \\n\\nA measurement of distance is a fundamental operation in the unsupervised learning process [91]. \\n\\nSmaller is the distance between any two objects; closer these objects are assumed on the basis of \\n\\nsimilarity. A family of distance measures is the Minkowski metrics [29], where the distance is \\n\\nmeasured by following equation  r\\nd\\n\\nk\\n\\nr\\n\\njkikr\\nxxij\\n\\n/1\\n\\n1 \\uf0fe\\n\\uf0fd\\n\\uf0fc\\n\\n\\uf0ee\\n\\uf0ed\\n\\uf0ec\\n\\n\\uf02d\\uf03d \\uf0e5\\n\\uf03d                                                  (12) \\n\\nwhere xik is the value of the k-th variable for entity i, xjk is the value of the k-th variable for entity j. The \\n\\nmost popular and common distance measure is the Euclidean or L2 norm (r =2). More details on \\n\\nunsupervised classification for various non-Euclidean distances can be seen in Saxena et al. [160].   \\n\\n3.2 Cosine Measure \\n\\nCosine Measure [153] is a popular similarity score in text mining and information retrieval [152]. \\n\\nThe normalized inner product for Cosine measure is defined as:  \\n\\n( , )\\n\\nT\\n\\ni j\\n\\ni j\\n\\ni j\\n\\nx x\\nd x x\\n\\nx x\\n\\n\\uf0d7\\n\\uf03d\\n\\n\\uf0d7\\n                                                                 (13) \\n\\n3.3 Pearson Correlation Measure  \\n\\nCorrelation coefficient is first discovered by Bravais [154] and later shown by Person [155]. The \\n\\nnormalized Pearson correlation for two vectors xi and xj is defined as: \\uf028 \\uf029 \\uf028 \\uf029\\n( , )\\n\\nT\\n\\ni i j j\\n\\ni j\\n\\ni i j j\\n\\nx x x x\\nd x x\\n\\nx x x x\\n\\n\\uf02d \\uf0d7 \\uf02d\\n\\uf03d\\n\\n\\uf02d \\uf0d7 \\uf02d\\n                                                      (14) \\n\\nwhere ix\\n denotes the average feature value of x over all dimensions. \\n\\n3.4 Extended Jaccard Measure \\n\\nStrehl and Ghosh [107] represented the extended Jaccard measure as follows: \\n22\\n\\n( , )\\n\\nT\\n\\ni j\\n\\ni j\\nT\\n\\ni j i j\\n\\nx x\\nd x x\\n\\nx x x x\\n\\n\\uf0d7\\n\\uf03d\\n\\n\\uf02b \\uf02d \\uf0d7\\n                                                          (15) \\n\\n3.5 Dice Coefficient Measure \\n\\nIt was independently developed by the Thorvald Sørensen[156] and Raymond Dice [157] The \\n\\ndice coefficient measure is similar to the extended Jaccard measure and it is defined as: \\n22\\n\\n2\\n( , )\\n\\nT\\n\\ni j\\n\\ni j\\n\\ni j\\n\\nx x\\nd x x\\n\\nx x\\n\\n\\uf0d7\\n\\uf03d\\n\\n\\uf02b\\n                                                                  (16) \\n\\n3.6  Choice of Suitable Similarity Measure \\n\\nThe measures of similarities have been applied on millions of applications in clustering. In fact every \\n\\nclustering problem applies one of the similarity measures. The Euclidean distance is mostly applied to \\n\\nfind similarity between two objects, which are expressed numerically. Euclidean distance is highly \\n\\nsensitive to noise and usually not applied to data with hundreds of attributes also features with high \\n\\nvalues tend to dominate others [50] so it may be applied when translations of non-numeric objects to \\n\\nnumeric values are almost nil or minimum. Jaccard similarity coefficient is suitable sufficiently to be \\n\\nemployed in the documents or word similarity measurement. In efficiency measurement, the program \\n\\nperformance can deal appropriately with high stability when failure and mistake spelling occurred. \\n\\nNevertheless, this method is not able to detect the over-type words in the data sets [192]. Pearson \\n\\ncorrelation is usually unable to detect the difference between two variables [50]. Cosine similarity is \\n\\nhttp://en.wikipedia.org/wiki/Thorvald_S%C3%B8rensen\\nhttp://en.wikipedia.org/w/index.php?title=Lee_Raymond_Dice&action=edit&redlink=1\\n\\n\\nalso a good choice for document clustering, it is invariant to rotation but not to linear transformations \\n\\n[50].  \\n\\n4 Variants of Clustering Methods \\n\\n4.1 Graph (Theoretic) Clustering  \\n\\nThe graph theoretic clustering is a method that represents clusters via graphs. The edges of the \\n\\ngraph connect the instances represented as nodes. A well-known graph-theoretic algorithm is based on \\n\\nthe minimal spanning tree (MST) [46]. Inconsistent edges are edges whose weight (in the case of \\n\\nclustering length) is significantly larger than the average of nearby edge lengths. Another graph \\n\\ntheoretic approach constructs graphs based on limited neighbourhood sets [47]. The graph theoretic \\n\\nclustering is convenient to represent clusters via graphs but is weak in handling outliers especially in \\n\\nMST as well as detecting overlapping of clusters [176]. \\n\\n      The graph clustering [177] involves the task of dividing nodes into clusters, so that the edge \\n\\ndensity is higher within clusters as opposed to across clusters. A natural, classic and popular statistical \\n\\nsetting for evaluating solutions to this problem is the stochastic block model, also referred to as the \\n\\nplanted partition model. The general graph l-partition problem is to partition the nodes of an \\n\\nundirected graph into l equal-sized groups so as to minimize the total number of edges that cross \\n\\nbetween groups. Condon [178] presented a simple, linear-time algorithm for the graph l-partition \\n\\nproblem and analyzed it on a random “planted l-partition” model. In this model, the n nodes of a graph \\n\\nare partitioned into l groups, each of size n/l; two nodes in the same group are connected by an edge \\n\\nwith some probability p, and two nodes in different groups are connected by an edge with some \\n\\nprobability r<p. They showed that if p−r≥n−1/2+ϵ for some constant ϵ, then the algorithm finds the \\n\\noptimal partition with probability 1− exp(−nΘ(ε)). Graph clustering decomposes a network into sub \\n\\nnetworks based on some topological properties. In general we look for dense sub networks as shown \\n\\nin Fig. 8. \\n\\n \\n\\nFig. 8 Sub-network clustering of graph \\n\\n      Spectral Clustering, proposed by Donath and Hoffman [179], is an emerging technique under \\n\\ngraph clustering which consists of algorithms cluster points using eigenvectors of matrices derived \\n\\nfrom the data. In the machine learning community, spectral clustering has been made popular by the \\n\\nworks of Shi and Malik [180]. A useful tutorial is available on spectral clustering by Luxburg [181]. \\n\\nThe success of spectral clustering is mainly based on the fact that it does not make strong assumptions \\n\\non the form of the clusters. As opposed to k-means, where the resulting clusters form convex sets (or, \\n\\nto be precise, lie in disjoint convex sets of the underlying space), spectral clustering can solve very \\n\\n\\n\\ngeneral problems like intertwined spirals. Moreover, spectral clustering can be implemented \\n\\nefficiently even for large data sets, as long as we make sure that the similarity graph is sparse. Once \\n\\nthe similarity graph is chosen, we just have to solve a linear problem, and there are no issues of getting \\n\\nstuck in local minima or restarting the algorithm for several times with different initializations. \\n\\nHowever, we have already mentioned that choosing a good similarity graph is not trivial, and spectral \\n\\nclustering can be quite unstable under different choices of the parameters for the neighborhood graphs. \\n\\nSo spectral clustering cannot serve as a “black box algorithm” which automatically detects the correct \\n\\nclusters in any given data set. But it can be considered as a powerful tool which can produce good \\n\\nresults if applied with care [181]. More literature (partially) on graph and spectral clustering can be \\n\\nseen in [182-190]. \\n\\n4.2 Spectral Clustering Algorithms [181] \\n\\nNow we would like to state the most common spectral clustering algorithms. We assume that our \\n\\ndata consists of n “points” x1, . . . , xn, which can be arbitrary objects. We measure their pair wise \\n\\nsimilarities sij = s(xi , xj ) by some similarity function which is symmetric and non-negative, and we \\n\\ndenote the corresponding similarity matrix by S = (sij )I, j=1, ..., n. \\n\\n4.2.1 Un-normalized Spectral Clustering \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n4.2.2 Normalized Spectral Clustering According to Shi and Malik (2000)[180] \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n1. Input: Similarity matrix S ∈ R n×n, number k of clusters to construct. \\n\\n2. Construct a similarity graph by one of the ways described in Section 2 [181]. Let W be its \\n\\nweighted adjacency matrix. \\n\\n3. Compute the un-normalized Laplacian L. \\n\\n4. Compute the first k eigenvectors u1, . . . , uk of L. \\n\\n5. Let U ∈ R n×k be the matrix containing the vectors u1, . . . , uk as columns. \\n\\n6. For i = 1, . . . , n, let yi ∈ R k be the vector corresponding to the i-th row of U. \\n\\n7. Cluster the points (yi)i=1,...,n  in R k with the k-means algorithm into clusters C1, . . . , Ck. \\n\\n8. Output: Clusters A1, . . . , Ak with Ai = {j| yj ∈ Ci}. \\n\\n \\n\\n1. Input: Similarity matrix S ∈ R n×n, number k of clusters to construct.  \\n\\n2. Construct a similarity graph by one of the ways described in Section 2 [181]. Let W be its \\n\\nweighted adjacency matrix.  \\n\\n3. Compute the unnormalized Laplacian L. \\n\\n4. Compute the first k generalized eigenvectors u1, . . . , uk of the generalized eigen problem \\n\\nLu = λDu. \\n\\n5. Let U ∈ R n×k be the matrix containing the vectors u1, . . . , uk as columns.  \\n\\n6. For i = 1, . . . , n, let yi ∈ R k be the vector corresponding to the i-th row of U.  \\n\\n7. Cluster the points (yi)i=1,...,n in R k with the k-means algorithm into clusters C1, . . . , Ck.  \\n\\n8. Output: Clusters A1, . . . , Ak with Ai = {j| yj ∈ Ci}. \\n\\n\\n\\n4.3 Model Based Clustering Methods \\n\\nModel based clustering methods optimize as well as find the suitability of given data with some \\n\\nmathematical models. Similar to conventional clustering; model-based clustering methods also detect \\n\\nfeature details for each cluster, where each cluster represents a concept or class. Decision trees and \\n\\nneural networks are two most frequently used induction methods. \\n\\n(i) Decision Trees \\n\\nThe representation of data in decision tree [19] is modelled by a hierarchical tree, in which \\n\\neach leaf denotes a concept and implies a probabilistic description of that concept. There are \\n\\nmany algorithms, which produce classification trees for defining the unlabelled data. Number \\n\\nof algorithms that have been proposed for conceptual clustering are follows: CLUSTER/2 by \\n\\nMichalski and Stepp [93], COBWEB by Fisher [48], CYRUS by Kolodner [95], GALOIS by \\n\\nCarpineto and Romano [96], GCF by Talavera and Béjar [97], INC by Hadzikadic and Yun \\n\\n[98], ITERATE by Biswas, Weinberg and Fisher [99], LABYRINTH by Thompson and \\n\\nLangley [100], SUBDUE by Jonyer, Cook and Holder [101], UNIMEM by Lebowitz [102] \\n\\nand WITT by Hanson and Bauer [103]. COBWEB is one of the best known algorithms, where \\n\\neach concept defines a set of objects and each object defined as a binary values property list. \\n\\nIts aim is to achieve high predictability of nominal variable values, given a cluster. This \\n\\nalgorithm is not suitable for clustering large database data [48].  \\n\\n(ii) Neural Networks \\n\\n Neural networks [49] represent each cluster by a neuron, whereas input data is also \\n\\nrepresented by neurons, which are connected to the prototype neurons. Each connection is \\n\\nattributed by some weight, which is initialized randomly before learning of these weights \\n\\nadaptively. A very popular neural algorithm for clustering is the self-organizing map (SOM) \\n\\n[104, 105]. SOM is commonly used for vector quantization, feature extraction and data \\n\\nvisualization along with clustering analysis. This algorithm constructs a single-layered \\n\\nnetwork as shown in Fig. 9. The learning process takes place in a “winner-takes-all” fashion: \\n\\nThe prototype neurons compete for the current instance. The winner is the neuron whose \\n\\nweight vector is closest to the instance currently presented. The winner and its neighbours \\n\\nlearn by having their weights adjusted. While SOFMs has the merits of input space density \\n\\napproximation and independence of the order of input patterns, a number of user dependent \\n\\nparameters cause problems when applied in real practice. Like the K-means algorithm,SOFM \\n\\nneed to predefine the size of the lattice, i.e., the number of clusters, which is unknown for \\n\\nmost circumstances. Additionally, trained SOFM may be suffering from input space density \\n\\nmis representation [49], where areas of low pattern density may be over represented and areas \\n\\nof high density under represented [50]. \\n\\n \\n\\nFig. 9 Model of a single layered network \\n\\n4.4 Mixture Density-Based Clustering  \\n\\nXu and Wunsch [50, 51] described clustering in the perspective of probability that data objects are \\n\\ndrawn from a specific probability distribution and the overall distribution of the data is assumed to be \\n\\na mixture of several distributions [53]. Data points [117] can be derived from different types of density \\n\\nhttp://en.wikipedia.org/wiki/Cobweb_(clustering)\\n\\n\\nfunctions (e.g., multivariate Gaussian or t-distribution), or from the same families but with different \\n\\nparameters. The aim of these methods is to identify the clusters and their distribution. Cheeseman and \\n\\nStutz introduced an algorithm named AUTOCLASS [55], which is widely used and covers a broad \\n\\nvariety of distributions, including Gaussian, Bernoulli, Poisson, and log-normal distributions. Ester et \\n\\nal. [54] demonstrated an algorithm called DBSCAN (density-based spatial clustering of applications \\n\\nwith noise), which discovers clusters of arbitrary shapes and is efficient for large spatial databases.  \\n\\nOther well-known density-based techniques are: SNOB proposed by Wallace and Dowe in 1994 \\n\\n[56] and MCLUST introduced by Fraley and Raftery in 1998 [27]. Among these methods, the \\n\\nexpectation-maximization (EM) algorithm is the most popular [52, 56]. For EM algorithm, the log \\n\\nlikelihood function to maximize is as follows: \\nln ( | ) ln ( , | )\\n\\nY\\n\\np X p X Y\\uf051 \\uf03d \\uf051\\uf0e5\\n                                                            (17) \\n\\nwhere X denotes the set of all observed data \\n\\uf07b \\uf07d\\uf028 \\uf0291,..., NX x x\\uf03d\\n\\n, and Y denotes the set of all latent \\n\\nvariables \\n\\uf07b \\uf07d\\uf028 \\uf0291,..., NY y y\\uf03d\\n\\n. The complete data set is formed as \\n\\uf028 \\uf029 \\uf028 \\uf029\\uf07b \\uf07d, ,i iX Y x y\\uf03d\\n\\n and the joint \\n\\ndistribution \\n\\uf028 \\uf029, |p x y \\uf051\\n\\n is ruled by a set of parameters.  The major disadvantages for EM algorithm are \\n\\nthe sensitivity to the selection of initial parameters, the effect of a singular co-variance matrix, the \\n\\npossibility of convergence to a local optimum, and the slow convergence rate [50] [52]. \\n\\nProcedure of EM algorithm \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n4.5 Grid-Based Clustering Methods  \\n\\nThese methods partition the space into a finite number of cells that form a grid structure on which \\n\\nall of the operations for clustering are performed. The main advantage of the approach is its fast \\n\\nprocessing time [122], no need of distance computations and easy to determine which clusters are \\n\\nneighbouring.  \\n\\nThe basic steps of Grid based algorithm \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\nThere are many others interesting grid based techniques including: STING (statistical information \\n\\ngrid approach) by Wang, Yang and Muntz [57] in 1997, one of the highly scalable algorithm and has \\n\\nthe ability to decompose the data set into various levels of detail. STING retrieves spatial data and \\n\\ndivides into rectangular cells corresponding to different levels of resolution as shown in Fig. 10. \\n\\n \\n\\n1. Initialize the parameters \\nold\\uf051   \\n\\n2. E step: evaluate ( | , )oldp Y X \\uf051  \\n\\n3. M step: re-estimate the parameters \\narg max ( )new L\\uf051\\uf051 \\uf03d \\uf051\\n\\n \\n\\n4. Check for convergence. If the convergence criterion is not \\n\\nsatisfied, let \\nold new\\uf051 \\uf0ac\\uf051 and return to step 2.  \\n\\n \\n\\n1. Define a set of grid cells \\n\\n2. Assign objects to the appropriate grid cell and compute \\n\\nthe density of each cell \\n\\n3. Eliminate cells, whose density is below a certain \\n\\nthreshold \\n\\n4. Form clusters from contiguous groups of dense cells \\n\\n \\n\\n\\n\\n \\n\\nFig. 10 Rectangular cells corresponding to different levels of resolution \\n\\nEach cell at a higher level is partitioned into a number of smaller cells in the next lower level. Then \\n\\nmean, variance, minimum, maximum of each cell is computed by using the normal and uniform \\n\\ndistribution. Statistical information of each cell is calculated and stored in advance and it uses a top \\n\\ndown approach to answer spatial data queries. Wave Cluster [58] introduced by Sheikholeslami et al. \\n\\n[58] uses multi-resolution approach like STING and allows natural clustering to become more \\n\\ndistinguishable. It uses a signal processing technique that decomposes a signal into different frequency \\n\\nsub-band and data are transformed to preserve relative distance between objects at different levels of \\n\\nresolution. It is highly scalable and can handle outliers well. It is not suitable for high dimensional data \\n\\nset. It can be considered as both grid-based and density-based. CLIQUE is developed by Agrawal et \\n\\nal. [59] in 1998, which can be considered as both density-based and grid based clustering methods. It \\n\\nautomatically finds subspaces of high dimensional data space that allow better clustering than original \\n\\nspace. The accuracy of the clustering result may be degraded at the expense of simplicity of the \\n\\nmethod CLIQUE. \\n\\n4.6 Evolutionary Approaches Based Clustering Methods \\n\\nThe famous evolutionary approaches [60] include evolution strategies (ES) [61], evolutionary \\n\\nprogramming (EP) [62], genetic algorithm (GA) [63, 64], particle swarm optimization (PSO) [65-66], \\n\\nant colony optimization (ACO) [67] etc. \\n\\nThe common approach of evolutionary techniques to data clustering is as follows: \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\nOut of these approaches, GA has been most frequently used in clustering, where solutions are in \\n\\nthe form of binary strings. In GAs, a selection operator propagates solutions from the current \\n\\ngeneration to the next generation based on their fitness Selection employs a probabilistic scheme so \\n\\n1. Choose a random population of solutions. Each solution here corresponds \\n\\nto valid k partitions of the data.  \\n\\n2. Associate a fitness value with each solution. Typically fitness is inversely \\n\\nproportional to the squared error value. Higher the error, smaller the \\n\\nfitness and vice versa. \\n\\n3. A solution with a small squared error will have a larger fitness value.  \\n\\n4. Use the evolutionary operators’ viz.  selection, recombination and \\n\\nmutation to generate the next population of solutions. \\n\\n5. Evaluate the fitness values of these solutions. \\n\\n6. Repeat step until some termination condition is satisfied.  \\n\\n \\n\\n\\n\\nthat solutions with higher fitness have a higher probability of getting reproduced. A major problem \\n\\nwith GAs is their sensitivity to the selection of various parameters such as population size crossover \\n\\nand mutation probabilities etc. Grefenstette [123] has studied this problem and suggested guidelines \\n\\nfor selecting these control parameters.  \\n\\nThe general steps of GA for clustering are: \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n4.7 Search Based Clustering Approaches  \\n\\nSearch techniques are basically used to obtain the optimum value (minimum or maximum) of the \\n\\ncriterion function (e.g. distance) called objective function also. The search based approaches are \\n\\ncategorized into stochastic and deterministic search techniques. The stochastic search techniques can \\n\\nevolve an approximate optimal solution (based on fitness value). Most of the stochastic techniques are \\n\\nevolutionary approaches based. The rest of the search techniques come under deterministic search \\n\\ntechniques which guarantee an optimal solution by performing exhaustive enumeration. The \\n\\ndeterministic approaches are typically greedy descent approaches. The stochastic search techniques are \\n\\neither sequential or parallel such as simulated annealing (SA) [172] while evolutionary approaches are \\n\\ninherently parallel. Simulated annealing procedures are designed to avoid or recover from solutions \\n\\nwhich correspond to local optima of the objective functions. This is accomplished by accepting with \\n\\nsome probability a new solution for the next iteration of lower quality as measured by the criterion \\n\\nfunction. The probability of acceptance is governed by a critical parameter called the temperature by \\n\\nanalogy with annealing in metals which is typically specified in terms of a starting first iteration and \\n\\nfinal temperature value. Al Sultan et al [92] studied the effects of control parameters on the \\n\\nperformance of the algorithm and used SA to obtain near optimal partition of the data SA is \\n\\nstatistically guaranteed to find the global optimal solution. \\n\\nThe SA algorithm can be slow in reaching the optimal solution because optimal results require the \\n\\ntemperature to be decreased very slowly from iteration to iteration. Tabu search [68, 69] like SA is a \\n\\nmethod designed to cross boundaries of feasibility or local optimality and to systematically impose \\n\\nand release constraints to permit exploration of otherwise forbidden regions. Tabu search was used to \\n\\nsolve the clustering problem in [3]. \\n\\n4.8 Collaborative Fuzzy Clustering \\n\\nThis is relatively a recent type of clustering which has various applications. The database is \\n\\ndistributed on several sites. The collaborative clustering proposed by Pedrycz [70-73] concerns a \\n\\nprocess of revealing a structure being common or similar to a number of subsets. There are mainly two \\n\\nforms of collaborative clustering; horizontal and vertical collaborative clustering [74]. In horizontal \\n\\ncollaborative clustering, same database is split into different subsets of features, each subset having all \\n\\npatterns in the database. The horizontal collaborative clustering has been applied for Mamdani type \\n\\nfuzzy inference system [124] in order to decide some association between datasets. In vertical \\n\\ncollaborative clustering, database is divided into subsets of patterns such that each pattern of any \\n\\nsubset has all features.  \\n\\nInput: S (instance set), K (number of clusters), n (population size) \\n\\nOutput: clusters \\n\\n1. Randomly create a population of n structures; each \\n\\ncorresponds to valid K-clusters of the data. \\n\\n2. repeat \\n\\na. Associate a fitness value ∀ structure ∈ population. \\n\\nb. Regenerate a new generation of structures. \\n\\n3. until some termination condition is satisfied \\n\\n \\n\\n\\n\\nThe objective function for horizontal collaboration technique is explained in Eq. (13). For vertical \\n\\ncollaboration technique, please refer [73]: \\n2 2 2 2\\n\\n1 1 1 1 1\\n\\n[ ] [ ] [ ] [ , ] { [ ] [ ]} [ ]\\npN c N n\\n\\nij ij ij ij ij\\n\\ni j m i j\\nm l\\n\\nQ l u l d l l m u l u m d l\\uf062\\n\\uf03d \\uf03d \\uf03d \\uf03d \\uf03d\\n\\n\\uf0b9\\n\\n\\uf03d \\uf02b \\uf02d\\uf0e5\\uf0e5 \\uf0e5 \\uf0e5\\uf0e5\\n                          (18) \\n\\nwhere \\uf062  is a user defined parameter based on datasets ( \\uf062 >0), [ , ]l m\\uf062  denotes the collaborative \\n\\ncoefficient with collaborative effect on dataset l through m, c is a number of cluster. 1,2, ,l P\\uf03d . P  \\n\\nis a number of datasets, N  is the number of patterns in the dataset, u represents the partition matrix, n  \\nis a number of features, and d  is an Euclidean distance between patterns and prototypes. \\n\\n    The general scheme of collaborative clustering is shown in Fig. 11, which demonstrates the \\nconnections of matrices in order to accomplish the collaboration between the subsets of the dataset. \\nFirst, we solve the problem for each dataset separately and allow the results to interact globally by \\nforming a collaborative process between the datasets. Collaborative fuzzy partitioning is carried out \\nthrough an iterative optimization of the objective function as shown in Eq. (13). The optimization of \\nQ[l] involves the determination of the partition matrix U and the prototypes V of different data sets as \\nshown in Fig. 11(a) and (b). \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n(a) Collaborative clustering scheme for two datasets (b) Collaborative clustering scheme for three datasets \\n\\nFig. 11 Collaborative clustering scheme \\n\\n4.9 Multi Objective Clustering \\n\\nIn case of multi-objective clustering, many clustering approaches are optimized simultaneously. \\n\\nIn multi-objective clustering with automatic k-determination (MOCK) [78, 79], compactness of \\n\\nclusters is maximized as the first objective while the connectivity of the clusters is maximized as the \\n\\nsecond objective. The Pareto [80] approach is used to optimize the aforesaid two objectives \\n\\nsimultaneously. The multi objective clustering ensemble (MOCE) proposed by Faceili et.al [81] uses \\n\\nMOCK along with a special crossover operator which utilizes ensemble clustering. In Law et. al [82], \\n\\ndifferent clustering methods with different objectives are used. Some more surveys can be seen in \\n\\n[50]. \\n\\n4.10 Overlapping Clustering or Overlapping Community Detection \\n\\nThe partition clustering usually indicates exclusive and overlapping clustering algorithms (like k-\\n\\nmeans discussed above) such that each member or the object belongs to just one cluster. When an \\n\\nobject belongs to more than one cluster, it becomes overlapping clustering method or algorithm, e.g. \\n\\nfuzzy c-means clustering. Nowadays, community detection, as an effective way to reveal the \\n\\nrelationship between structure and function of networks, has drawn lots of attention and been well \\n\\ndeveloped [195]. Networks are modeled as graphs, where nodes represent objects and edges represent \\n\\ninteractions among them. Community detection divides a network into groups of nodes, where nodes \\n\\nare densely connected inside but sparsely connected outside. However, in real world, objects often \\n\\nhave diverse roles and belong to multiple communities. For example, a professor collaborates with \\n\\nresearchers in different fields and a person has his family group as well as friend group at the same \\n\\n\\n\\ntime.  In community detection, these objects should be divided into multiple groups, which are known \\n\\nas overlapping nodes [196]. The aim of overlapping community detection is to discover such \\n\\noverlapping nodes and communities. Until now, lots of overlapping community detection approaches \\n\\nhave been proposed, which can be roughly divided into two categories: node-based and link-based \\n\\nalgorithms. The node-based overlapping community detection algorithms [75, 76] directly divide \\n\\nnodes of the network into different communities. Based on an intuition that a link in networks usually \\n\\nrepresents the unique relation, the link-based algorithms firstly cluster on edges of network, and then \\n\\nmap the link communities to node communities by gathering nodes incident to all edges within each \\n\\nlink community [77]. The newly proposed link-based algorithms have shown its superiority on \\n\\ndetecting complex multi-scale communities. However, they have the high computational complexities \\n\\nand bias on the discovered communities. Shi et. al. [196] proposed a genetic algorithm, GaoCD, for \\n\\noverlapping community detection based on the link clustering framework. Different from those node-\\n\\nbased overlapping community detection algorithms, GaoCD utilized the property of the unique role of \\n\\nlinks and applies a novel genetic algorithm to cluster on edges. Experiments on artificial and real \\n\\nnetworks showed that GaoCD can effectively reveal overlapping structure. \\n\\n5 Evaluation Criteria \\n\\nThe formation of clusters is an important process. However, it is also meaningful to test the \\n\\nvalidity and accuracy of the clusters so formed by any method. It should be tested whether the clusters \\n\\nformed by a certain method show maximum similarity among the objects in the same cluster and \\n\\nminimum similarity among those in other clusters. Recently, many evaluation criteria have been \\n\\ndeveloped. These criteria are divided mainly into two categories: Internal and External. \\n\\n5.1 Internal Quality Criteria Measures \\n\\nInternal Criteria generally measure the compactness of the clusters by applying similarity measure \\n\\ntechniques. In general, it measures the inter-cluster separability and intra-cluster homogeneity, or a \\n\\ncombination of these two.  \\n\\n5.1.1 Sum of Squared Error \\n\\nSum of Square Error (SSE) [158, 159] is the most frequently used criterion measure for clustering. \\n\\nIt is defined as: \\n2\\n\\n1 i k\\n\\nK\\n\\ni k\\n\\nk x C\\n\\nSSE x \\uf06d\\n\\uf03d \\uf022 \\uf0ce\\n\\n\\uf03d \\uf02d\\uf0e5 \\uf0e5\\n                                                            (19) \\n\\nwhere Ck is the set of instances in cluster k; μk is the vector mean of cluster k.  \\n\\n5.1.2 Scatter Criteria \\n\\nThe scatter criteria matrix [1, 22] is defined as follows for the k-th cluster: \\n( )( )\\n\\nk\\n\\nT\\n\\nk k k\\n\\nx C\\n\\nS x x\\uf06d \\uf06d\\n\\uf0ce\\n\\n\\uf03d \\uf02d \\uf02d\\uf0e5\\n                                                          (20) \\n\\n5.1.3 Condorcet’s Criterion.  \\n\\nThe Condorcet’s criterion [110] is another approach to apply for the ranking problem [111]. The \\n\\ncriterion is defined as follows: \\n, ;\\n\\n( , ) ( , )\\ni j k i i j i k i\\n\\nj k\\n\\nj k j k\\n\\nC C x x C C C x C x C\\n\\nx x\\n\\ns x x d x x\\n\\uf0ce \\uf0ce \\uf0ce \\uf0ce \\uf0cf\\n\\n\\uf0b9\\n\\n\\uf02b\\uf0e5 \\uf0e5 \\uf0e5 \\uf0e5\\n\\n                                    (21) \\n\\nwhere s(xj , xk) and d(xj , xk) measure the similarity and distance of the vectors xj and xk. \\n\\n\\n\\n5.1.4 The C-criterion  \\n\\nFortier and Solomon [108] defined the C-criterion, which is an extension of Condorcet’s criterion \\n\\nand it is defined as: \\n, ;\\n\\n( ( , ) ) ( ( , ))\\ni j k i i j i k i\\n\\nj k\\n\\nj k j k\\n\\nC C x x C C C x C x C\\n\\nx x\\n\\ns x x s x x\\uf067 \\uf067\\n\\uf0ce \\uf0ce \\uf0ce \\uf0ce \\uf0cf\\n\\n\\uf0b9\\n\\n\\uf02d \\uf02b \\uf02d\\uf0e5 \\uf0e5 \\uf0e5 \\uf0e5\\n\\n                           (22) \\n\\nwhere γ is a threshold value. \\n\\n5.1.5 Category Utility Metric  \\n\\nThe category utility defined in [109, 112] which measures the goodness of category. A set of \\n\\nentities with size n binary feature set F= {fi}, i=1, …, n and a binary category { , }C c c\\uf03d is calculated \\n\\nas follows: \\n\\n1 1 1\\n\\n( , ) ( ) ( | ) log ( | ) ( ) ( | ) log ( | ) ( ) log ( )\\nn n n\\n\\ni i i i i i\\n\\ni i i\\n\\nCU C F p c p f c p f c p c p f c p f c p f p f\\n\\uf03d \\uf03d \\uf03d\\n\\n\\uf0e9 \\uf0f9\\n\\uf03d \\uf02b \\uf02d\\uf0ea \\uf0fa\\n\\uf0eb \\uf0fb\\n\\n\\uf0e5 \\uf0e5 \\uf0e5\\n   (23) \\n\\nwhere p(c) is the prior probability of an entity belonging to the positive category c, \\n( | )ip f c\\n\\nis the \\n\\nconditional probability of an entity having feature fi given that the entity belongs to category c, \\n( | )ip f c is likewise the conditional probability of an entity having feature fi given that the entity \\n\\nbelongs to category c , and p(fi) is the prior probability of an entity processing feature fi.  \\n\\n5.1.6 Edge Cut Metrics \\n\\nAn edge cut minimization problem [125, 126] is very useful in some cases for dealing with \\n\\nclustering problems. In this case, the cluster quality is measured as the ratio of the remaining edge \\n\\nweights to the total precut edge weights. Finding the optimal value is easy with edge cut minimization \\n\\nproblem, where there is no restriction on the size of the clusters. \\n\\n5.2 External Quality Criteria Measures \\n\\nIn order to match the structure of cluster to a predefined classification of the instances, the \\n\\nexternal quality criteria measure can be useful. \\n\\n5.2.1 Mutual Information Based Measure \\n\\nStrehl et al [113] proposed mutual information based measure, which can be used as an external \\n\\nmeasure for clustering. The criteria measure for m instances clustered using C = {C1,….,Cg} and \\n\\nreferring to the target attribute z whose domain is dom(z) = {c1,….,ck} is defined as follows: \\n,\\n\\n, .\\n\\n1 1 ., ,.\\n\\n2\\nlog\\n\\ng k\\nl h\\n\\nl h g k\\n\\nl h l l\\n\\nm m\\nC m\\n\\nm m m\\uf03d \\uf03d\\n\\n\\uf0e6 \\uf0f6\\uf0d7\\n\\uf03d \\uf0e7 \\uf0f7\\uf0e7 \\uf0f7\\uf0d7\\uf0e8 \\uf0f8\\n\\n\\uf0e5\\uf0e5\\n                                             (24) \\n\\nwhere ml,h indicates the number of instances that are in cluster Cl  and also in class ch. m.,h denotes the \\n\\ntotal number of instances in the class ch. Similarly, ml,.  Indicates the number of instances in cluster Cl. \\n\\n5.2.2 Rand Index \\n\\nThe Rand index [115] is a simple criterion used to compute how similar the clusters are to the \\n\\nbenchmark classifications. The Rand index is defined as: \\nTP TN\\n\\nRAND\\nTP FP FN TN\\n\\n\\uf02b\\n\\uf03d\\n\\n\\uf02b \\uf02b \\uf02b                                                      (25) \\n\\nwhere TP is the number of true positives, TN is the number of true negatives, FP is the number of \\n\\nfalse positives and FN is the number of false negatives. The Rand index lies between 0 and 1. When \\n\\nthe two partitions agree perfectly, the Rand index is 1.  \\n\\n\\n\\n5.2.3 F-measure \\n\\nIn Rand index, the false positives and false negatives are equally weighted and this may cause for \\n\\nan undesirable features for some clustering applications. The F-measure [116] addresses this concern \\n\\nand used to balance of false negatives by weighting recall parameter 0\\uf068 \\uf0b3 . The F-measure is defined \\n\\nas follows: 2\\n\\n2\\n\\n( 1) P R\\nF\\n\\nP R\\n\\n\\uf068\\n\\n\\uf068\\n\\n\\uf02b \\uf0d7 \\uf0d7\\n\\uf03d\\n\\n\\uf0d7 \\uf02b                                                                 (26) \\n\\nwhere P is the precision rate and R is the recall rate. Recall has no impact when 0\\uf068 \\uf03d and increasing η \\n\\nallocates an increasing amount of weight to recall in the final F-measure. Precision and Recall [119, \\n\\n120] is defined as follows: TP\\nP\\n\\nTP FP\\n\\uf03d\\n\\n\\uf02b                                                                       (27) \\nTP\\n\\nR\\nTP FN\\n\\n\\uf03d\\n\\uf02b                                                                         (28) \\n\\n5.2.4 Jaccard Index \\n\\nThe Jaccard index [121] is considered to identify the equivalency between two datasets. The \\n\\nJaccard index is defined as follows: \\n| |\\n\\n( , )\\n| |\\n\\nA B TP\\nJ A B\\n\\nA B TP FP FN\\n\\n\\uf0c7\\n\\uf03d \\uf03d\\n\\n\\uf0c8 \\uf02b \\uf02b                                              (29) \\n\\nIf A and B are both empty, then ( , ) 1J A B \\uf03d , i.e 0 ( , ) 1J A B\\uf0a3 \\uf0a3 . This is simply the number of unique \\n\\nelements common to both sets divided by the total number of unique elements in both sets. \\n\\n5.2.5 Fowlkes–Mallows Index \\n\\nThe Fowlkes-Mallows index [118] determines the similarity between the clusters obtained after \\n\\nthe clustering algorithm. The higher value of the Fowlkes-Mallows index indicates a more similarity \\n\\nbetween the clusters. It can be determined as follows: \\nTP TP\\n\\nFM\\nTP FP TP FN\\n\\n\\uf03d \\uf0d7\\n\\uf02b \\uf02b                                                    (30) \\n\\n5.2.6 Confusion Matrix \\n\\nA confusion matrix is also known as a contingency table or an error matrix [114]. It can be used \\n\\nto quickly visualize the results of a clustering. If a classification system has been trained to distinguish \\n\\nbetween apples, oranges and tomatoes, a confusion matrix will summarize the results of testing the \\n\\nalgorithm for further inspection. Assuming a sample of 27 fruits; 8 apples, 6 oranges, and 13 tomatoes, \\n\\nthe result of confusion matrix look like the table below: \\n\\nTable 3 Confusion Matrix \\n \\n\\nActual class \\n\\nPredicted class \\n\\nApple Orange Tomato \\n\\nApple 5 3 0 \\n\\nOrange 2 3 1 \\n\\nTomato 0 2 11 \\n\\nhttp://en.wikipedia.org/wiki/Fowlkes%E2%80%93Mallows_Index\\nhttp://en.wikipedia.org/wiki/Confusion_matrix\\n\\n\\nExternal indices are based on some pre-specified structure, which is the reflection of prior information \\n\\non the data, and used as a standard to validate the clustering solutions [50]. Internal tests are not \\n\\ndependent on external information (prior knowledge). On the contrary, they examine the clustering \\n\\nstructure directly from the original data. For more on evaluation, refer to [193,194]. \\n\\n6 Applications \\n\\nClustering is useful in several applications. Out of endless useful applications, a few applications \\n\\nare given below in diverse fields. \\n\\n6.1 Image Segmentation \\n\\nImage segmentation is an essential component of image processing. Image segmentation can be \\n\\nachieved using hierarchical clustering [37, 83]. K-means can also be applied for segmentation. \\n\\nMagnetic resonance imaging (MRI) provides a visualization of the internal structures of objects and \\n\\nliving organisms. MRI images have better contrast than computerized tomography; therefore, most \\n\\nmedical image segmentation research uses MRI images. Segmenting an MRI image is a key task in \\n\\nmany medical applications, such as surgical planning and abnormality detection. MRI segmentation \\n\\naims to partition an input image into significant anatomical areas, each of which is uniform according \\n\\nto certain image properties. MRI segmentation can be formulated as a clustering problem in which a \\n\\nset of feature vectors obtained through transformation image measurements and pixel positions is \\n\\ngrouped into a number of structures [28]. \\n\\n6.2 Bioinformatics—Gene Expression Data \\n\\nRecently, advances in genome sequencing projects and DNA microarray technologies have been \\n\\nachieved [50]. The first draft of the human genome sequence project was completed in 2001, several \\n\\nyears earlier than expected [84, 94]. The applications of clustering algorithms in bioinformatics can be \\n\\nseen from two aspects. The first aspect is based on the analysis of gene expression data generated from \\n\\nDNA microarray technologies. The second aspect describes clustering processes that directly work on \\n\\nlinear DNA or protein sequences. The assumption is that functionally similar genes or proteins usually \\n\\nshare similar patterns or primary sequence structures [50]. \\n\\n6.3 Object Recognition  \\n\\nThe use of clustering to group views of 3D objects for the purposes of object recognition in range \\n\\ndata was described in [85]. The system under consideration employed a view point dependent (or view \\n\\ncantered) approach to the object recognition problem; each object to be recognized was represented in \\n\\nterms of a library of range images of that object. \\n\\n6.4 Character Recognition \\n\\nClustering was employed in Jain [86] to identify lexemes in handwritten text for the purposes of \\n\\nwriter independent hand writing recognition. The success of a handwriting recognition system is \\n\\nvitally dependent on its acceptance by potential users. Writer dependent systems can give a higher \\n\\nlevel of recognition accuracy than that given by writer independent systems but the former require a \\n\\nlarge amount of training data. A writer independent system on the other hand must be able to \\n\\nrecognize a wide variety of writing styles in order to satisfy an individual user. \\n\\n6.5 Information Retrieval   \\n\\nInformation retrieval (IR) is concerned with automatic storage and retrieval of documents [87]. \\n\\nMany university libraries use IR systems to provide access to books, journals and other documents. \\n\\nLibraries use the library of congress classification (LCC) scheme for efficient storage and retrieval of \\n\\nbooks. The LCC scheme consists of classes labelled A to Z [88] which are used to characterize books \\n\\nbelonging to different subjects. For example, label Q corresponds to books in the area of science and \\n\\nthe subclass QA is assigned to mathematics. Labels QA76 to QA76.8 are used for classifying books \\n\\nrelated to computers and other areas of computer science. \\n\\n\\n\\n6.6 Data Mining  \\n\\nData mining [21] is the extraction of knowledge from large databases. It can be applied to \\n\\nrelational, transaction and spatial databases as well as large stores of unstructured data such as the \\n\\nWorld Wide Web. There are many data mining systems in use today and applications include the U.S. \\n\\nTreasury detecting money laundering. National basketball association coaches detecting trends and \\n\\npatterns of play for individual players and teams and categorizing patterns of children in the foster care \\n\\nsystem [89]. Several articles have had recent published in special issues on data mining [90].  \\n\\n6.7 Spatial Data Analysis \\n\\nClustering is useful to extract interesting features and identify the patterns, which exist in huge \\n\\namounts of spatial databases [106, 127-129]. It is expensive and very hard for user to deal with large \\n\\nspatial datasets like satellite images, medical equipment, geographical information systems (GIS), \\n\\nimage database exploration etc. Clustering process helps to understand spatial data by analyzing \\n\\nprocess automatically. \\n\\n6.8 Business \\n\\nThe role of clustering is quite interesting in business areas [135-139]. It helps marketer \\n\\nresearchers to do some analysis and prediction about customers in order to provide services based on \\n\\ntheir requirements and it also helps for market segmentation, new product development and product \\n\\npositioning. Clustering may be used to set all available shopping items on web into a group of unique \\n\\nproducts.  \\n\\n6.9 Data Reduction  \\n\\nData reduction or compression is one of the necessary tasks for handling very large data [132-134] \\n\\nand its processing becomes very demanding. Clustering can be applied to help in compressing data \\n\\ninformation by clustering them in different set of interesting clusters. After different set of clusters we \\n\\ncan choose the information or set of data which is useful for us. This process will save data processing \\n\\ntime along with doing data reduction.  \\n\\n6.10 Big Data Mining \\n\\nBig data [161-168] is also an emerging issue. The volume of data which is beyond the capacity of \\n\\nconventional data base management tools is processed under big data mining. Due to use of various \\n\\nsocial sites, travel, e-governance etc practices, mammoth amount of data is being heaped every \\n\\nmoment. Clustering of information (data) can help in aggregating similar information collected in \\n\\nunformatted databases (mainly text). Hadoop is one such big data processing tool [169-171]. It is \\n\\nexpected that big data processing will play an important role in detection of cyber crime, clustering \\n\\ngroups of people with similar behaviour on social network such as face book, WhatsApp etc. or \\n\\npredicting market behaviour based on various polls over these social sites. \\n\\n6.11 Other Applications \\n\\nSequence analysis [140], human genetic clustering [141], social network analysis [142], search \\n\\nresult grouping [143], software evolution [144, 145], recommender systems [146], educational data \\n\\nmining [147-149], Climatology [150], Field Robotics [151] etc. \\n\\n7 Choice of Appropriate Clustering Methods \\n\\nAs depicted in Fig.1, and from the wide amount of literature available with some referred in the \\n\\npaper, it becomes an obvious question: which method is uniformly good? It is to remember that \\n\\naccording to No Free Lunch concept given by Wolpert [197], no algorithm can be uniformly good \\n\\nunder all circumstances. In fact, each algorithm has its merit (strength) under some specific nature of \\n\\ndata but fails on other type of data. The selection of an appropriate clustering method may sometimes \\n\\nalso involve decision on certain parameters. Whether one wants only a proper alignment (or \\n\\nunsupervised grouping) of objects into a number of clusters (say user define k), then only choosing the \\n\\nvalue of k matters. This choice can be made on the ‘how fine tuning among the intra-cluster objects (or \\n\\npatterns) by virtue of distance is expected’. Selecting k can be heuristic or stochastic and evolutionary \\n\\n\\n\\ncomputing like genetic algorithms (GA) can be applied to find k. On the other hand, in case of data \\n\\nmining or data processing applications with dimensionality reduction, mostly it is required to reduce \\n\\nthe number of attributes or features in the existing dataset in order to extract rules with better \\n\\nprediction capability. In many of these occasions, it is expected that while reducing the dimensionality \\n\\nof the dataset, whether the structure or the internal topology of the dataset is not disturbed in the \\n\\nreduced data space. Saxena et. al [23] proposed four unsupervised methods for feature selection using \\n\\ngenetic algorithms. \\n\\nIn [27], Fraley presents a comprehensive discussion on how to decide a clustering method and \\n\\ndescribed a clustering methodology based on multivariate normal mixture models and shown that it \\n\\ncan give much better performance than existing methods. This approach has some limitations, \\n\\nhowever. The first limitation is that computational methods for hierarchical clustering have storage \\n\\nand time requirements that grow at a faster than linear rate relative to the size of the initial partition, so \\n\\nthat they cannot be directly applied to large data sets.  Secondly, although experience to date suggests \\n\\nthat models based on multivariate normal distribution are sufficiently flexible to accommodate many \\n\\npractical situations, the underlying assumption is that groups are concentrated locally about linear \\n\\nsubspaces, so that other models or methods may be more suitable in some instances. Bensmail et al. \\n\\n[198] showed that exact Bayesian inference via Gibbs sampling, with calculations of Bayes factors \\n\\nusing the Laplace–Metropolis estimator, works well in several real and simulated examples [27].   \\n\\nFurther, for large data sets, CURE method is advisable whereas BIRCH being also good but with \\n\\nless time complexity although quality of clustering is inferior to that obtained by CURE, refer to Table \\n\\n1. Under partitioned clustering method, k-means clustering dominates and is still the most popular \\n\\nclustering method, refer to Table 2. How many clusters i.e. k depends on how close or fine tuning we \\n\\nwant among clusters. We should also keep in mind, for what purpose we are applying k-means. In \\n\\nvarious clustering methods presented in the paper already, the strengths and weaknesses of each are \\n\\nmostly given therein. Apart from the discussion above on selection of appropriate method for \\n\\nclustering, it is worth noting looking to a huge amount of literature available with wide variety of \\n\\napplication of clustering; it is not possible to settle to an agreeable recommendation. Specific task \\n\\n(objectives) calls for specific strategy and should be tested experimentally. Finally, a part of \\n\\ncomprehensive and comparative table for various clustering algorithms presented before is given in \\n\\nTable 4, for details and meaning of symbols refer to [199]. \\n\\nTable 4. Comparative study of some clustering algorithms [199] \\n\\nCategory of \\n\\nClustering \\n\\nAlgorithm \\n\\nName \\n\\nTime complexity Scalability Suitable for \\n\\nlarge scale data \\n\\nSuitable for \\n\\nhigh \\n\\ndimensional \\n\\ndata \\n\\nSensitive \\n\\nof noise/ \\n\\noutlier \\n\\nPartition k-means Low O(knt) Middle Yes No High \\n\\nPAM High O(k(n-k)ˆ2)) Low No No little \\n\\nCLARA Middle O(ksˆ 2+k(n-k)) High Yes No Little \\n\\nCLARANS High O(nˆ2) Middle Yes No Little \\n\\nHierarchy BIRCH Low O(n) High Yes No Little \\n\\nCURE Low O(s ˆ2*logs) High Yes Yes Little \\n\\nROCK High O(nˆ2*logn) Middle No Yes Little \\n\\nChameleon High O(nˆ2) High No No Little \\n\\nFuzzy based FCM Low O(n) Middle No No High \\n\\nDensity based DBSCAN Middle O(n*logn) Middle Yes No Little \\n\\nGraph theory CLICK Low O(k*f(v,e)) High Yes No High \\n\\nGrid based CLIQUE Low O(n+kˆ2) High No Yes Moderate \\n\\n \\n\\n\\n\\n8 Conclusions \\n\\nThe classification of objects finds prime importance in several data processing applications \\n\\nincluding data mining, medical diagnostics, pattern recognition and social paradigms. The objects \\n\\nalready labeled are placed in supervised classified groups while those not labeled are grouped in \\n\\nunsupervised classified groups. This paper presented various methods used for clusters with their \\n\\nstates of arts and limitations. In the hierarchical type of clustering methods, clusters are formed by \\n\\niteratively dividing the patterns (instances) into top-down or bottom up manner accordingly \\n\\nagglomerative and divisive or splitting hierarchical clustering methods are discussed. As opposed to \\n\\nhierarchical clustering, partitional clustering assigns data into K clusters without any hierarchical \\n\\nstructure by optimizing some criterion function. The most common criterion is finding Euclidean \\n\\ndistance between the points with each of the available clusters and assigning the point to the cluster \\n\\nwith minimum distance. The benchmark k-means clustering methods with its variations like Fuzzy K-\\n\\nmeans are discussed. The graph theoretic methods produce clusters via graphs. In the mixture density \\n\\nbased methods, data objects are assumed to be generated according to several probability distributions \\n\\nand can be derived from different types of density functions (e.g., multivariate Gaussian or t-\\n\\ndistribution), or from the same families but with different parameters. The grid based clustering \\n\\ntechniques include: STING (statistical information grid approach) a highly scalable algorithm and has \\n\\nthe ability to decompose the data set into various levels of details. The evolutionary approaches for \\n\\nclustering start with a random population of candidate solutions with some fitness function, which \\n\\nwould be optimized. Clustering based on simulated annealing, collaborative clustering, multi objective \\n\\nclustering with their states of art are also included. Various types of the similarity criteria for \\n\\nclustering have been given in the paper. After the clusters have been formed, the evaluation criteria are \\n\\nalso summarised to see the performance and accuracy of clusters. The applications of clustering in \\n\\nimage segmentation, object and character recognition, information retrieval and data mining are \\n\\nhighlighted in the paper. Of course there is an abundant amount of literature available in clustering and \\n\\nits applications; it is not possible to cover that entirely, only basic and few important methods are \\n\\nincluded in this paper with their merits and demerits. \\n\\nAcknowledgement \\n\\nThe authors would like to thank the anonymous reviewers for their valuable suggestions and \\n\\ncomments to improve the quality of the paper. This work is partially supported by the Australian \\n\\nResearch Council (ARC) under discovery grant DP150101645. \\n\\nReferences \\n\\n1. R. O. Duda, P. E. Hart, and D. G. Stork, “Pattern Classification,” Wiley Publications, 2001. \\n\\n2. Y. Zhang, Y. Yin, D. Guo, X. Yu, and L. Xiao, “Cross-validation based weights and structure determination of \\n\\nChebyshev-polynomial neural networks for pattern classification,” Pattern Recognition, vol. 47, no. 10, pp. 3414-\\n\\n3428, 2014. \\n\\n3. H. Nakayama, N. Kagaku, “Pattern classification by linear goal programming and its extensions,” Journal of \\n\\nGlobal Optimization, vol. 12, no. 2, pp. 111–126, 1998. \\n\\n4. C. M. Bishop, “Pattern recognition and machine learning,” Berlin: Springer, ISBN 978-0-387-31073-2. \\n\\n5. G.P. Zhang, “Neural networks for classification: a survey,” IEEE Transaction on Systems, Man, and Cybernetics, \\n\\nPart C: Applications and Reviews, vol. 30, no. 4, pp. 451–462, 2002. \\n\\n6. H. Zhang, J. Liu, D. Ma, and Z. Wang, “Data-core-based fuzzy min–max neural network for pattern classification,” \\n\\nIEEE Transaction on Neural Networks, vol. 22, no. 12, pp. 2339–2352, 2011. \\n\\n7. X. Jiang and A. H. K. S. Wah, “Constructing and training feed-forward neural net- works for pattern \\n\\nclassification,” Pattern Recognition, vol. 36, no. 4, pp. 853–867, 2003. \\n\\n8. G. Ou and Y. L. Murphey, “Multi-class pattern classification using neural networks,” Pattern Recognition, vol. 40, \\n\\nno. 1, pp. 4–18. 2007. \\n\\n9. J. D. Paola and R. A. Schowengerdt, “A detailed comparison of back propagation neural network and maximum-\\n\\nlikelihood classifiers for urban land use classification,” IEEE Transaction on Geoscience and Remote Sensing, vol. \\n\\n33, no. 4, pp. 981–996, 1995. \\n\\n10. D. E. Rumelhart and J. L. McClelland, “Parallel Distributed Processing,” MIT Press, Cambridge, 1986. \\n\\n11. W. Zhou, “Verification of the nonparametric characteristics of back-propagation neural networks for image \\n\\nclassification,” IEEE Transaction on Geoscience and Remote Sensing, vol. 37, no. 2, pp. 771–779, 1999. \\n\\n12. G. Jaeger, U. C. Benz, “Supervised fuzzy classification of SAR data using multiple sources,” IEEE International \\n\\nGeoscience and Remote Sensing Symposium, 1999. \\n\\nhttp://en.wikipedia.org/wiki/International_Standard_Book_Number\\nhttp://en.wikipedia.org/wiki/Special:BookSources/0-387-31073-8\\nhttp://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=772033&queryText%3Dsupervised+fuzzy+classification\\nhttp://ieeexplore.ieee.org/xpl/mostRecentIssue.jsp?punumber=6246\\n\\n\\n13. F. S. Marzano, D. Scaranari, and G. Vulpiani, “Supervised Fuzzy-Logic Classification of Hydrometeors Using C-\\n\\nBand Weather Radars,” IEEE Transaction on Geoscience and Remote Sensing, vol. 45 , no. 11, pp. 3784-3799, \\n\\n2007. \\n14. B. Xue, M. Zhang, and W. N. Browne, “Particle Swarm Optimization for Feature Selection in Classification: A \\n\\nMulti-Objective Approach,” IEEE Transaction on Cybernetics, vol. 43, no. 6, pp. 1656-1671, 2013. \\n\\n15. A. Saxena and M. Vora, “Novel Approach for the use of Small World Theory in Particle Swarm Optimization,” \\n\\n16th International Conference on Advanced Computing and Communications, 2008. \\n\\n16. Z. Pawlak, “Rough sets”, International Journal of Computer and Information Science, vol. 11, no. 5, pp. 341-356. \\n\\n1982. \\n\\n17. Z. Pawlak, “Rough sets In Theoretical Aspects of Reasoning about Data,” Kluwer, Netherlands, 1991. \\n\\n18. S. Dalai, B. Chatterjee, D. Dey, S. Chakravorti, and K. Bhattacharya, “Rough-Set-Based Feature Selection and \\n\\nClassification for Power Quality Sensing Device Employing Correlation Techniques,” IEEE Sensors Journal, vol. \\n\\n13, no. 2, pp. 563–573, 2013 \\n\\n19. J. R. Quinlan, “Induction of decision trees,” Machine Learning, vol. 1, no. 1, pp. 81-106, 1986. \\n\\n20. D. M. Farida, L Zhang, C. M. Rahman, M. A. Hossain, and R. Strachan, “Hybrid decision tree and naïve Bayes \\n\\nclassifiers for multi-class classification tasks,” Expert Systems with Applications, vol. 41, no. 2, pp. 1937–1946, \\n\\n2014. \\n\\n21. J. Han, M. Kamber, and J. Pei, “Data Mining: Concepts and Techniques,” Morgan Kaufmann Publishers, 2011. \\n\\n22. L. Rokach, “Clustering Methods,” Data Mining and Knowledge Discovery Handbook, pp 331-352, Springer 2005. \\n\\n23. A. Saxena, N. R. Pal, and M. Vora, “Evolutionary methods for unsupervised feature selection using Sammon’s \\n\\nstress function, Fuzzy Information and Engineering,” vol. 2, no. 3, pp. 229-247, 2010. \\n\\n24. A. K. Jain, “Data Clustering: 50 years beyond k-means,” Pattern Recognition Letters, vol. 31, no. 8, pp. 651–666, \\n\\n2010. \\n\\n25. Merriam-Webster Online Dictionary, 2008 \\n\\n26. V. E. Castro and J. Yang, “A Fast and robust general purpose clustering algorithm,” International Conference on \\n\\nArtificial Intelligence, 2000. \\n\\n27. C. Fraley and A. E. Raftery, “How Many Clusters? Which Clustering Method? Answers Via Model-Based Cluster \\n\\nAnalysis”, Technical Report No. 329, Department of Statistics University of Washington, 1998. \\n\\n28. A. K. Jain, M. N. Murty, and P. J. Flynn, “Data Clustering: A review. ACM Computing Surveys, vol. 31, no. 3, pp. \\n\\n264-323, 1999. \\n\\n29. P. Sneath and R. Sokal, “Numerical Taxonomy,” W.H. Freeman Co, San Francisco, CA, 1973. \\n\\n30. B. King, “Step-wise Clustering Procedures,” Journal of American Statistical Association , vol. 69, no. 317, pp. 86-\\n\\n101, 1967. \\n\\n31. J. H. Ward, “Hierarchical grouping to optimize an objective function,” Journal of the American Statistical \\n\\nAssociation, vol. 58, no. 301, pp. 236-244, 1963. \\n\\n32. F. Murtagh, “A survey of recent advances in hierarchical clustering algorithms which use cluster centers,” \\n\\nComputer Journal, vol. 26, no. 4, pp. 354-359, 1984. \\n\\n33. A. Nagpal, A. Jatain, and D. Gaur, “Review based on Data Clustering Algorithms,” IEEE Conference on \\n\\nInformation and Communication Technologies, 2013. \\n\\n34. A. Periklis, “Data Clustering Techniques,” University of Toronto, 2002. \\n\\n35. S. Guha, R. Rastogi, and S. Kyuseok, “CURE: An efficient clustering algorithm for large databases,” ACM, 1998. \\n\\n36. K. George, E. H. Han, and V. Kumar, “CHAMELEON: A Hierarchical Clustering Algorithm Using Dynamic \\n\\nModeling,” IEEE Computer, vol. 32, no. 8, pp. 68-75, 1999. \\n\\n37. D. Lam and D. C. Wunsch, “Clustering,” Academic Press Library in Signal Processing,” Signal Processing Theory \\n\\nand Machine Learning, vol. 1, 2014 \\n\\n38. J. B. MacQueen, “Some Methods for classification and Analysis of Multivariate Observations,” 5th Symposium on \\n\\nMathematical Statistics and Probability, Berkeley, University of California Press, vol. 1, pp. 281-297, 1967. \\n\\n39. A. Gersho and R. Gray, “Vector Quantization and Signal Compression,” Kluwer Academic Publishers, 1992. \\n\\n40. J. C. Dunn, “A Fuzzy Relative of the ISODATA Process and Its Use in Detecting Compact Well-Separated \\n\\nClusters,” Journal of Cybernetics, vol. 3, no. 3, pp. 32-57, 1973. \\n\\n41. J. C. Bezdek, “Pattern Recognition with Fuzzy Objective Function Algorithms,” Plenum Press, New York, 1981. \\n\\n42. R. Yager and D. Filev, “Approximate clustering via the mountain method,” IEEE Transaction on Systems, Man \\n\\nand Cybernetics, Part B: Cybernetics, vol. 24, no. 8, pp. 1279–1284. 1994 \\n\\n43. I. Gath and A. Geva, “Unsupervised optimal fuzzy clustering,” IEEE Transaction on Pattern Analysis  and \\n\\nMachine Intelligence, vol. 11, no. 7, pp. 773–781. 1989. \\n\\n44.  R. Hathaway, J. Bezdek, and Y. Hu, “Generalized fuzzy c-Means clustering strategies using Lp norm distances,” \\n\\nIEEE Transaction on Fuzzy Systems, vol. 8, no. 5, pp. 576–582. 2000. \\n\\n45.  R. Krishnapuram and J. Keller, “A possibilistic approach to clustering,” IEEE Transaction on Fuzzy Systems, vol. \\n\\n1, no. 2, pp. 98–110, 1993. \\n\\n46. C. T. Zahn, “Graph-theoretical methods for detecting and describing gestalt clusters,” IEEE Transaction on \\n\\nComputer, vol. C-20, no. 1, pp. 68-86, 1971. \\n\\n47. R. Urquhart, “Graph-theoretical clustering based on limited neighborhood sets,” Pattern Recognition, vol. 15, no. \\n\\n3, pp. 173-187, 1982. \\n\\nhttp://ieeexplore.ieee.org/search/searchresult.jsp?searchWithin=p_Authors:.QT.Marzano,%20F.S..QT.&searchWithin=p_Author_Ids:37269625100&newsearch=true\\nhttp://ieeexplore.ieee.org/search/searchresult.jsp?searchWithin=p_Authors:.QT.Scaranari,%20D..QT.&searchWithin=p_Author_Ids:37660501800&newsearch=true\\nhttp://ieeexplore.ieee.org/search/searchresult.jsp?searchWithin=p_Authors:.QT.Vulpiani,%20G..QT.&searchWithin=p_Author_Ids:37550431200&newsearch=true\\nhttp://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4373376&queryText%3Dsupervised+fuzzy+classification\\nhttp://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4373376&queryText%3Dsupervised+fuzzy+classification\\nhttp://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=36\\nhttp://ieeexplore.ieee.org/search/searchresult.jsp?searchWithin=p_Authors:.QT.Bing%20Xue.QT.&newsearch=true\\nhttp://ieeexplore.ieee.org/search/searchresult.jsp?searchWithin=p_Authors:.QT.Mengjie%20Zhang.QT.&newsearch=true\\nhttp://ieeexplore.ieee.org/search/searchresult.jsp?searchWithin=p_Authors:.QT.Browne,%20W.N..QT.&newsearch=true\\nhttp://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6381531&searchWithin%3Dpso+based+classification%26searchWithin%3Dpso+based+classification%26refinements%3D4291944246%26queryText%3Dpso+based+classifications\\nhttp://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6381531&searchWithin%3Dpso+based+classification%26searchWithin%3Dpso+based+classification%26refinements%3D4291944246%26queryText%3Dpso+based+classifications\\nhttp://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6221036\\nhttp://ieeexplore.ieee.org/search/searchresult.jsp?searchWithin=p_Authors:.QT.Dalai,%20S..QT.&newsearch=true\\nhttp://ieeexplore.ieee.org/search/searchresult.jsp?searchWithin=p_Authors:.QT.Chatterjee,%20B..QT.&newsearch=true\\nhttp://ieeexplore.ieee.org/search/searchresult.jsp?searchWithin=p_Authors:.QT.Dey,%20D..QT.&newsearch=true\\nhttp://ieeexplore.ieee.org/search/searchresult.jsp?searchWithin=p_Authors:.QT.Chakravorti,%20S..QT.&newsearch=true\\nhttp://ieeexplore.ieee.org/search/searchresult.jsp?searchWithin=p_Authors:.QT.Bhattacharya,%20K..QT.&newsearch=true\\nhttp://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6304906&refinements%3D4291944246%26queryText%3Drough+set+based+classifications\\nhttp://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6304906&refinements%3D4291944246%26queryText%3Drough+set+based+classifications\\nhttp://link.springer.com/journal/12543\\n\\n\\n48. D. H. Fisher, “Knowledge acquisition via incremental conceptual clustering,” Machine Learning 2, pp. 139-172. \\n\\n1987. \\n\\n49. S. Haykin, “Neural Networks: A Comprehensive Foundation,” 2nd Edition, Prentice Hall, 1999. \\n\\n50. R. Xu and D. Wunsch, “Survey of clustering algorithms,” IEEE Transaction on Neural Networks, vol. 16, no. 3, \\n\\n645–678, 2005. \\n\\n51. R. Xu. D.C. Wunsch, “Clustering algorithms in biomedical research: a review,” IEEE Reviews in Biomedical \\n\\nEngineering, vol. 3, pp. 120–154. 2010. \\n\\n52. G. McLachlan, T. Krishnan, “The EM Algorithm and Extensions,” Wiley, New York, 1997. \\n\\n53. J. D. Banfield and A. E. Raftery, “Model-based Gaussian and non-Gaussian clustering Biometrics,” vol. 49, no. 3, \\n\\npp. 803-821, 1993. \\n\\n54. M. Ester, H. P. Kriegel, S. Sander S, and X. Xu, “A density-based algorithm for discovering clusters in large \\n\\nspatial databases with noise,” 2nd International Conference on Knowledge Discovery and Data Mining, 1996. \\n\\n55. P. Cheeseman, J. Stutz, “Bayesian Classification (AutoClass): Theory and Results,” Advances in Knowledge \\n\\nDiscovery and Data Mining, pp. 153-180, 1996. \\n\\n56. C. S. Wallace and D. L. Dowe, “Intrinsic classification by mml–the snob program,” 7th Australian Joint Conference \\n\\non Artificial Intelligence, pp. 37-44, 1994. \\n\\n57. W. Wang, J. Yang, and R. R. Muntz, “STING: A Statistical Information Grid Approach to Spatial Data Mining,” \\n\\n23rd VLDB Conference, pp. 86-195, 1997. \\n\\n58. G. Sheikholeslami, S. Chatterjee and A. Zhang, “WaveCluster: a wavelet-based clustering approach for spatial data \\n\\nin very large databases,” The International Journal on Very Large Data Bases, vol. 8, no. 3-4, pp. 289-304, 2000. \\n\\n59. R. Agrawal, G. Johannes, G. Dimitrios, and P. Raghavan, “Automatic Subspace Clustering of High Dimensional \\n\\nData for Data Mining Applications,” SIGMOD Conference, pp. 94-105, 1998. \\n\\n60. A. K. Jain and M. Flynn, “Data clustering: a review,” ACM Computing Surveys (CSUR), vol. 31, no. 3, pp. 264-\\n\\n323, 1999. \\n\\n61. H. P. Schwefel, “Numerical Optimization of Computer Models,” John Wiley, New York, 1981. \\n\\n62. L. J. Fogel, A. J. Owens, and M J Walsh, “Artificial Intelligence Through Simulated Evolution,” John Wiley , New \\n\\nYork, 1965. \\n\\n63. J. H. Holland, “Adaption in Natural and Artificial Systems,” University of Michigan Press, 1975. \\n\\n64. D. Goldberg, “Genetic Algorithms in Search Optimization and Machine Learning,” Addison Wesley Reading, \\n\\n1989. \\n\\n65. J. Kennedy and R. C. Eberhart, “Swarm Intelligence,” Morgan Kaufmann, 2001. \\n\\n66. J. Kennedy and R. Eberhart, “Particle Swarm Optimization,” 4th IEEE International Conference on Neural \\n\\nNetworks. pp. 1942–1948, 1995. \\n\\n67. M. Dorigoand T. Stützle, “Ant Colony Optimization,” MIT Press, 2004. \\n\\n68. F. Glover, “Future Paths for Integer Programming and Links to Artificial Intelligence,” Computers and Operations \\n\\nResearch, vol. 5, no. 5, pp. 533-549, 1986. \\n\\n69. K. S. Al. Sultan, “A Tabu Search Approach to Clustering Problem,” Pattern Recognition, vol. 28, no. 9, pp. 1443-\\n\\n1451, 1995. \\n\\n70. W. Pedrycz, “Collaborative fuzzy clustering,” Pattern Recognition Letters, vol. 23, no. 14, pp. 1675–1686, 2002. \\n\\n71. L. F. S. Coletta, L. Vendramin, E. R. Hruschka, R. J. G. B. Campello, and W. Pedrycz, “Collaborative Fuzzy \\n\\nClustering Algorithms: Some Refinements and Design Guidelines,” IEEE Transactions on Fuzzy Systems, vol. 20, \\n\\nno. 3, pp. 444-462, 2012. \\n\\n72. W. Pedrycz and P. Rai, “Collaborative clustering with the use of Fuzzy C-Means and its quantification,” Fuzzy \\n\\nSets and Systems, vol. 159, no. 18, pp. 2399–2427, 2008. \\n\\n73. W. Pedrycz, “Knowledge Based Clustering: From data to information granules,” Wiley Publications, 2005. \\n\\n74. M. Prasad, C. T. Lin, C. T. Yang, and A. Saxena, “Vertical Collaborative Fuzzy C-Means for Multiple EEG Data \\n\\nSets,” Springer Intelligent Robotics and Applications Lecture Notes in Computer Science, vol. 8102, pp 246-257, \\n\\n2013. \\n\\n75. C. Pizzuti, “Overlapping Community Detection in Complex Networks,” GECCO, pp. 859–866, 2009. \\n\\n76. S. Gregory, “A Fast Algorithm to Find Overlapping Communities in Networks,” PKDD, pp. 408–423, 2008. \\n\\n77. Y. Y. Ahn, J. P. Bagrow, and S. Lehmann, “Link communities reveal multi-scale complexity in networks,” Nature, \\n\\nvol. 466, pp. 761–764, 2010. \\n\\n78. G Forestier, P Gancarski, and C Wemmert, “Collaborative Clustering with back ground knowledge,” Data and \\n\\nKnowledge Engineering, vol. 69, no. 2, pp. 211–228, 2010. \\n\\n79. J. Handl and J. Knowles, “An evolutionary approach to Multiobjective clustering,” IEEE Transaction on \\n\\nEvolutionary Computation, vol.11, no. 1, pp. 56-76, 2007. \\n\\n80. A. Konak, D. Coit, and A. Smith, “Multiobjective optimization using genetic algorithms: A tutorial,” Reliability \\n\\nEngineering and System Safety, vol. 91, no. 9, pp. 992-1007, 2006. \\n\\n81. K. Faceili, A. D. Carvalho, and D. Souto, “Multiobjective Clustering ensemble,” International Conference, on \\n\\nHybrid Intelligent Systems, 2006. \\n\\n82. M. K. Law, A. Topchy, and A. K. Jain, “Multiobjective Data Clustering,” IEEE Conference on Computer Vision \\n\\nand Pattern Recognition, vol. 2, pp. 424-430, 2004. \\n\\n83. D. Forsyth and J. Ponce, “Computer vision: a modern approach,” Prentice Hall, 2002. \\n\\nhttp://www.informatik.uni-trier.de/~ley/pers/hd/g/Gehrke:Johannes.html\\nhttp://www.informatik.uni-trier.de/~ley/pers/hd/g/Gunopulos:Dimitrios.html\\nhttp://www.informatik.uni-trier.de/~ley/pers/hd/r/Raghavan:Prabhakar.html\\nhttp://www.informatik.uni-trier.de/~ley/db/conf/sigmod/sigmod98.html#AgrawalGGR98\\nhttp://www.google.co.in/search?tbo=p&tbm=bks&q=inauthor:%22Marco+Dorigo%22\\nhttp://www.google.co.in/search?tbo=p&tbm=bks&q=inauthor:%22Thomas+St%C3%BCtzle%22\\nhttp://link.springer.com/search?facet-author=%22Mukesh+Prasad%22\\nhttp://link.springer.com/search?facet-author=%22Chin-Teng+Lin%22\\nhttp://link.springer.com/search?facet-author=%22Chien-Ting+Yang%22\\nhttp://link.springer.com/search?facet-author=%22Amit+Saxena%22\\nhttp://link.springer.com/book/10.1007/978-3-642-40852-6\\nhttp://link.springer.com/bookseries/558\\n\\n\\n84. I. H. G. S. Consortium, “Initial sequencing and analysis of the human genome,” Nature, vol. 409, pp. 860–921, \\n\\n2001.  \\n\\n85. C. Dorai and A. K. Jain, “Shape Spectra Based View Grouping for Free Form Object,” International Conference on \\n\\nImage Processing, vol. 3, pp. 240-243, 1995. \\n\\n86. S. Connell and A. K. Jain, “Learning Prototypes for On-Line Handwritten Digits,” 14th International Conference on \\n\\nPattern Recognition, vol. 1, pp. 182-184, 1998. \\n\\n87. E. Rasmussen, “Clustering Algorithms,” Information Retrieval: Data Structures and Algorithms, Prentice Hall \\n\\nEnglewood Cliffs, pp 419-442, 1992. \\n\\n88. G. McKiernan, “LC Classification Outline,” Library of Congress Washington, D. C, 1990. \\n\\n89. S. R. Hedberg, “Searching for the mother lode: Tales of the first data miners,” IEEE Expert: Intelligent Systems an \\n\\nTheir Applications, vol. 11, no. 5, pp. 4-7, 1996. \\n\\n90. J. Cohen, “Communications of the ACM: Data Mining Association for Computing Machinery,” Nov. 1996.  \\n\\n91. A. Saxena, J. Wang, “Dimensionality Reduction with Unsupervised Feature Selection and Applying Non-\\n\\nEuclidean Norms for Classification Accuracy,” International Journal of Data Warehousing and Mining, vol. 6, no. \\n\\n2, pp 22-40, 2010. \\n\\n92. K. S. Al. Sultan and M. M. Khan, “Computational experience on four algorithms for the hard clustering problem,” \\n\\nPattern Recognition Letters, vol. 17, no. 3, pp. 295-308, 1996. \\n\\n93. R. Michalski, R. E. Stepp, and E. Diday, “Automated construction of classifications: conceptual clustering versus \\n\\nnumerical taxonomy,” IEEE Transaction on Pattern Analysis and Machine Intelligence, vol. 5, no. 4, pp. 396–409, \\n\\n1983. \\n\\n94. J. C. Venter et. al.,“The sequence of the human genome,”Science,vol. 291, pp. 1304–1351, 2001. \\n\\n95. J. L. Kolodner, “Reconstructive memory: A computer model,” Cognitive Science, vol. 7, no. 4, pp. 281-328, 1983. \\n\\n96. C. Carpineto and G. Romano, “An order-theoretic approach to conceptual clustering,” 10th International \\n\\nConference on Machine Learning, pp. 33–40, 1993. \\n\\n97. L. Talavera and J. Bejar. “Generality-Based Conceptual Clustering with Probabilistic Concepts,” IEEE \\n\\nTransactions on Pattern Analysis and Machine Intelligence, vol. 23, no. 2, pp. 196-206, 2001. \\n\\n98. M. Hadzikadic and D. Yun, “Concept formation by incremental conceptual clustering,” 11th International Joint \\n\\nConference Artificial Intelligence, pp. 831-836, 1989. \\n\\n99. G. Biswas, J. B. Weinberg, and D. H. Fisher, “Iterate: A conceptual clustering algorithm for data mining,” IEEE \\n\\nTransactions on Systems, Man, and Cybernetics, Part C, vol. 28, no. 2, pp. 219–230, 1998. \\n\\n100. K. Thompson and P. Langley, “Concept formation in structured domains,” Concept Formation: Knowledge and   \\n\\nExperience in Unsupervised Learning, Morgan Kaufmann, 1991. \\n\\n101. I. Jonyer, D. Cook, and L. Holder, “Graph-based hierarchical conceptual clustering,” Journal of Machine Learning \\n\\nResearch, vol. 2, pp. 19-43, 2001. \\n\\n102. M. Lebowitz, “Experiments with Incremental Concept Formation: UNIMEM,” Machine Learning, vol. 2, no. 2, \\n\\npp. 103-138, 1987. \\n\\n103. S. Hanson and M. Bauer, “Conceptual clustering, categorization and polymorphy,” Machine Learning Journal, \\n\\nvol. 3, no. 4, pp. 343-372, 1989. \\n\\n104. T. Kohonen, “The self-organizing map,” Neurocomputing, vol. 21, no. 1–3, Pages 1–6, 1998. \\n\\n105. J. Vesanto and E. Alhoniemi, “Clustering of the Self-Organizing Map,” IEEE Transactions on Neural Networks, \\n\\nvol. 11, no. 3, 2000. \\n\\n106. J. G. Upton and B. Fingelton, “Spatial Data Analysis by Example,” Point Pattern and Quantitative Data, John \\n\\nWiley & Sons, New York, vol. 1, 1985. \\n\\n107. A. Strehl, J. Ghosh, and R. Mooney, “Impact of similarity measures on web-page clustering,” Workshop on \\n\\nArtificial Intelligence for Web Search, pp 58–64, 2000. \\n\\n108. J. J. Fortier, and H. Solomon, “Clustering procedures,” The Multivariate Analysis, pp. 493-506, 1996. \\n\\n109. M. A. Gluck and  J. E. Corter,(1985), “Information, uncertainty, and the utility of categories,” Program of the 7th \\n\\nAnnual Conference of the Cognitive Science Society, pp. 283–287, 1985. \\n\\n110.  M. J. A. N. Condorcet, “Essai sur l’Application de l’Analyse `a la Probabilite´ des decisions rendues a la \\n\\nPluralite´ des Voix,” paris: L’Imprimerie Royale, 1785. \\n\\n111. J. F. Marcotorchino and P. Michaud, “Optimisation en Analyse Ordinale des Donnees Masson, Paris, 1979. \\n\\n112. J. E. Corter and M. A. Gluck, “Explaining basic categories: Feature predictability and information,” Psychological \\n\\nBulletin, vol. 111, no. 2, pp. 291–303, 1992. \\n\\n113. A. Strehl and J. Ghosh, “Clustering Guidance and Quality Evaluation Using Relationship-based Visualization,” \\n\\nIntelligent Engineering Systems through Artificial Neural Networks, St. Louis, Missouri, USA, pp 483-488, 2000. \\n\\n114. S. V. Stehman, “Selecting and interpreting measures of thematic classification accuracy” Remote Sensing of \\n\\nEnvironment, vol. 62, no. 1, pp. 77–89, 1997. \\n\\n115. W. M. Rand, “Objective criteria for the evaluation of clustering methods,” Journal of the American Statistical \\n\\nAssociation, vol. 66, no. 336, pp. 846– 850, 1971. \\n\\n116. V. Rijsbergen, “Information retrieval,” Butterworths, London, 1979. \\n\\n117. J. F. Brendan and D. Dueck, “Clustering by passing messages between data points,”.Science, vol. 315, pp. 972–\\n\\n976, 2007. \\n\\n118. E. B. Fowlkes and C. L. Mallows (1983), “A Method for Comparing Two Hierarchical Clusterings,” Journal of \\n\\nthe American Statistical Association, vol. 78, no. 383, pp. 553–569, 2010. \\n\\nhttp://ieeexplore.ieee.org/search/searchresult.jsp?searchWithin=p_Authors:.QT.Kohonen,%20T..QT.&searchWithin=p_Author_Ids:37318173400&newsearch=true\\nhttp://www.sciencedirect.com/science/journal/09252312\\nhttp://www.sciencedirect.com/science/journal/09252312/21/1\\nhttp://en.wikipedia.org/wiki/Journal_of_the_American_Statistical_Association\\nhttp://en.wikipedia.org/wiki/Journal_of_the_American_Statistical_Association\\nhttp://www.sciencedirect.com/science/article/pii/S0306457301000486#BIB33\\nhttp://en.wikipedia.org/wiki/Science_(journal)\\n\\n\\n119. D. L. Olson and D. Delen, “Advanced Data Mining Techniques,” Springer, 1st edition, 2008. \\n\\n120. D. M. W. Powers, “Evaluation: From Precision, Recall and F-Factor to ROC, Informedness, Markedness & \\n\\nCorrelation,” Journal of Machine Learning Technologies, vol. 2, no. 1, pp. 37–63, 2007. \\n\\n121. P. Jaccard, “Distribution de la flore alpine dans le bassin des Dranses et dans quelques régions voisines,” Bulletin \\n\\nde la Société Vaudoise des Sciences Naturelles, vol. 37, pp. 241-272, 1901. \\n\\n122. J. Han, M. Kamber, and J. Pei, “Data mining: Concepts and techniques,” Morgan Kaufman, San Francisco, USA, \\n\\n2011. \\n\\n123. J. J. Grefenstette, “Optimization of Control Parameters for Genetic Algorithms,” IEEE Transaction on Systems, \\n\\nMan and Cybernetics, vol. 16, no. 1, pp. 122–128, 1986. \\n\\n124. C. T. Lin, M. Prasad, and J. Y. Chang, “Designing mamdani type fuzzy rule using a collaborative FCM scheme,” \\n\\nInternational Conference on Fuzzy Theory and Its Applications, 2013. \\n\\n125. L. Eugene, “Chapter 4.5. Combinatorial Implications of Max-Flow Min-Cut Theorem, Chapter 4.6. Linear \\n\\nProgramming Interpretation of Max-Flow Min-Cut Theorem,” Combinatorial Optimization: Networks and \\n\\nMatroids, Dover. pp. 117–120, 2001. \\n\\n126. C. H. Papadimitriou and K. Steiglitz, “Chapter 6.1 The Max-Flow, Min-Cut Theorem,” Combinatorial \\n\\nOptimization: Algorithms and Complexity. Dover. pp. 120– 128, 1998. \\n\\n127. A. S. Fotheringham, M. E. Charlton, and C. Brunsdon, “Geographically weighted regression: a natural evolution \\n\\nof the expansion method for spatial data analysis,” Environment and Planning, vol. 30, no. 11, pp. 1905-1927, \\n\\n1998. \\n\\n128. M. Honarkhah, and J. Caers, “Stochastic Simulation of Patterns Using Distance-Based Pattern Modeling,” \\n\\nMathematical Geosciences, vol. 42, no. 5, pp. 487–517, 2010. \\n\\n129. P. Tahmasebi, A. Hezarkhani, and M Sahimi, “Multiple-point geostatistical modeling based on the cross-\\n\\ncorrelation functions,” Computational Geosciences, vol.16, no. 3, pp. 779-797, 2012. \\n\\n130. S. Guha, R. Rastogi, and K. Shim, “ROCK: A Robust Clustering Algorithm for Categorical Attributes,” IEEE \\n\\nConference on Data Engineering, 1999. \\n\\n131. T. Zhang, R. Ramakrishnan, and M. Linvy, “BIRCH: An Efficient Method for Very Large Databases,” ACM \\n\\nSIGMOD, 1996. \\n\\n132.  D. Jiang, G. Chen, B. C. Ooi, K. L. Tan, and S. W, “epiC: an Extensible and Scalable System for Processing Big \\n\\nData,” 40th VLDB Conference, pp. 541 - 552, 2014. \\n\\n133. Z. Huang, “A Fast Clustering Algorithm to Cluster very Large Categorical Data Sets in Data Mining,” DMKD, \\n\\n1997. \\n\\n134. A. Hinneburg and D. Keim, “An Efficient Approach to Clustering in Large Multimedia Databases with Noise,” \\n\\nKDD Conference, 1998. \\n\\n135. M. J. A. Berry and G. Linoff, “Data Mining Techniques For Marketing, Sales and Customer Support,” John Wiley \\n\\n& Sons, Inc., USA, 1996. \\n\\n136. G. Fennell, G. M. Allenby, S. Yang and Y. Edwards, “The Effectiveness of Demographics and Phychographic \\n\\nVariables for Explaining Brand and Product Category Use,” Quantitative Marketing and Economics, vol. 1, no. 2, \\n\\npp. 223-224, 2003. \\n\\n137. M. Y. Kiang, D. M. Fisher, M. Y. Hu, “The effect of sample size on the extended self-organizing map network- A \\n\\nmarket segmentation application,” Computational Statistics and Data Analysis, vol. 51, no. 12, pp. 5940-5948, \\n\\n2007. \\n\\n138. S. Dolnicar, “Using Cluster Analysis for Market Segmentation–Typical Misconceptions, Established \\n\\nMethodological Weaknesses and Some Recommendations for Improvement,” Journal of Marketing Research, vol. \\n\\n11, no. 2, pp. 5-12, 2003. \\n\\n139. R. Wagner, S. W. Scholz, and R. Decker, “The number of clusters in market segmentation,” Data Analysis and \\n\\nDecision Support, Heidelberg: Springer, pp. 157-176, 2005. \\n\\n140. R. M. Durbin, S. R. Eddy, A. Krogh, and G. Mitchison, “Biological Sequence Analysis: Probabilistic Models of \\n\\nProteins and Nucleic Acids,” Cambridge: Cambridge University Press, 1998. \\n\\n141. J. M. Kaplan,el, R. G. Winther, “Prisoners of Abstraction? The Theory and Measure of Genetic Variation, and the \\n\\nVery Concept of “Race”,” Biological Theory, vol. 7. 2012. \\n\\n142. P. J. Carrington, and J. Scott, “Social Network Analysis: An Introduction,” The Sage Handbook of Social \\n\\nNetwork Analysis, London, vol. 1, 2011. \\n\\n143. “Yippy growing by leaps, bounds,” The News-Press. 23 May 2010, Retrieved 24 May 2010. \\n\\n144. D. Dirk, “A concept-oriented approach to support software maintenance and reuse activities” 5th Joint Conference \\n\\non Knowledge Based Software Engineering, 2002. \\n\\n145. M. G. B. Dias, N. Anquetil, and K. M. D. Oliveira, “Organizing the knowledge used in software maintenance,” \\n\\nJournal of Universal Computer Science, vol. 9, no. 7, pp. 641–658, 2003. \\n\\n146. R. Francesco, L Rokach and B. Shapira, “Introduction to Recommender Systems Handbook,” Recommender \\n\\nSystems Handbook, Springer, 2011, pp. 1-35. \\n\\n147.  “www.educationaldatamining.org,” 2013. \\n\\n148. R. Baker, “Data Mining for Education,” International Encyclopedia of Education (3rd edition), Oxford, UK, \\n\\nElsevier, vol. 7, pp. 112-118, 2010. \\n\\n149. G. Siemens, R. S. J. D. Baker, “Learning analytics and educational data mining: towards communication and \\n\\ncollaboration,” 2nd International Conference on Learning Analytics and Knowledge, pp. 252–254, 2012. \\n\\nhttp://www.bioinfo.in/uploadfiles/13031311552_1_1_JMLT.pdf\\nhttp://www.bioinfo.in/uploadfiles/13031311552_1_1_JMLT.pdf\\nhttp://www.sciencedirect.com/science/article/pii/S095741740700663X#bib18\\nhttp://ieeexplore.ieee.org/search/searchresult.jsp?searchWithin=p_Authors:.QT.Grefenstette,%20J.J..QT.&searchWithin=p_Author_Ids:37388906500&newsearch=true\\nhttp://en.wikipedia.org/wiki/Eugene_Lawler\\nhttp://en.wikipedia.org/wiki/Christos_H._Papadimitriou\\nhttp://en.wikipedia.org/wiki/Kenneth_Steiglitz\\nhttp://www.vldb.org/pvldb/vol7/p541-jiang.pdf\\nhttp://www.vldb.org/pvldb/vol7/p541-jiang.pdf\\nhttp://en.wikipedia.org/wiki/Richard_M._Durbin\\nhttp://en.wikipedia.org/wiki/Sean_Eddy\\nhttp://en.wikipedia.org/wiki/Anders_Krogh\\nhttp://books.google.com/books?id=2chSmLzClXgC&pg=PA1\\nhttp://www.news-press.com/article/20100523/BUSINESS/5230344/1014/BUSINESS/Yippy-growing-by-leaps--bounds\\nhttp://en.wikipedia.org/wiki/The_News-Press\\nhttp://www.educationaldatamining.org/\\n\\n\\n150. R. Huth, C. Beck, A. Philipp, M. Demuzere, Z. Ustrnul, M. Cahynova, J. Kysely, and O. E. Tveito, \\n\\n“Classifications of Atmospheric Circulation Patterns: Recent Advances and Applications” Annals of the New \\n\\nYork Academy Science, vol. 1146, no. 1, pp. 105-152, 2008. \\n\\n151. A. Bewley. R. Shekhar, S. Leonard, B. Upcroft, and P. Lever, “Real-time volume estimation of a dragline \\n\\npayload,” IEEE International Conference on Robotics and Automation\", pp. 1571-1576, 2011. \\n\\n152. C. D. Manning, P. Raghavan, and H. Schu¨tze, “An Introduction to Information Retrieval,” Cambridge University, \\n\\nPress, 2009. \\n\\n153. D. T. Nguyen, L. Chen, and C. K. Chan, “Clustering with Multi-viewpoint-Based Similarity Measure,” IEEE \\n\\nTransactions on Knowledge and Data Engineering, vol. 24, no. 6, pp. 988-1001, 2012. \\n\\n154.  Bravais, “Memoires par divers savants,” T, IX, Paris, pp. 255–332, 1846. \\n\\n155. K. Pearson, “Mathematical Contributions to the Theory of Evolution, III, Regression, Heredity, and Panmixia,” \\n\\nPhilosophical Transactions of the Royal Society of London, Series A, vol. 187, pp. 253–318, 1896. \\n\\n156. T. Sørensen, “A method of establishing groups of equal amplitude in plant sociology based on similarity of \\n\\nspecies and its application to analyses of the vegetation on Danish commons,” Kongelige Danske Videnskabernes \\n\\nSelskab, vol. 5, no. 4, pp. 1–34, 1948. \\n\\n157. L. R. Dice, “Measures of the Amount of Ecologic Association Between Species,” Ecology, vol. 26, no. 3, pp. \\n\\n297–302, 1945. \\n\\n158. J. D. Hamilton, “Time Series Analysis,” Princeton University Press, 1994. \\n\\n159. R. S. Tsay, “Analysis of Financial Time Series,” John Wiley & SONS, 2005. \\n\\n160. A Saxena and J. Wang, “Dimensionality Reduction with Unsupervised Feature Selection and Applying Non-\\n\\nEuclidean Norms for Classification Accuracy,” International Journal of Data Warehousing and Mining (IJDWM), \\n\\nvol. 6, no. 2, pp. 22–40, 2010. \\n\\n161. S. Arora, I. Chana, “A Survey of Clustering Techniques for Big Data Analysis,” 5th International Conference on \\n\\nThe Next Generation Information Technology Summit (Confluence), 2014. \\n\\n162. A. S. Shirkhorshidi, S. Aghabozorgi, T. Y. Wah, and T. Herawan, “Big Data Clustering: A Review,” Lecture \\n\\nNotes in Computer Science, vol. 8583, pp. 707-720, 2014. \\n\\n163. H. Wang, W. Wang, J. Yang, and P. S. Yu, “Clustering by Pattern Similarity in Large Data Sets,” International \\n\\nConference on Management of Data, ACM, 2002. \\n\\n164. Z. Huang, “A Fast Clustering Algorithm to Cluster Very Large Categorical Data Sets in Data Mining,” DMKD. \\n\\n1997. \\n\\n165. X. Wu, X. Zhu, G. Q. Wu, and W. Ding, “Data mining with big data,” IEEE Transaction on Knowledge and Data \\n\\nEngineering, vol. 26, no. 1, pp. 97-107, 2014. \\n\\n166. P. Russom, “Big Data Analytics,” TDWI Best Practices Report, Fourth Quarter, 2011. \\n\\n167. C. Xiao, F. Nie, and H. Huang, “Multi-view k-means clustering on big data,” The Twenty-Third International \\n\\nJoint Conference on Artificial Intelligence, AAAI, 2013. \\n\\n168. W. Fan and B. Albert, “Mining Big Data: Current Status and Forecast to the Future,” ACM SIGKDD Explorations \\n\\nNewsletter, vol. 14, no. 2, pp. 1-5, 2013. \\n\\n169. K. Shvachko, H. Kuang, S. Radia, and R. Chansler, “The hadoop distributed file system,” IEEE 26th Symposium \\n\\non Mass Storage Systems and Technologies (MSST), 2010. \\n\\n170. D. Jeffrey and S. Ghemawat, “MapReduce: a flexible data processing tool,” Communications of the ACM, vol. \\n\\n53, no. 1, pp. 72-77, 2010. \\n\\n171. https://hadoop.apache.org/ \\n\\n172. G. Celeux, and G. Govaert, “A classification EM algorithm for clustering and two stochastic versions,” \\n\\nComputational statistics & Data analysis, vol. 14, no. 3, pp. 315-332, 1992. \\n\\n173. L. Kaufman and P. Rousseeuw, “Finding Groups in Data: An Introduction to Cluster Analysis,” Wiley, 1990. \\n\\n174. R. Ngand and J. Han, “CLARANS: A method for clustering objects for spatial data mining,” IEEE Trans. \\n\\nKnowledge Data Engineering, vol. 14, no. 5, pp. 1003–1016, 2002. \\n\\n175. Sisodia, singh, sisodia, and saxena, “Clustering Techniques: A Brief Survey of Different Clustering Algorithms”, \\n\\nInternational Journal of Latest Trends in Engineering and Technology (IJLTET), vol. 1, no. 3, pp. 82-87, 2012. \\n\\n176. Zhong, Miao, and Wang, “A graph-theoretical clustering method based on two rounds of minimum spanning \\n\\ntrees,” Pattern Recognition, vol. 43, pp. 752 – 766, 2010. \\n\\n177.  Y. Chen, S. Sanghavi, and H. Xu, “Improved graph clustering,” IEEE Transactions on Information Theory, vol. \\n\\n60, no. 10, pp. 6440-6455, 2014. \\n\\n178. A. Condon, and R. Karp, “Algorithms for graph partitioning on the planted partition model,” Random Structures \\n\\nAlgorithms, vol. 18, no. 2, pp. 116-140, 2001. \\n\\n179. W. E. Donath and A. J. Hoffman, “Lower bounds for the partitioning of graphs,” IBM J. Res. Develop., vol. 17, \\n\\npp. 420 – 425, 1973. \\n\\n180. J. Shi, J. and J. Malik, “Normalized cuts and image segmentation,” IEEE Transactions on Pattern Analysis and \\n\\nMachine Intelligence, vol. 22, no. 8, pp. 888 – 905, 2000. \\n\\n181. U. Luxburg, “A tutorial on spectral clustering,” Statistics and Computing, vol. 17, no. 4, pp. 395-416, 2007. \\n\\n182. K. Rohe, S. Chatterjee, and B. Yu, “Spectral clustering and the high-dimensional stochastic block model,” The \\n\\nAnnals of Statistics, vol. 39, no. 4, pp. 1878-1915, 2011. \\n\\n183.  S. Gunnemann, I. Farber, B. Boden, and T. Seidl, “Subspace clustering meets dense sub-graph mining,” A \\n\\nsynthesis of two paradigms, In ICDM, 2010. \\n\\nhttp://en.wikipedia.org/wiki/Plant_sociology\\nhttp://en.wikipedia.org/wiki/Kongelige_Danske_Videnskabernes_Selskab\\nhttp://en.wikipedia.org/wiki/Kongelige_Danske_Videnskabernes_Selskab\\nhttp://amzn.to/1cqB6QD\\nhttp://amzn.to/1blTqWD\\nhttp://link.springer.com/bookseries/558\\nhttp://link.springer.com/bookseries/558\\n\\n\\n184. K. Macropol and A. Singh, “Scalable discovery of best clusters on large graphs,” Proceedings of the VLDB \\n\\nEndowment, vol. 3, no. 1-2, pp. 693-702, 2010. \\n\\n185. J. J. Whang, X. Sui, and I. S. Dhillon, “Scalable and memory-efficient clustering of large-scale social networks,” \\n\\nIn ICDM, 2012. \\n\\n186. G. Karypis and V. Kumar, “A fast and high quality multilevel scheme for partitioning irregular graphs,” SIAM \\n\\nJournal on Scientific Computing, vol. 20, no. 1, pp. 359-392, 1998. \\n\\n187. G. Karypis and V. Kumar, “Multilevel k-way partitioning scheme for irregular graphs,” Journal of Parallel and \\n\\nDistributed Computing, vol. 48, pp. 96-129, 1998. \\n\\n188. D. Yan, L. Huang, and M. I. Jordan, “Fast approximate spectral clustering,” In KDD, pp. 907-916, 2009. \\n\\n189. J. Liu, C. Wang, M. Danilevsky, and J. Han, “Large-scale spectral clustering on graphs,” In IJCAI, 2013. \\n\\n190. W. Yang and H. Xu, “A divide and conquer framework for distributed graph clustering,” In ICML, 2015. \\n\\n191. Ghosh and Dubey, “Comparative Analysis of K-Means and Fuzzy C Means Algorithms,” International Journal of \\n\\nAdvanced Computer Science and Applications, vol. 4, no.4, pp. 35-39, 2013. \\n\\n192. S. Niwattanakul, J. Singthongchai, E. Naenudorn and S. Wanapu, “Using of Jaccard Coefficient for Keywords \\n\\nSimilarity”, Proceedings of the International MultiConference of Engineers and Computer Scientists 2013 Vol I, \\n\\nIMECS 2013, March 13 - 15, 2013, Hong Kong, 1-5.  \\n\\n193. C. Chen, L. Pau, and P. Wang, “Hand book of Pattern Recognition and Computer Vision , Eds., World Scientific, \\n\\nSingapore, pp. 3 –32. R.Dubes, “Cluster analysis and related issue”. \\n\\n194. A. Jain and R. Dubes, “Algorithms for Clustering Data,” Englewood, Cliffs, NJ: Prentice-Hall, 1988. \\n\\n195. C. Shi, Y. Cai, D. Fu, Y. Dong, and B. Wu, “A link clustering based overlapping community detection algorithm,” \\n\\nData & Knowledge Engineering, vol. 87, pp. 394–404, 2013. \\n\\n196. G. Palla, I. Derenyi, I. Farkas, and T. Vicsek, “Uncovering the overlapping community structure of complex \\n\\nnetworks in nature and society,” Nature, vol. 435, pp. 814–818, 2005. \\n\\n197. D. H. Wolpert and W. G. Macready, “No Free Lunch Theorem for Optimization,” IEEE Transactions on \\n\\nEvolutionary Computation, vol. 1, No. 1, pp. 67-82, 1997 \\n\\n198. Bensmail, H., Celeux, G., Raftery, A. E. and Robert, C. P. (1997) Inference in model-based cluster analysis. \\n\\nStat.Comput., 7, 1–10. \\n\\n199. Xu.D., Tian, Y., “A Comprehensive Survey o f Clustering Algorithms”, Ann. Data Sci. 2, 165-193,2015. \\n\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "#Testing on a sample PDF file\n",
        "filename = '/Dataset-IK/Amit Saxena/A Review of Clustering Techniques.pdf'\n",
        "extract_text(filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb15d444",
      "metadata": {
        "id": "cb15d444"
      },
      "source": [
        "## If you only want to extract the 'Abstract' from a research paper\n",
        "### Please note that this function might not work correctly for some PDFs.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abf8c8e1",
      "metadata": {
        "id": "abf8c8e1"
      },
      "outputs": [],
      "source": [
        "# Function to extract abstract from PDF\n",
        "def extract_abstract(filename):\n",
        "    # Parse PDF file\n",
        "    parsed = parser.from_file(filename)\n",
        "    print(type(parsed))\n",
        "\n",
        "    # Extract text from parsed data\n",
        "    text = parsed[\"content\"]\n",
        "    # Find introduction section by looking for keywords\n",
        "    abstract_start = text.find(\"Abstract\")\n",
        "    print(f\"abstract_start {abstract_start}\")\n",
        "    abstract_end_options = [text.find(\"\\n1 \", abstract_start), text.find(\"\\ni \", abstract_start), text.find(\"\\nI \", abstract_start)]\n",
        "    abstract_end = min(pos for pos in abstract_end_options if pos != -1)\n",
        "    # Extract introduction section\n",
        "    abstract = text[abstract_start:abstract_end]\n",
        "\n",
        "    return abstract"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filename = '/Dataset-IK/Amit Saxena/A Review of Clustering Techniques.pdf'\n",
        "extract_abstract(filename)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "NTeIc2_dmDpW",
        "outputId": "32d57d07-8f23-4a48-f6eb-fc9f22671895"
      },
      "id": "NTeIc2_dmDpW",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'dict'>\n",
            "abstract_start 794\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Abstract \\n\\nThis paper presents a comprehensive study on clustering: exiting methods and developments made at various \\n\\ntimes. Clustering is defined as an unsupervised learning where the objects are grouped on the basis of some \\n\\nsimilarity inherent among them. There are different methods for clustering the objects such as hierarchical, \\n\\npartitional, grid, density based and model based. The approaches used in these methods are discussed with their \\n\\nrespective states of art and applicability. The measures of similarity as well as the evaluation criteria, which are \\n\\nthe central components of clustering are also presented in the paper. The applications of clustering in some \\n\\nfields like image segmentation, object and character recognition and data mining are highlighted. \\n\\nKeywords: Unsupervised learning, Clustering, Data mining, Pattern recognition, Similarity measures \\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dcd681d9",
      "metadata": {
        "id": "dcd681d9"
      },
      "source": [
        "# Extracting Introduction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1f5c8de",
      "metadata": {
        "id": "e1f5c8de"
      },
      "outputs": [],
      "source": [
        "def extract_intro(filename):\n",
        "    # Parse PDF file\n",
        "    parsed = parser.from_file(filename)\n",
        "\n",
        "    # Extract text from parsed data\n",
        "    text = parsed[\"content\"]\n",
        "\n",
        "    # Find introduction section by looking for the \"1 Introduction\" heading\n",
        "    intro_start = text.find(\"Introduction\")\n",
        "\n",
        "    # Find the next heading that starts with \"2 \", \"ii \", or \"II \"\n",
        "    intro_end_options = [text.find(\"\\n2 \", intro_start), text.find(\"\\nii \", intro_start), text.find(\"\\nII \", intro_start)]\n",
        "    intro_end = min(pos for pos in intro_end_options if pos != -1)\n",
        "\n",
        "    # Extract the introduction section\n",
        "    intro = text[intro_start:intro_end]\n",
        "\n",
        "    return intro"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9fc9167f",
      "metadata": {
        "id": "9fc9167f"
      },
      "source": [
        "# Using NER to extract author names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf35040d",
      "metadata": {
        "id": "bf35040d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "dd1c7507-4e68-4d05-8f4d-ac2d91cd097e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-23.2.1-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (67.7.2)\n",
            "Collecting setuptools\n",
            "  Downloading setuptools-68.2.2-py3-none-any.whl (807 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m807.9/807.9 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (0.41.2)\n",
            "Installing collected packages: setuptools, pip\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 67.7.2\n",
            "    Uninstalling setuptools-67.7.2:\n",
            "      Successfully uninstalled setuptools-67.7.2\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 23.1.2\n",
            "    Uninstalling pip-23.1.2:\n",
            "      Successfully uninstalled pip-23.1.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pip-23.2.1 setuptools-68.2.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "_distutils_hack",
                  "pkg_resources",
                  "setuptools"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.7)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.9)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.10.12)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (68.2.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.10)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.1.2)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.3)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m2023-09-29 01:42:21.188983: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-09-29 01:42:23.040915: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Collecting en-core-web-sm==3.6.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.6.0/en_core_web_sm-3.6.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.6.0) (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.4.7)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.9)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.10.12)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (68.2.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.7.10)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.1.2)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.1.3)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ],
      "source": [
        "!pip install -U pip setuptools wheel\n",
        "!pip install -U spacy\n",
        "!python3 -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "beb30d3e",
      "metadata": {
        "id": "beb30d3e"
      },
      "source": [
        "### Via 'spacy'\n",
        "spaCy is a free open-source library for Natural Language Processing in Python. It features NER, POS tagging, dependency parsing, word vectors and more."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8269eeb",
      "metadata": {
        "id": "b8269eeb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f3f4977-3327-4a9e-f46c-1a101b4414a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install PyPDF2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efe3176c",
      "metadata": {
        "id": "efe3176c"
      },
      "outputs": [],
      "source": [
        "import PyPDF2 #Another text extractor. You can use it when tika doesn't work.\n",
        "import spacy\n",
        "\n",
        "def extract_author_names(filename):\n",
        "\n",
        "    # Load the spaCy NER model\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "    # Open the PDF file\n",
        "    pdf_file = open(filename, 'rb')\n",
        "\n",
        "    # Create a PDF reader object\n",
        "    pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
        "\n",
        "    # Extract the text from the first page\n",
        "    first_page = pdf_reader.pages[0]\n",
        "    first_page_text = first_page.extract_text()\n",
        "\n",
        "    # Close the PDF file\n",
        "    pdf_file.close()\n",
        "\n",
        "    # Apply the spaCy NER model to the text\n",
        "    doc = nlp(first_page_text)\n",
        "    author_names = []\n",
        "    for entity in doc.ents:\n",
        "        if entity.label_ == 'PERSON':\n",
        "            author_names.append(entity.text)\n",
        "\n",
        "    # Return the list of author names\n",
        "    return author_names"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "extract_author_names(filename)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjAQyprozcz2",
        "outputId": "b208186b-0cf5-4e5a-e2c1-a8035be0bcdf"
      },
      "id": "pjAQyprozcz2",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Aruna Tiwari4',\n",
              " 'Meng Joo',\n",
              " 'Weiping Ding6',\n",
              " 'Chin -Teng Lin2',\n",
              " 'Guru Ghasidas Vishwavidyal']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a77b384",
      "metadata": {
        "id": "3a77b384"
      },
      "source": [
        "## Extract 'Abstract', 'Introduction', and 'Author_Names' together"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86a14361",
      "metadata": {
        "id": "86a14361",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a87c1f7-f7dd-40d7-e4d2-bef6bfd18903"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-09-29 01:44:12.952225: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Collecting en-core-web-sm==3.6.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.6.0/en_core_web_sm-3.6.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.6.0) (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.4.7)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.9)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.10.12)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (68.2.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.7.10)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.1.2)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.1.3)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "!python3 -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7dc42b01",
      "metadata": {
        "id": "7dc42b01",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "635a7801-f75b-4edd-ccaa-9d68005fec1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-09-29 01:44:30.542018: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Collecting en-core-web-lg==3.6.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.6.0/en_core_web_lg-3.6.0-py3-none-any.whl (587.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.7/587.7 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-lg==3.6.0) (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2.4.7)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2.0.9)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (1.10.12)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (68.2.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (0.7.10)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (0.1.2)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2.1.3)\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-3.6.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n"
          ]
        }
      ],
      "source": [
        "!python3 -m spacy download en_core_web_lg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04e1e5d2",
      "metadata": {
        "id": "04e1e5d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3361390b-85d6-4784-9035-f14509265380"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-09-29 01:45:23.097669: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'en' are deprecated. Please use the\n",
            "full pipeline package name 'en_core_web_sm' instead.\u001b[0m\n",
            "Collecting en-core-web-sm==3.6.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.6.0/en_core_web_sm-3.6.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.6.0) (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.4.7)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.9)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.10.12)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (68.2.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.7.10)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.1.2)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.1.3)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ],
      "source": [
        "!python3 -m spacy download en"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d69bded",
      "metadata": {
        "id": "3d69bded",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aea40838-f4ba-4212-83b9-78f9743a38fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'dict'>\n",
            "abstract_start 794\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import tika\n",
        "from tika import parser\n",
        "\n",
        "#tqdm --> Can help you with the progress bar\n",
        "\n",
        "# Set up Tika\n",
        "tika.initVM()\n",
        "\n",
        "#Testing on a sample PDF file\n",
        "filename = '/Dataset-IK/Amit Saxena/A Review of Clustering Techniques.pdf'\n",
        "\n",
        "# Extract the introduction from the PDF file\n",
        "text = extract_text(filename)\n",
        "intro = extract_intro(filename)\n",
        "abstract = extract_abstract(filename)\n",
        "author_names = extract_author_names(filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "688e2355",
      "metadata": {
        "id": "688e2355"
      },
      "source": [
        "# Complete Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22c1a894",
      "metadata": {
        "id": "22c1a894"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tika\n",
        "from tika import parser\n",
        "\n",
        "#tqdm --> Can help you with the progress bar\n",
        "\n",
        "# Set up Tika\n",
        "tika.initVM()\n",
        "\n",
        "folder_names = []\n",
        "texts = []\n",
        "filenames = []\n",
        "introduction_vector = []\n",
        "abstract_vector = []\n",
        "author_names_vector = []\n",
        "\n",
        "c = 0 # For printing indexes of the PDF files on which tika couldn't extract the text successfully\n",
        "\n",
        "for root, dirs, files in os.walk('/Dataset-IK/'):\n",
        "     for file in files:\n",
        "        #print(os.path.join(root, file))\n",
        "\n",
        "        c = c + 1\n",
        "\n",
        "        try:\n",
        "            # Extract the introduction from the PDF file\n",
        "            text = extract_text(os.path.join(root, file))\n",
        "            #intro = extract_intro(os.path.join(root, file))\n",
        "            #abstract = extract_abstract(os.path.join(root, file))\n",
        "            #author_names = extract_author_names(os.path.join(root, file))\n",
        "\n",
        "            folder_names.append(root.split('/')[-1])\n",
        "            texts.append(text)\n",
        "            #introduction_vector.append(intro)\n",
        "            #abstract_vector.append(abstract)\n",
        "            #author_names_vector.append(author_names)\n",
        "            filenames.append(file)\n",
        "\n",
        "        except:\n",
        "            print(\"Index for the failed files:\", c)\n",
        "            #introduction_vector.append([])\n",
        "            #abstract_vector.append([])\n",
        "            #author_names_vector.append([])\n",
        "            texts.append([])\n",
        "            filenames.append([])\n",
        "            folder_names.append([])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83de3e35",
      "metadata": {
        "id": "83de3e35"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "df = pd.DataFrame(filenames,columns = ['FileName'])\n",
        "df['Author'] = folder_names\n",
        "df['Text'] = texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "211fedc3",
      "metadata": {
        "id": "211fedc3",
        "outputId": "b799d041-5949-46b7-8b4f-38425a5302d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(16, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27fddce3",
      "metadata": {
        "id": "27fddce3",
        "outputId": "c3902239-412a-4d4f-8096-47960e76b60c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                             FileName       Author  \\\n",
              "0            Controlled synthesis.pdf  Amit Saxena   \n",
              "1       High Oxygen Nanocomposite.pdf  Amit Saxena   \n",
              "2                    Cutting Edge.pdf  Amit Saxena   \n",
              "3      Maternal and Fetal Factors.pdf  Amit Saxena   \n",
              "4  Opposing Actions of Fibroblast.pdf  Amit Saxena   \n",
              "\n",
              "                                                Text  \n",
              "0  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...  \n",
              "1  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...  \n",
              "2  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...  \n",
              "3  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...  \n",
              "4  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-aa516cf9-0794-4dd5-93aa-79136e36509e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>FileName</th>\n",
              "      <th>Author</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Controlled synthesis.pdf</td>\n",
              "      <td>Amit Saxena</td>\n",
              "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>High Oxygen Nanocomposite.pdf</td>\n",
              "      <td>Amit Saxena</td>\n",
              "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Cutting Edge.pdf</td>\n",
              "      <td>Amit Saxena</td>\n",
              "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Maternal and Fetal Factors.pdf</td>\n",
              "      <td>Amit Saxena</td>\n",
              "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Opposing Actions of Fibroblast.pdf</td>\n",
              "      <td>Amit Saxena</td>\n",
              "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-aa516cf9-0794-4dd5-93aa-79136e36509e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-aa516cf9-0794-4dd5-93aa-79136e36509e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-aa516cf9-0794-4dd5-93aa-79136e36509e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-37e96827-682f-4d9b-a1db-64f23c547af3\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-37e96827-682f-4d9b-a1db-64f23c547af3')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-37e96827-682f-4d9b-a1db-64f23c547af3 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "508bddea",
      "metadata": {
        "id": "508bddea",
        "outputId": "1f69b611-28ae-4bdd-8da6-a33fb9f30459",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAdvances in Natural Sciences:\\nNanoscience and Nanotechnology\\n\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\n\\nPAPER • OPEN ACCESS\\n\\nControlled synthesis of size-tunable nickel and\\nnickel oxide nanoparticles using water-in-oil\\nmicroemulsions\\nTo cite this article: Ajeet Kumar et al 2013 Adv. Nat. Sci: Nanosci. Nanotechnol. 4 025009\\n\\n\\xa0\\n\\nView the article online for updates and enhancements.\\n\\nYou may also like\\nCapacitive Behavior of Porous Nickel\\nOxide/Hydroxide Electrodes with\\nInterconnected Nanoflakes Synthesized by\\nAnodic Electrodeposition\\nMao-Sung Wu, Yu-An Huang and Chung-\\nHsien Yang\\n\\n-\\n\\nNickel oxide nanotube synthesis using\\nmultiwalled carbon nanotubes as sacrificial\\ntemplates for supercapacitor application\\nAhmed M Abdalla, Rakesh P Sahu,\\nCameron J Wallar et al.\\n\\n-\\n\\nInfluence of transverse magnetic field on\\nthe properties of laser ablation produced\\nnickel oxide nanoparticles\\nMina Safa, Davoud Dorranian, Amir Ali\\nMasoudi et al.\\n\\n-\\n\\nThis content was downloaded from IP address 122.184.65.228 on 22/02/2023 at 18:53\\n\\nhttps://doi.org/10.1088/2043-6262/4/2/025009\\nhttps://iopscience.iop.org/article/10.1149/1.2969948\\nhttps://iopscience.iop.org/article/10.1149/1.2969948\\nhttps://iopscience.iop.org/article/10.1149/1.2969948\\nhttps://iopscience.iop.org/article/10.1149/1.2969948\\nhttps://iopscience.iop.org/article/10.1088/1361-6528/aa53f3\\nhttps://iopscience.iop.org/article/10.1088/1361-6528/aa53f3\\nhttps://iopscience.iop.org/article/10.1088/1361-6528/aa53f3\\nhttps://iopscience.iop.org/article/10.1088/1402-4896/abd057\\nhttps://iopscience.iop.org/article/10.1088/1402-4896/abd057\\nhttps://iopscience.iop.org/article/10.1088/1402-4896/abd057\\n\\n\\nIOP PUBLISHING ADVANCES IN NATURAL SCIENCES: NANOSCIENCE AND NANOTECHNOLOGY\\n\\nAdv. Nat. Sci.: Nanosci. Nanotechnol. 4 (2013) 025009 (9pp) doi:10.1088/2043-6262/4/2/025009\\n\\nControlled synthesis of size-tunable nickel\\nand nickel oxide nanoparticles using\\nwater-in-oil microemulsions\\nAjeet Kumar1, Amit Saxena1, Arnab De2, Ravi Shankar1\\n\\nand Subho Mozumdar1\\n\\n1 Department of Chemistry, University of Delhi, Delhi 110007, India\\n2 Department of Microbiology and Immunology, Columbia University, USA\\n\\nE-mail: subhoscom@yahoo.co.in\\n\\nReceived 11 November 2012\\nAccepted for publication 3 April 2013\\nPublished 19 April 2013\\nOnline at stacks.iop.org/ANSN/4/025009\\n\\nAbstract\\nIndustrial demands have generated a growing need to synthesize pure metal and metal–oxide\\nnanoparticles of a desired size. We report a novel and convenient method for the synthesis of\\nspherical, size tunable, well dispersed, stable nickel and nickel oxide nanoparticles by\\nreduction of nickel nitrate at room temperature in a TX-100/n-hexanol/cyclohexane/water\\nsystem by a reverse microemulsion route. We determined that reduction with alkaline sodium\\nborohydrate in nitrogen atmosphere leads to the formation of nickel nanoparticles, while the\\nuse of hydrazine hydrate in aerobic conditions leads to the formation of nickel oxide\\nnanoparticles. The influence of several reaction parameters on the size of nickel and nickel\\noxide nanoparticles were evaluated in detail. It was found that the size can be easily controlled\\neither by changing the molar ratio of water to surfactant or by simply altering the\\nconcentration of the reducing agent. The morphology and structure of the nanoparticles were\\ncharacterized by quasi-elastic light scattering (QELS), transmission electron microscopy\\n(TEM), x-ray diffraction (XRD), electron diffraction analysis (EDA) and energy dispersive\\nx-ray (EDX) spectroscopy. The results show that synthesized nanoparticles are of high purity\\nand have an average size distribution of 5–100 nm. The nanoparticles prepared by our simple\\nmethodology have been successfully used for catalyzing various chemical reactions.\\n\\nKeywords: nickel nanoparticles, nickel oxide nanoparticles, water-in-oil microemulsions,\\nnon-ionic surfactant\\n\\nClassification number: 4.02\\n\\n1. Introduction\\n\\nNanoparticles in general have different electronic, magnetic\\nand chemical properties as compared to the property of\\nthe bulk material. The main reason for that is their large\\nsurface-to-volume ratio which results from their small sizes.\\nNickel nanoparticles in particular have been used in chemical\\ncells, fuel cells, for solar energy absorption, as catalysts [1],\\nas magnetic materials, etc [2, 3]. Nickel nanoparticles when\\n\\nContent from this work may be used under the terms of\\nthe Creative Commons Attribution 3.0 licence. Any further\\n\\ndistribution of this work must maintain attribution to the author(s) and the\\ntitle of the work, journal citation and DOI.\\n\\nmixed in minima amounts (1%) with solid rocket propellant\\ncan double its heat of combustion [4]. The demand for\\nnano-nickel material has risen with the rapid development\\nof different telecommunication equipments [5]. Therefore,\\nit is necessary to develop a protocol for synthesizing well\\ndispersed, stable, nickel nanoparticles with different sizes so\\nas to meet different demands. The protocol will be most\\nuseful if the sizes of the synthesized particles can be tuned\\nby adjusting simple reaction parameters.\\n\\nThere are a number of ways to synthesize nickel\\nnanoparticles, however, they all suffer from crucial\\nshortcomings. These methods include chemical vapor\\ndeposition (CVD) [6], wet chemical [4, 7], laser-driven\\n\\n2043-6262/13/025009+09$33.00 1 © 2013 Vietnam Academy of Science & Technology\\n\\nhttp://dx.doi.org/10.1088/2043-6262/4/2/025009\\nmailto:subhoscom@yahoo.co.in\\nhttp://stacks.iop.org/ANSN/4/025009\\nhttp://creativecommons.org/licenses/by/3.0\\n\\n\\nAdv. Nat. Sci.: Nanosci. Nanotechnol. 4 (2013) 025009 Ajeet Kumar et al\\n\\naerosol [8], hydrothermal [9], co-precipitation [10],\\nsol–gel [11] and microemulsion [12–14]. However, the\\nreported methodology is not always convenient for the\\nproduction of monodispersed, stable and ultra-low sized\\nnanoparticles as it requires extensive use of expensive\\nequipment and difficult production conditions. Typically,\\npreparation of nickel involves the solution phase chemistry\\nroute, which in theory should provide multiple, simple\\nways to control the morphology, particle size and desirable\\ncrystalline phase. Ideally, the process should be amenable to\\nscaling up. Nickel nanoparticles have been synthesized by\\nreduction of metal salts using reducing agents such as NaBH4\\n\\n[15, 16], hydrazine [17–20] and polyols [21–25]. While\\nthese processes can produce spherical, stable nanoparticles\\nwithout agglomeration; the synthesized particle surfaces\\nare often found to be rough and exhibit spiky surface\\nmorphology [26–28].\\n\\nWater-in-oil (w/o) microemulsion solutions can serve\\nas a model medium for synthesis of very fine and\\nmonodispersed [29–31] nanoparticles. Firstly, they can act as\\nmicroreactors for the reaction. Additionally, they can inhibit\\nagglomeration of the particles as there is a self-correcting\\ntendency of the surfactants adsorbing on the particle surface\\nif the particle size approaches that of the water pool.\\n\\nWe disclose herein a method for the synthesis of\\nwell dispersed, sphere-shaped, highly stable nickel and\\nnickel oxide nanoparticles in similar microemulsion pool.\\nWe discover that the use of alkaline sodium borohydrate\\nin nitrogen atmosphere leads to the formation of nickel\\nnanoparticles, while the use of hydrazine hydrate in\\naerobic conditions leads to the formation of nickel oxide\\nnanoparticles. Additionally, the size of the nickel and nickel\\noxide nanoparticles can be easily controlled either by\\nchanging the molar ratio of water to surfactant or by altering\\nthe concentration of the reducing agent Wo. The protective\\nnature of the surfactant molecules prevents the nanoparticles\\nfrom oxidation (in the case of metallic nickel nanoparticles)\\nand agglomeration. The method does not require elevated\\ntemperature and the entire synthesis can be carried out at room\\ntemperature.\\n\\n2. Material and methods\\n\\nAll the experiments were performed at room temperature.\\nElectron micrographs were taken with a TEM TECHNAI\\n300 kV, Ultra twin FEI with EDAX transmission electron\\nmicroscope operating at 300 kV. The average particle\\ndiameter of the prepared nanoparticles was carried out by\\na dynamic light scattering instrument (Photocor FC, USA).\\nThe measuring range was from 1 to 5000 nm and the light\\nsource was He–Ne 633 nm laser diode of 1–40 MW. Data\\nanalysis was performed with Alango Dynal V 2.0 Software.\\nWide angle x-ray diffraction pattern was obtained for nickel\\nand nickel oxide nanoparticles by using a Philips analytica\\nPW 1830 x-ray VB equipped with a 2θ compensating slit,\\nCuKα radiation (1.54 Å) at 40 kV, 40 mA passing through\\nNi filter with a wavelength of 0.154 nm at 20 mA and\\n35 kV. Data collection was made in a continuous scan mode\\nwith a step size of 0.01◦ and step time of 1 s over a 2θ\\n\\nrange of 30–90◦. Data analysis was performed with PC-APD\\ndiffraction software.\\n\\n2.1. Preparation of nickel and nickel oxide nanoparticles\\n\\nA typical synthesis of the nickel nanoparticles involved the\\nmixing of two reverse microemulsions (RM-A and RM-B).\\nRM-A was prepared by taking 25 ml of 0.1 M solution of\\nTX-100 in cyclohexane and adding 300 µl of n-hexanol\\nand 225 µl of a 5% (w/v) aqueous solution of Ni(NO3)2.\\nSimilarly, RM-B was prepared by taking 25 ml of 0.1 M\\nsolution of TX-100 in cyclohexane and adding 300 µl of\\nn-hexanol and 225 µl of a 5% (w/v) aqueous solution of\\nalkaline NaBH4. The typical Wo value of this system was\\nfound to be 5. Both the microemulsions were left stirring for\\n30 min to obtain an optically clear homogeneous dispersion.\\nRM-B was then added to RM-A in a drop-wise manner with\\ncontinuous stirring. The resulting solution was left stirring for\\nanother 3 h to allow complete particle growth via Ostwald\\nripening. Instant appearance of black color indicates the\\nformation of nickel nanoparticles. Nitrogen atmosphere was\\nmaintained throughout the reaction procedure to ensure the\\ncomplete removal of oxygen. For the synthesis of nickel oxide\\nnanoparticles, amount and concentration of the reactants\\nwere similar to that of nickel nanoparticles. However, the\\nreduction reaction was carried out using 5% (w/v) N2H4 ·H2O\\nsolution in an aerobic condition instead of alkaline NaBH4\\n\\nin a nitrogen atmosphere. A diagrammatic scheme for the\\npreparation of nickel and nickel oxide nanoparticles using\\nreverse microemulsion is shown in scheme 1.\\n\\n3. Results and discussion\\n\\nThe nucleation step and the growth step are the two major\\nsteps in the process of synthesizing stable nanoparticles\\n[32, 33]. It is conceivable that one could change the reaction\\nparameters as a way of controlling these two crucial steps.\\nAqueous core of the reverse micellar solution behaves as a tiny\\nreaction vessel which was exploited for the reaction of nickel\\nnitrate with alkaline sodium borohydride under nitrogen\\natmosphere so as to generate highly stable, black colored,\\noptically clear solution of nickel nanoparticles. However,\\nwhen hydrazine hydrate is used as a reducing agent, the\\nreverse micellar solution gives an off-white colored, optically\\nclear solution of nickel oxide nanoparticles (scheme 1). It was\\nexperimentally determined that sodium borohydrate reduced\\nNi(NO3)2 to nickel nanoparticles; however, an inert nitrogen\\natmosphere is required for this (to prevent oxidation of\\nthe reduced metal). On the other hand, hydrazine hydrate\\nreduced Ni(NO3)2 to nickel oxide nanoparticles in an aerobic\\ncondition. Various other reaction parameters were studied\\nto tailor the size and morphology of the nanoparticles as\\nneeded.\\n\\n3.1. Influence of Wo over particle size\\n\\nWo can be defined as the ratio of molar concentration of\\nwater to surfactant. The composition of the solution has a\\nmajor effect in the size of the droplet of the microemulsion. If\\nthe size of the particle approaches that of the microemulsion\\ndroplet, the surfactant molecules get adsorbed on the surface\\nof the particle. The surfactant molecules can now act as a\\nprotective agent. Thus, the composition of the microemulsion\\nsolution influences not only the stability of microemulsion\\n\\n2\\n\\n\\n\\nAdv. Nat. Sci.: Nanosci. Nanotechnol. 4 (2013) 025009 Ajeet Kumar et al\\n\\nAq.Nickel Nitrate\\nTX-100\\n\\nn-hexanol\\nCyclohexane\\n\\nH2O\\n\\nAlk.Sodium Borohydride\\nTX-100\\n\\nn-hexanol\\nCyclohexane\\n\\nH2O\\n\\nAq.Hydrazine hydrate\\nTX-100\\n\\nn-hexanol\\nCyclohexane\\n\\nH2O\\n\\nDropwise addition\\n\\nNitrogen atmosphere\\n\\nDropwise addition\\n\\nWithout Nitrogen atmosphere\\n\\nNickel Nanoparticles \\nin microemulsion\\n\\nNickel oxide Nanoparticles\\nin Microemulsion\\n\\nOstwald Ripening Ostwald Ripening\\n\\nNickel Nanoparticles\\n\\nDrpwise addition of \\nabsolute ethanol\\n\\nWashed Thrice with \\nEthanol\\nCentrifuged and \\nRedispersed in \\nethanol\\n\\nDrpwise addition of \\nabsolute ethanol\\n\\nWashed Thrice with \\nEthanol\\nCentrifuged and \\nRedispersed in \\nethanol\\n\\nNickel oxide Nanoparticles\\n\\nSynthesis\\n of \\n\\nNickel Nanoparticles\\n\\nSynthesis\\n of \\n\\nNickel oxide\\nNanoparticles\\n\\n2 Ni2+  +  N2H4  +  6OH Air atmosphere; room temperature\\nNi2O+  N2 + 5H2O\\n\\n2 Ni2+  +  NaBH4 +  2H2O N2 atmosphere; room temperature\\n2Ni +  NaBO2  + 4H2\\n\\nScheme 1. General procedure for the synthesis of nickel and nickel oxide nanoparticles.\\n\\nTable 1. Effect of variation of Wo on the diameter of nickel and nickel oxide nanoparticles.\\n\\nDiameter of nickel Diameter of nickel oxide\\nnanoparticles (nm) nanoparticles (nm)\\n\\nWo ±2 nm Remarks ±2 nm Remarks\\n\\n1 8 Precipitated in 1 h 9 Stable\\n3 16 Stable 13 Stable\\n5 29 Stable 20 Stable\\n7 59 Stable 39 Stable\\n9 68 Stable 86 Stable\\n\\n11 81 Stable 110 Stable\\n13 – Precipitated – Precipitated\\n15 – Precipitated – Precipitated\\n\\nConditions: concentration of Ni(NO3)2 (aqueous solution (aq. sol.)) is 2% (w/v),\\nconcentration of N2H4 (aq. sol.) is 5% (w/v), set volume is 25 ml, surfactant concentration\\nis 0.1 M, concentration of NaBH4 (alk. sol.) is 5% (w/v), co-surfactant used n-hexanol.\\n\\nsystem but also the formation and growth of the nanoparticles.\\nThe average diameters for all of the nickel nanoparticles\\nobtained at different Wo are tabulated in table 1 and shown\\nin figure 1. In general, the size of the nanoparticles increases\\n\\nwith the Wo value. As shown in table 1 and figure 1,\\nthe size of the nickel particles increases from an average\\ndiameter of 8–100 nm as the Wo value increases from 1 to 11.\\nThus, increase of water content leads to bigger nanoparticles\\n\\n3\\n\\n\\n\\nAdv. Nat. Sci.: Nanosci. Nanotechnol. 4 (2013) 025009 Ajeet Kumar et al\\n\\nTable 2. Effect of variation of reducing agent concentration on the diameter of nickel and nickel oxide nanoparticles.\\n\\nReducing agent Diameter of nickel Diameter of nickel oxide\\nconcentration nanoparticles (nm) nanoparticles (nm)\\n(% w/v) ±2 nm Remarks ±2 nm Remarks\\n\\n0.01 34 Precipitated in 20 min 43 Stable\\n0.05 31 Stable 40 Stable\\n0.10 30 Stable 38 Stable\\n0.50 30 Stable 38 Stable\\n1.00 31 Stable 39 Stable\\n2.00 30 Stable 38 Stable\\n4.00 29 Precipitated 37 Precipitated\\n8.00 30 Precipitated 38 Precipitated\\n\\nConditions: concentration of Ni(NO3)2 (aq. sol.) is 2% w/v, set volume is 25 ml, surfactant concentration\\nis 0.1 M, co-surfactant used n-hexanol.\\n\\nTable 3. Effect of variation of nickel nitrate concentration on the diameter of nickel and nickel oxide nanoparticles.\\n\\nMetal ion Diameter of nickel Diameter of nickel oxide\\nconcentration nanoparticles (nm) nanoparticles (nm)\\n(% w/v) ±2 nm Remarks ±2 nm Remarks\\n\\n0.01 10 Precipitated in 20 min 11 Stable\\n0.05 09 Stable 41 Stable\\n0.10 08 Stable 08 Stable\\n0.50 06 Stable 08 Stable\\n1.00 06 Stable 07 Stable\\n2.00 30 Stable 27 Stable\\n4.00 32 Precipitated 33 Precipitated\\n8.00 33 Precipitated 36 Precipitated\\n\\nConditions: concentration of N2H4 (aq.sol.) is 5% w/v, set volume is 25 ml, surfactant concentration is\\n0.1 M, co-surfactant used n-hexanol\\n\\n0\\n\\n20\\n\\n40\\n\\n60\\n\\n80\\n\\n100\\n\\n120\\n\\n0 1 2 3 4 5 6 7 8 9 10 11 12\\n\\nNickel nanopar�cles\\n\\nNickel oxide nanopar�cles\\n\\nVaria�on of Wo\\n\\nDi\\nam\\n\\net\\ner\\n\\n (n\\nm\\n\\n)\\n\\nFigure 1. Effect of variation of Wo on the diameter of nickel and\\nnickel oxide nanoparticles.\\n\\n(greater diameter). Additionally, this also shows that the\\nparticle diameter depends on the initial size of droplets of the\\nemulsion, (microreactor effect).\\n\\nWith increasing Wo value, the larger water pool provides\\na larger space for nanoparticles to grow. This is primarily\\nresponsible for the corresponding synthesis of larger particles.\\nNo investigation was performed when the Wo was above\\n11 because the microemulsion solution became extremely\\nunstable and phase separation occurred.\\n\\n3.2. Effect of concentration of reducing agent on particle size\\n\\nThe effect of the concentration of reducing agent on the\\nsize of nickel and nickel oxide nanoparticles has also been\\n\\nFigure 2. Effect of variation of reducing agent concentration on the\\ndiameter of nickel and nickel oxide nanoparticles.\\n\\nFigure 3. Effect of variation of nickel nitrate concentration on the\\ndiameter of nickel and nickel oxide nanoparticles.\\n\\n4\\n\\n\\n\\nAdv. Nat. Sci.: Nanosci. Nanotechnol. 4 (2013) 025009 Ajeet Kumar et al\\n\\nFigure 4. TEM micrograph of (a)–(d) nickel nanoparticles and (e)–(h) nickel oxide nanoparticles.\\n\\ninvestigated in the microemulsion system of water/TX-100/\\ncyclohexane/n-hexanol at a Wo value of 5, keeping the\\nconcentration of the metal ion constant at 2% w/v. The results\\nhave been tabulated in table 2 and presented in figure 2. It can\\nbe seen that the average diameters of nanoparticles decreased\\nwith the increase of the concentration of the reducing agent\\nand before reaching a constant value.\\n\\nThe size and stability of nanoparticles mainly depends\\nupon the nucleation and the subsequent growth rate to form\\n\\nstable nanoparticles. In the beginning, a minimal number of\\natoms get together to form a stable nucleus. Subsequently,\\ncollisions between these atoms lead to the assembly of stable\\nnanoparticles. Hence, one can assume that once the nuclei are\\nformed, the growth process supersedes the nucleation.\\n\\nWe observed that the nanoparticles were mono-dispersed.\\nThis was probably because the nuclei were all formed together\\nand grew at the same rate. Hence one can conclude that it is\\nthe number of nuclei formed at the beginning of the reduction\\n\\n5\\n\\n\\n\\nAdv. Nat. Sci.: Nanosci. Nanotechnol. 4 (2013) 025009 Ajeet Kumar et al\\n\\nFigure 5. X-ray diffraction of (a) nickel nanoparticles and (b) nickel oxide nanoparticles.\\n\\n(a) (b)\\n\\nFigure 6. Electron diffraction of nickel nanoparticles (a) and nickel oxide nanoparticles (b).\\n\\nthat essentially determines the number and size of the resultant\\nparticles.\\n\\n3.3. Effect of concentration of metal ion on particle size\\n\\nThe effect of the concentration of nickel nitrate on the\\nsize of nickel/ nickel oxide nanoparticles has also been\\ninvestigated in the microemulsion system of water/TX-100/\\ncyclohexane/n-hexanol at a Wo value of 5, keeping the\\nconcentration of the reducing agent constant (2% w/v). The\\nresults are tabulated in table 3 and shown in figure 3. It can\\nbe seen that the effect of the concentration of the metal ions\\nfollows an ‘S’-shaped curve. This means that on increasing\\nthe concentration of the metal ions, the average diameter of\\nthe nanoparticles so formed initially decreases, approaches a\\nconstant value and then reaches a final stabilized value.\\n\\nWe observed that the size of the nanoparticles did not\\nchange initially with the concentration of the metal ion.\\nHowever, when the concentration of the nickel nitrate was\\nabove 1% w/v, the average diameters of nickel nanoparticles\\nincreased significantly. The initial concentration of the\\nreducing agent was very high with respect to the metal ion\\nconcentration and this led to the formation of small and\\nunstable nuclei. With increasing concentration of the metal\\nions, the number of nuclei formed at the very beginning of the\\nreduction remained constant. The atoms formed at the latter\\nperiod were used for the growth of particles. This resulted in\\nthe formation of larger and stable particles.\\n\\n4. Characterization of nickel and nickel oxide\\nnanoparticles\\n\\n4.1. Transmission electron microscopic (TEM) analysis\\n\\nIn order to prepare the samples for TEM analysis,\\napproximately 5 mg of the dry nanoparticles were dispersed\\nin 25 ml of ethanol using an ultrasonicator and this yielded\\na clear dispersed solution of the nanoparticles. 10 µl of\\nthis nanoparticle dispersion was put on a formvar coated\\ncopper grid (1% solution of formvar was prepared in\\nspectroscopic grade chloroform) and air-dried in a vacuum\\ndesiccator. The dried grid was then examined under an\\nelectron microscope (TEM TECHNAI 300 KV, Ultra twin FEI\\nwith EDAX transmission electron microscope operating at\\n300 kV).\\n\\nFigures 4(a)–(d) and (e)–(h) show the TEM images\\nof nickel nanoparticles and nickel oxide nanoparticles,\\nrespectively, at an optimized parameter. The resulting\\nspherical nanoparticles were monodispersed and this confirms\\nthat all the nuclei were formed almost at the same time and\\ngrew at the same rate. The results obtained were analyzed\\nand it was found that they were in accordance with the\\nresults obtained from QELS and there were no signs of\\nagglomeration. The particles were uniform in shape and well\\ndispersed. The particle size ranged from 8 to 100 nm.\\n\\n6\\n\\n\\n\\nAdv. Nat. Sci.: Nanosci. Nanotechnol. 4 (2013) 025009 Ajeet Kumar et al\\n\\nElement Weight % Atomic % Net inte. Backgrd. Inte. error P/B\\n\\nNiK 80.9 82.1 1109.15 16.17 2.17 68.59\\nCuK 19.1 17.9 238.50 12.13 4.38 19.67\\nTotal 100 100\\n\\nElement Weight % Atomic % Net inte. Backgrd. Inte. error P/B\\n\\nOK 18.1 44.7 31.32 8.86 8.89 3.54\\nNiLK 81.9 55.3 126.56 11.39 3.84 11.11\\nTotal 100 100\\n\\nFigure 7. EDX analysis of (a) nickel nanoparticles and (b) nickel oxide nanoparticles.\\n\\n4.2. X-ray diffraction (XRD) analysis\\n\\nX-ray diffractogram of the nickel and nickel oxide\\nnanoparticles prepared are shown in figure 5 with the\\n2θ values lying between 30 and 80◦. The XRD of the\\nnanoparticles shows some broad and low intensity peaks.\\nThis is because the sample is mainly amorphous and/or\\nalso because the crystal domains are very small and less\\nthan 50 nm. We also observed peaks corresponding to\\nface-centered cubic (fcc) nickel and nickel oxide. The XRD\\npattern of nickel nanoparticles, as in figure 5(a), shows two\\ncharacteristic peaks at 2θ values of 44.7◦, 52.6◦ corresponding\\n\\nto the marked indices of (111) and (200), respectively. These\\ncharacteristic peaks indicate the formation of an fcc nickel\\nphase without significant oxides or other impurity phases.\\n\\nFigure 5(b) shows the XRD patterns for nickel oxide\\nsynthesized using hydrazine hydrate in air. The characteristic\\npeaks occur at 2θ values of 37.60◦, 43.65◦ and 63.20◦\\n\\ncorresponding to the marked indices of (111), (200) and\\n(220), respectively. The average primary particle size of the\\nnickel and nickel oxide nanoparticles was calculated from the\\nfull-width at half-maximum (FWHM) of the (111) peaks in\\nthe XRD patterns using the Scherrer equation. This results\\n\\n7\\n\\n\\n\\nAdv. Nat. Sci.: Nanosci. Nanotechnol. 4 (2013) 025009 Ajeet Kumar et al\\n\\nin an average primary particle size of about 29 and 33 nm,\\nrespectively.\\n\\n4.3. Selected area electron diffraction (SAED) analysis\\n\\nFigure 6 shows the areas for the electron diffraction pattern\\nof the resultant nanoparticles. We observed three main fringe\\npatterns with their radii in the ratio of 0.20:1.75 Å which are\\nclose to the interplanar spacing values for Ni nanoparticles\\nto the (111) and (200) planes. This could be because the\\nresultant particles were purely metallic nickel with an fcc\\nstructure.\\n\\nThe corresponding electron diffraction pattern\\n(figure 6(a)) shows sharp rings with plane distances of\\n2.99, 2.47, 2.12, 1.51, and 1.28 Å. This indicates the\\nd-spacing for pure cubic nickel oxide. Figure 6(b) shows the\\nelectron diffraction pattern for nickel oxide nanoparticles.\\nThe spacing of 0.21 nm corresponds to the nickel oxide (200)\\nplanes and matches with the reported pattern shown by other\\nresearch groups [34, 35].\\n\\n4.4. Energy dispersive x-ray (EDX) analysis\\n\\nThe EDX was performed for the synthesized nickel and nickel\\noxide nanoparticles on various regions given in figures 7(a)\\nand (b), respectively, and confirmed that the presence of\\nrespective nanoparticles as the elementary component with\\nenergy bands centered on 7.5 and 0.8 keV corresponds to K\\nand L lines, respectively. Figure 7(b) shows the presence of\\noxygen with nickel as elementary components which confirm\\nthe formation of nickel oxide nanoparticles.\\n\\n5. Catalytic activity\\n\\nNickel and nickel oxide nanoparticles synthesized by our\\nprotocol have been used by our group for catalyzing various\\nchemical reactions such as the chemo-selective Knoevanagel\\ncondensation [12], synthesis of quinoxalines [1] and selective\\nprotection of carbonyl compounds [36]. These reactions show\\nthat the highly stable, monodispersed nanoparticles retain\\ntheir catalytic activity up to five reaction cycles and are\\nchemo-selective in nature. We have used our nanoparticles for\\nsynthesizing a number of different reactions [37–46].\\n\\n6. Conclusions\\n\\nThe present study illustrates a novel, simple and convenient\\nmethod for the synthesis of nickel and nickel oxide\\nnanoparticles through the reduction of nickel salts in the water\\npool of reverse microemulsion. In the thermodynamically\\nstable reverse microemulsion, nickel nanoparticles could be\\nprepared by the reduction of Ni(NO3)2 with alkaline NaBH4\\n\\nand nickel oxide nanoparticles could be synthesized using\\nhydrazine hydrate as a reducing agent. Nanoparticles formed\\nwere characterized using advanced sophisticated techniques\\nsuch as QELS, TEM, XRD, electron diffraction and EDX\\nwhich confirmed that particles formed were spherical in shape\\nand pure. The size of the nanoparticles can be easily controlled\\nby changing the molar ratio of water to surfactant or by\\naltering the concentration of the reducing agent. The method\\n\\nillustrates a simple way of preparing nickel and nickel oxide\\nnanoparticles.\\n\\nAcknowledgments\\n\\nThe authors (SM, SA and AK) acknowledge the financial\\nsupport from the Department of Science and Technology\\n(DST), Council of Scientific Industrial Research (CSIR), and\\nUniversity Grant Commission (UGC), Government of India.\\n\\nReferences\\n\\n[1] Kumar A, Kumar S, Saxena A, De A and Mozumdar S 2008\\nCatal. Commun. 9 778\\n\\n[2] Knecht M R, Garcia-Martinez J C and Crooks R M 2006\\nChem. Mater. 18 5039\\n\\n[3] Patel J D, Canar O and Jones J 2007 Pharm. Res. 24 343\\n[4] Tan L H, Li F S and Liu L L 2003 Mater. Rev. 17 41\\n[5] Chen R Y and Zhou K G 2006 Trans. Nonferr. Met. Soc. China\\n\\n16 1223\\n[6] Vladimir V B, Valentin N M and Nikolay V G 2005 Chem.\\n\\nVapor Depos. 11 368\\n[7] Qin Z P, Guo H X, Li D S and Shi X P 2004 J. Funct. Mater.\\n\\nDev. 10 95\\n[8] He Y Q, Li X G and Swihart M T 2005 Chem. Mater. 17 1017\\n[9] Liu Z, Li S, Yang Y, Peng S, Hu Z and Qian Y 2003 Adv.\\n\\nMater. 15 1946\\n[10] Tang Z X, Sorensen C M, Klabunde K J and Hadjipanayis G C\\n\\n1991 J. Colloid Interface Sci. 146 38\\n[11] Fegley B J, White P and Bowen H K 1985 Am. Ceram. Soc.\\n\\nBull. 64 1115\\n[12] Kumar A, Dewan M, Saxena A, De A and Mozumdar S 2010\\n\\nCatal. Commun. 11 679\\n[13] Gao B J, Gao J F, Zhou J Q and Chin J 2001 Inorg. Chem.\\n\\n17 491\\n[14] Chen D H and Wu S H 2000 Chem. Mater. 12 1354\\n[15] Hou Y and Gao S 2003 J. Mater. Chem. 13 1510\\n[16] Green M and O’Brien P 2001 Chem. Commun. 19 1912\\n[17] Chen D H and Hsieh C H 2002 J. Mater. Chem. 12 2412\\n[18] Chen L, Chen J, Zhou H, Zhang D and Wan H 2007 Mater.\\n\\nSci. Eng. A 262 452\\n[19] Jeon Y T, Moon J Y, Lee G H, Park J and Chang Y 2006\\n\\nJ. Phys. Chem. B 110 1187\\n[20] Jeon Y, Lee G H, Park J, Kim B and Chang Y 2005 J. Phys.\\n\\nChem. B 109 12257\\n[21] Murray C B, Sun S, Doyle H and Betley T 2001 MRS Bull.\\n\\n26 985\\n[22] Toneguzzo P, Viau G, Acher O, Guillet F, Bruneton E and\\n\\nFievet V F 2000 J. Mater. Sci. 35 3767\\n[23] Hinotsu T, Jeyadevan B, Chinnasamy C N, Shinoda K and\\n\\nTohji K 2004 J. Appl. Phys. 95 7477\\n[24] Chinnasamy C N, Jeyadevan B, Shinoda K, Tohji K,\\n\\nNarayanasamy A, Sato K and Hisano S 2005 J. Appl. Phys.\\n97 10J309\\n\\n[25] Tzitzios V, Basina G, Gjoka M, Alexandrakis V, Georgakilas\\nV, Niarchos D, Boukos N and Petridis D 2006\\nNanotechnology 17 3750\\n\\n[26] Park J W, Chae E H, Kim S H, Lee J H, Kim J W and Yoon\\nS M 2006 Mater. Chem. Phys. 97 371\\n\\n[27] Goh C F, Yu H, Yong S S, Mhaisalkar S G, Boey F Y C and\\nTeo P S 2005 Mater. Sci. Eng. B 117 153\\n\\n[28] Jiang C, Zou G, Zhang W, Yu W and Qian Y 2006 Mater. Lett.\\n60 2319\\n\\n[29] Luisi P L and Magid L J 1986 Crit. Rev. Biochem. 20 409\\n[30] Pileni M P (ed) 1989 Structure and Reactivity in Reverse\\n\\nMicelles (Amsterdam: Elsevier)\\n[31] Paul B K and Moulik S P J 1997 Dispers. Sci. Technol.\\n\\n18 301\\n\\n8\\n\\nhttp://dx.doi.org/10.1016/j.catcom.2007.08.021\\nhttp://dx.doi.org/10.1021/cm061272p\\nhttp://dx.doi.org/10.1007/s11095-006-9154-7\\nhttp://dx.doi.org/10.1016/S1003-6326(06)60405-6\\nhttp://dx.doi.org/10.1002/cvde.200506376\\nhttp://dx.doi.org/10.1021/cm048128t\\nhttp://dx.doi.org/10.1002/adma.200305663\\nhttp://dx.doi.org/10.1016/0021-9797(91)90004-R\\nhttp://dx.doi.org/10.1016/j.catcom.2010.01.017\\nhttp://dx.doi.org/10.1021/cm991167y\\nhttp://dx.doi.org/10.1039/b303226d\\nhttp://dx.doi.org/10.1039/b107108b\\nhttp://dx.doi.org/10.1039/b200603k\\nhttp://dx.doi.org/10.1021/jp054608b\\nhttp://dx.doi.org/10.1021/jp050489o\\nhttp://dx.doi.org/10.1557/mrs2001.254\\nhttp://dx.doi.org/10.1023/A:1004864927169\\nhttp://dx.doi.org/10.1063/1.1688534\\nhttp://dx.doi.org/10.1063/1.1851951\\nhttp://dx.doi.org/10.1088/0957-4484/17/15/023\\nhttp://dx.doi.org/10.1016/j.matchemphys.2005.08.028\\nhttp://dx.doi.org/10.1016/j.mseb.2004.11.007\\nhttp://dx.doi.org/10.1016/j.matlet.2005.12.133\\nhttp://dx.doi.org/10.3109/10409238609081999\\nhttp://dx.doi.org/10.1080/01932699708943740\\n\\n\\nAdv. Nat. Sci.: Nanosci. Nanotechnol. 4 (2013) 025009 Ajeet Kumar et al\\n\\n[32] La M V K and Dinegar R H 1950 J. Am. Chem. Soc. 72 4847\\n[33] Liu Q, Liu H J, Liang Y Y, Xu Z and Yin G 2006 Mater. Res.\\n\\nBull. 41 697\\n[34] Phung X, Groza J, Stach E A, Williams L N and Ritchey S B\\n\\n2003 Mater. Eng. A 359 261\\n[35] Chen Y, Peng D L, Lin D and Luo X 2007 Nanotechnology\\n\\n18 505703\\n[36] Kumar A, Kumar S, Saxena A, De A and Mozumdar S 2008\\n\\nCatal. Lett. 122 98\\n[37] Kumar A, Aerry S, Saxena A, De A and Mozumdar S 2012\\n\\nGreen Chem. 14 1298\\n[38] Dewan M, Kumar A, Saxena A, De A and Mozumdar S 2012\\n\\nPLoS ONE 7 e29131\\n[39] Dewan M, Kumar A, Saxena A, De A and Mozumdar S 2010\\n\\nTet. Lett. 51 6108\\n\\n[40] Dewan M, Kumar A, Saxena A, De A and Mozumdar S 2012\\nPLoS ONE 7 e43078\\n\\n[41] Kumar A, Dewan M, Saxena A, De A and Mozumdar S 2011\\nTet. Lett. 52 4835\\n\\n[42] Kumar A, Singh P, Saxena A, De A and Mozumdar S 2008\\nCatal. Commun. 10 17\\n\\n[43] Kumar A, Dewan M, Saxena A, De A and Mozumdar S 2013\\nRSC Adv. 3 603\\n\\n[44] Aerry S, De A, Kumar A, Saxena A, Majumdar D K and\\nMozumdar S 2012 J. Biomed. Mater. Res. A doi:\\n10.1002/jbm.a.34476\\n\\n[45] Aerry S, Kumar A, Saxena A, De A and Mozumdar S 2013\\nGreen Chem. Lett. Rev. 6 183–8\\n\\n[46] Kumar A, Saxena A, De A, Shankar R and Mozumdar S 2013\\nRSC Adv. 15 5015–21\\n\\n9\\n\\nhttp://dx.doi.org/10.1021/ja01167a001\\nhttp://dx.doi.org/10.1016/j.materresbull.2005.10.013\\nhttp://dx.doi.org/10.1016/S0921-5093(03)00348-4\\nhttp://dx.doi.org/10.1088/0957-4484/18/50/505703\\nhttp://dx.doi.org/10.1007/s10562-007-9349-5\\nhttp://dx.doi.org/10.1039/c2gc35070j\\nhttp://dx.doi.org/10.1371/journal.pone.0029131\\nhttp://dx.doi.org/10.1016/j.tetlet.2010.09.052\\nhttp://dx.doi.org/10.1371/journal.pone.0043078\\nhttp://dx.doi.org/10.1016/j.tetlet.2011.07.016\\nhttp://dx.doi.org/10.1016/j.catcom.2008.07.030\\nhttp://dx.doi.org/10.1039/c2ra22522k\\nhttp://dx.doi.org/10.1002/jbm.a.34476\\nhttp://dx.doi.org/10.1080/17518253.2012.737029\\nhttp://dx.doi.org/10.1039/C3RA23455J\\n\\n\\t1. Introduction\\n\\t2. Material and methods\\n\\t2.1. Preparation of nickel and nickel oxide nanoparticles\\n\\n\\t3. Results and discussion\\n\\t3.1. Influence of Wo over particle size\\n\\t3.2. Effect of concentration of reducing agent on particle size\\n\\t3.3. Effect of concentration of metal ion on particle size\\n\\n\\t4. Characterization of nickel and nickel oxide nanoparticles\\n\\t4.1. Transmission electron microscopic (TEM) analysis\\n\\t4.2. X-ray diffraction (XRD) analysis\\n\\t4.3. Selected area electron diffraction (SAED) analysis\\n\\t4.4. Energy dispersive x-ray (EDX) analysis\\n\\n\\t5. Catalytic activity\\n\\t6. Conclusions\\n\\tAcknowledgments\\n\\tReferences\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 64
        }
      ],
      "source": [
        "df.loc[0,'Text']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17e4b472",
      "metadata": {
        "id": "17e4b472"
      },
      "outputs": [],
      "source": [
        "df.to_csv('/IK_rr_DataFrame.csv', index = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e93d448f",
      "metadata": {
        "id": "e93d448f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66d9d64f-a4eb-4f92-9ab1-c020dc3a77ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/IK_rr_DataFrame.csv\n"
          ]
        }
      ],
      "source": [
        "!ls /IK_rr_DataFrame.csv"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}