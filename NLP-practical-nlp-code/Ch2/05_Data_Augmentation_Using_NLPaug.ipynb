{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yavI9mt4gayF"
   },
   "source": [
    "## Data Augmentation using NLPaug\n",
    "\n",
    "This notebook demostrate the usage of a character augmenter, word augmenter. There are other types such as augmentation for sentences, audio, spectrogram inputs etc. All of the types many before mentioned types and many more can be found at the [github repo](https://github.com/makcedward/nlpaug) and [docs](https://nlpaug.readthedocs.io/en/latest/) of nlpaug."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-03T11:15:03.818048Z",
     "start_time": "2021-04-03T11:15:01.468101Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cF5zJdr-kAPY",
    "outputId": "3e4f51e9-151e-4de0-bf29-17056be5ddf3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy==1.19.5 in /usr/local/lib/python3.7/dist-packages (1.19.5)\n",
      "Collecting nlpaug==0.0.14\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1f/6c/ca85b6bd29926561229e8c9f677c36c65db9ef1947bfc175e6641bc82ace/nlpaug-0.0.14-py3-none-any.whl (101kB)\n",
      "\u001b[K     |████████████████████████████████| 102kB 4.4MB/s \n",
      "\u001b[?25hInstalling collected packages: nlpaug\n",
      "Successfully installed nlpaug-0.0.14\n",
      "Collecting wget==3.2\n",
      "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
      "Building wheels for collected packages: wget\n",
      "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for wget: filename=wget-3.2-cp37-none-any.whl size=9675 sha256=53699b93bdf14ec28540dbe53e9978af5099ce729b974b2c43ec5f5f12e13c59\n",
      "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
      "Successfully built wget\n",
      "Installing collected packages: wget\n",
      "Successfully installed wget-3.2\n",
      "Requirement already satisfied: matplotlib==3.2.2 in /usr/local/lib/python3.7/dist-packages (3.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.2.2) (2.8.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.2.2) (2.4.7)\n",
      "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.2.2) (1.19.5)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.2.2) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.2.2) (0.10.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib==3.2.2) (1.15.0)\n",
      "Requirement already satisfied: requests==2.23.0 in /usr/local/lib/python3.7/dist-packages (2.23.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests==2.23.0) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests==2.23.0) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests==2.23.0) (2021.5.30)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests==2.23.0) (2.10)\n"
     ]
    }
   ],
   "source": [
    "# To install only the requirements of this notebook, uncomment the lines below and run this cell\n",
    "\n",
    "# ===========================\n",
    "\n",
    "# !pip install numpy==1.19.5\n",
    "# !pip install nlpaug==0.0.14\n",
    "# !pip install wget==3.2\n",
    "# !pip install matplotlib==3.2.2\n",
    "# !pip install requests==2.23.0\n",
    "\n",
    "# ==========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "XReg1SQkrXUR"
   },
   "outputs": [],
   "source": [
    "# To install the requirements for the entire chapter, uncomment the lines below and run this cell\n",
    "\n",
    "# ===========================\n",
    "\n",
    "# try :\n",
    "#     import google.colab\n",
    "#     !curl https://raw.githubusercontent.com/practical-nlp/practical-nlp/master/Ch2/ch2-requirements.txt | xargs -n 1 -L 1 pip install\n",
    "# except ModuleNotFoundError :\n",
    "#     !pip install -r \"ch2-requirements.txt\"\n",
    "\n",
    "# ==========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-03T11:15:11.595619Z",
     "start_time": "2021-04-03T11:15:11.593618Z"
    },
    "id": "8yhkOl3cgZ28"
   },
   "outputs": [],
   "source": [
    "# This will be the base text which we will be using throughout this notebook\n",
    "text=\"The quick brown fox jumps over the lazy dog .\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-03T11:15:15.458928Z",
     "start_time": "2021-04-03T11:15:12.067195Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ekFhzIWHUmoj",
    "outputId": "259fdc96-6559-420f-9772-349462187764"
   },
   "outputs": [],
   "source": [
    "import nlpaug.augmenter.char as nac\n",
    "import nlpaug.augmenter.word as naw\n",
    "import nlpaug.augmenter.sentence as nas\n",
    "import nlpaug.flow as nafc\n",
    "\n",
    "from nlpaug.util import Action\n",
    "import os\n",
    "# !git clone https://github.com/makcedward/nlpaug.git\n",
    "os.environ[\"MODEL_DIR\"] = 'nlpaug/model/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Xo3CzNhh-zU"
   },
   "source": [
    "### Augmentation at the Character Level\n",
    "\n",
    "\n",
    "1.   OCR Augmenter: To read textual data from on image, we need an OCR(optical character recognition) model. Once the text is extracted from the image, there may be errors like; '0' instead of an 'o', '2' instead of 'z' and other such similar errors.  \n",
    "2.   Keyboard Augmenter: While typing/texting typos are fairly common this augmenter simulates the errors by substituting characters in words with ones at a similar distance on a keyboard.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-03T11:15:15.474943Z",
     "start_time": "2021-04-03T11:15:15.459929Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lfAaokTmjzak",
    "outputId": "32ae0735-9724-4c73-b5ae-6c58a3ce86bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "The quick brown fox jumps over the lazy dog .\n",
      "Augmented Texts:\n",
      "['The 9oick brown fox jumps over the 1a2y d09.', 'The quick bk0wn fox jumps over the lazy dog.', 'The qoicr brown fox jumps uvek the lazy du9.']\n"
     ]
    }
   ],
   "source": [
    "# OCR augmenter\n",
    "# import nlpaug.augmenter.char as nac\n",
    "\n",
    "aug = nac.OcrAug()  \n",
    "augmented_texts = aug.augment(text, n=3) # specifying n=3 gives us only 3 augmented versions of the sentence.\n",
    "\n",
    "print(\"Original:\")\n",
    "print(text)\n",
    "\n",
    "print(\"Augmented Texts:\")\n",
    "print(augmented_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-03T11:15:16.903143Z",
     "start_time": "2021-04-03T11:15:16.880652Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fKQCpS35j9Ie",
    "outputId": "ad301a33-bed2-4a00-d34c-083a181c84ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "The quick brown fox jumps over the lazy dog .\n",
      "Augmented Text:\n",
      "['The 1kick brown fox j*mLs over the OaXy dog.', 'The quick brLEn fox jumps iFer the lqay dog.', 'The quick br(en fox jImpC over the Kasy dog.']\n"
     ]
    }
   ],
   "source": [
    "# Keyboard Augmenter\n",
    "# import nlpaug.augmenter.word as naw\n",
    "\n",
    "\n",
    "aug = nac.KeyboardAug()\n",
    "augmented_text = aug.augment(text, n=3) # specifying n=3 gives us only 3 augmented versions of the sentence.\n",
    "\n",
    "print(\"Original:\")\n",
    "print(text)\n",
    "\n",
    "print(\"Augmented Text:\")\n",
    "print(augmented_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XbfPMwZWmper"
   },
   "source": [
    "There are other types of character augmenters too. Their details are avaiable in the links mentioned at the beginning of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MufLJXsQm4i1"
   },
   "source": [
    "### Augmentation at the Word Level\n",
    "\n",
    "Augmentation is important at the word level as well , here we use word2vec to insert or substitute a similar word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tc_K1-niTGFP"
   },
   "source": [
    "**Spelling** **augmentor**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-03T11:16:58.980739Z",
     "start_time": "2021-04-03T11:16:58.532879Z"
    },
    "id": "2Qzmv4QCYrJe"
   },
   "outputs": [],
   "source": [
    "# Downloading the required txt file\n",
    "import wget\n",
    "\n",
    "if not os.path.exists(\"spelling_en.txt\"):\n",
    "    wget.download(\"https://raw.githubusercontent.com/makcedward/nlpaug/5238e0be734841b69651d2043df535d78a8cc594/nlpaug/res/word/spelling/spelling_en.txt\")\n",
    "else:\n",
    "    print(\"File already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-03T11:17:00.723918Z",
     "start_time": "2021-04-03T11:17:00.619823Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gOHrgDIill2F",
    "outputId": "e6b83876-f478-479a-c47e-7a484f8407c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "The quick brown fox jumps over the lazy dog .\n",
      "Augmented Texts:\n",
      "['The quick brown fox jumps over ther lszy doga.']\n"
     ]
    }
   ],
   "source": [
    "# Substitute word by spelling mistake words dictionary\n",
    "aug = naw.SpellingAug('spelling_en.txt')\n",
    "augmented_texts = aug.augment(text)\n",
    "print(\"Original:\")\n",
    "print(text)\n",
    "print(\"Augmented Texts:\")\n",
    "print(augmented_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eaeQOtVqTQKG"
   },
   "source": [
    "**Word embeddings augmentor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  ../glove.6B.zip\n",
      "  inflating: glove.6B.50d.txt        \n",
      "  inflating: glove.6B.100d.txt       \n",
      "  inflating: glove.6B.200d.txt       \n",
      "  inflating: glove.6B.300d.txt       \n"
     ]
    }
   ],
   "source": [
    "!unzip ../glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: ../glove: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!ls ../glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-03T11:42:53.178843Z",
     "start_time": "2021-04-03T11:42:53.163829Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jIB9bMi5KWZ5",
    "outputId": "addbe83a-7c57-4eb0-81aa-575700dcb8c9"
   },
   "outputs": [],
   "source": [
    "# import gzip\n",
    "# import shutil\n",
    "# import wget\n",
    "\n",
    "\n",
    "# import gensim.downloader as api\n",
    "# wv = api.load('word2vec-google-news-300')\n",
    "\n",
    "# gn_vec_path = \"GoogleNews-vectors-negative300.bin\"\n",
    "# print(f\"Model at {gn_vec_path}\")\n",
    "\n",
    "# if not os.path.exists(\"GoogleNews-vectors-negative300.bin\"):\n",
    "#     if not os.path.exists(\"../Ch3/GoogleNews-vectors-negative300.bin\"):\n",
    "#         # Downloading the reqired model\n",
    "#         if not os.path.exists(\"../Ch3/GoogleNews-vectors-negative300.bin.gz\"):\n",
    "#             if not os.path.exists(\"GoogleNews-vectors-negative300.bin.gz\"):\n",
    "#                 wget.download(\"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\")\n",
    "#             gn_vec_zip_path = \"GoogleNews-vectors-negative300.bin.gz\"\n",
    "#         else:\n",
    "#             gn_vec_zip_path = \"../Ch3/GoogleNews-vectors-negative300.bin.gz\"\n",
    "#         # Extracting the required model\n",
    "#         with gzip.open(gn_vec_zip_path, 'rb') as f_in:\n",
    "#             with open(gn_vec_path, 'wb') as f_out:\n",
    "#                 shutil.copyfileobj(f_in, f_out)\n",
    "#     else:\n",
    "#         gn_vec_path = \"../Ch3/\" + gn_vec_path\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jf_QHk-SgegN"
   },
   "source": [
    "Insert word randomly by word embeddings similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove/glove.6B.50d.txt\n"
     ]
    }
   ],
   "source": [
    "!ls glove/glove.6B.50d.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I went Shopping Today, and my trolly was filled with Bananas. I also had food at burgur palace\n",
      "['I went Shopping 5kday, and my tr(l/y was billrd with BXnsnaC. I xls8 had food at H8rgur palace']\n"
     ]
    }
   ],
   "source": [
    "import nlpaug.augmenter.char as nac\n",
    "\n",
    "test_sentence = 'I went Shopping Today, and my trolly was filled with Bananas. I also had food at burgur palace'\n",
    "\n",
    "aug = nac.KeyboardAug(name='Keyboard_Aug', aug_char_min=1, aug_char_max=10, aug_char_p=0.3, aug_word_p=0.3, \n",
    "                      aug_word_min=1, aug_word_max=10, stopwords=None, tokenizer=None, reverse_tokenizer=None, \n",
    "                      include_special_char=True, include_numeric=True, include_upper_case=True, lang='en', verbose=0, \n",
    "                      stopwords_regex=None, model_path=None, min_char=4)\n",
    " \n",
    "test_sentence_aug = aug.augment(test_sentence)\n",
    "print(test_sentence)\n",
    "print(test_sentence_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I went Shopping Today, and my trolly was filled with Bananas. I also had food at burgur palace\n",
      "['1 went 8huppin9 Tuday, and my trolly was filled with Eanana8. I also had food at burgur palace']\n"
     ]
    }
   ],
   "source": [
    "aug = nac.OcrAug(name='OCR_Aug', aug_char_min=1, aug_char_max=10, aug_char_p=0.3, aug_word_p=0.3, aug_word_min=1, \n",
    "                 aug_word_max=10, stopwords=None, tokenizer=None, reverse_tokenizer=None, verbose=0, stopwords_regex=None, \n",
    "                 min_char=1)\n",
    " \n",
    "test_sentence_aug = aug.augment(test_sentence)\n",
    "print(test_sentence)\n",
    "print(test_sentence_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/linghuang/nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/linghuang/nltk_data...\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/linghuang/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I went Shopping Today, and my trolly was filled with Bananas. I also had food at burgur palace\n",
      "['One went Shopping Today, and my trolly was occupy with Bananas. I besides get nutrient at burgur palace']\n"
     ]
    }
   ],
   "source": [
    "aug = naw.SynonymAug(aug_src='wordnet', model_path=None, name='Synonym_Aug', aug_min=1, aug_max=10, aug_p=0.3, lang='eng', \n",
    "                     stopwords=None, tokenizer=None, reverse_tokenizer=None, stopwords_regex=None, force_reload=False, \n",
    "                     verbose=0)\n",
    " \n",
    "test_sentence_aug = aug.augment(test_sentence)\n",
    "print(test_sentence)\n",
    "print(test_sentence_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "very beautiful\n",
      "['very ugly']\n"
     ]
    }
   ],
   "source": [
    "aug = naw.AntonymAug(name='Antonym_Aug', aug_min=1, aug_max=10, aug_p=0.3, lang='eng', stopwords=None, tokenizer=None, \n",
    "                     reverse_tokenizer=None, stopwords_regex=None, verbose=0)\n",
    " \n",
    "test_sentence_aug = aug.augment(\"very beautiful\")\n",
    "print(\"very beautiful\")\n",
    "print(test_sentence_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I went Shopping Today, and my trolly was filled with Bananas. I also had food at burgur palace\n",
      "['0I went Shoping Today, Ande my trolly has filled with Bananas. I also had Feed at burgur palce']\n"
     ]
    }
   ],
   "source": [
    "aug = naw.SpellingAug(dict_path=None, name='Spelling_Aug', aug_min=1, aug_max=10, aug_p=0.3, stopwords=None, \n",
    "                      tokenizer=None, reverse_tokenizer=None, include_reverse=True, stopwords_regex=None, verbose=0)\n",
    " \n",
    "test_sentence_aug = aug.augment(test_sentence)\n",
    "print(test_sentence)\n",
    "print(test_sentence_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I went Shopping Today, and my trolly was filled with Bananas. I also had food at burgur palace\n",
      "['I went Sh opping Tod ay, and my trol ly was fill ed wi th Bananas. I also had food at burgur pal ace']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "aug = naw.SplitAug(name='Split_Aug', aug_min=1, aug_max=10, aug_p=0.3, min_char=4, stopwords=None, tokenizer=None, \n",
    "                   reverse_tokenizer=None, stopwords_regex=None, verbose=0)\n",
    " \n",
    "test_sentence_aug = aug.augment(test_sentence)\n",
    "print(test_sentence)\n",
    "print(test_sentence_aug)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kUB3Nd4Wghd0"
   },
   "source": [
    "Substitute word by word2vec similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-03T11:44:12.948639Z",
     "start_time": "2021-04-03T11:44:12.446757Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pSeZNfQRfy2l",
    "outputId": "4b00468b-0151-405e-eae1-e6d06e442142"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "The quick brown fox jumps over the lazy dog .\n",
      "Augmented Text:\n",
      "His quick brown fox jumps morethan the whiny dog .\n"
     ]
    }
   ],
   "source": [
    "aug = naw.WordEmbsAug(\n",
    "    model_type='word2vec', model_path=gn_vec_path,\n",
    "    action=\"substitute\")\n",
    "augmented_text = aug.augment(text)\n",
    "print(\"Original:\")\n",
    "print(text)\n",
    "print(\"Augmented Text:\")\n",
    "print(augmented_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "reALNlOuDI9u"
   },
   "source": [
    "There are many more features which nlpaug offers you can visit the github repo and documentation for further details"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "05_Data_Augmentation_Using_NLPaug.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
