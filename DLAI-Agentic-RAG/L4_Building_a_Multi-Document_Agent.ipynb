{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b523e0a",
   "metadata": {},
   "source": [
    "# Lesson 4: Building a Multi-Document Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a323703",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9625ab2-71b6-4fd0-904e-42df80d3215f",
   "metadata": {
    "height": 47,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3221a474-5817-4db2-af46-e029042a75a5",
   "metadata": {
    "height": 47,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20adaa26",
   "metadata": {},
   "source": [
    "## 1. Setup an agent over 3 papers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b71ff6",
   "metadata": {},
   "source": [
    "**Note**: The pdf files are included with this lesson. To access these papers, go to the `File` menu and select`Open...`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed10a24b-d65c-4b98-a93a-94ccdb8900d0",
   "metadata": {
    "height": 200,
    "tags": []
   },
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://openreview.net/pdf?id=VtmBAGCN7o\",\n",
    "    \"https://openreview.net/pdf?id=6PmJoRfdaK\",\n",
    "    \"https://openreview.net/pdf?id=hSyW5go0v8\",\n",
    "]\n",
    "\n",
    "papers = [\n",
    "    \"metagpt.pdf\",\n",
    "    \"longlora.pdf\",\n",
    "    \"selfrag.pdf\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d8f3185-3221-4b00-bd38-41d36e4a3307",
   "metadata": {
    "height": 149,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting tools for paper: metagpt.pdf\n",
      "Getting tools for paper: longlora.pdf\n",
      "Getting tools for paper: selfrag.pdf\n"
     ]
    }
   ],
   "source": [
    "from utils import get_doc_tools\n",
    "from pathlib import Path\n",
    "\n",
    "paper_to_tools_dict = {}\n",
    "for paper in papers:\n",
    "    print(f\"Getting tools for paper: {paper}\")\n",
    "    vector_tool, summary_tool = get_doc_tools(paper, Path(paper).stem)\n",
    "    paper_to_tools_dict[paper] = [vector_tool, summary_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e541bdd-14e1-41b6-81b5-b1bfda078d07",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "initial_tools = [t for paper in papers for t in paper_to_tools_dict[paper]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bff58c52",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f2c6a9f",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(initial_tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a124a438-5609-402e-8642-69d1088cb9ad",
   "metadata": {
    "height": 166,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.core.agent import AgentRunner\n",
    "\n",
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    initial_tools, \n",
    "    llm=llm, \n",
    "    verbose=True\n",
    ")\n",
    "agent = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17409d4c-05a9-4bf4-b74f-75135fa3cb6b",
   "metadata": {
    "height": 81,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Tell me about the evaluation dataset used in LongLoRA, and then tell me about the evaluation results\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_longlora with args: {\"input\": \"evaluation dataset used in LongLoRA\"}\n",
      "=== Function Output ===\n",
      "The evaluation dataset used in LongLoRA includes the RedPajama dataset, the PG19 dataset, the PG19 validation set, the PG19 test split, the LongAlpaca-12k dataset, and the SAFECONV dataset.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_longlora with args: {\"input\": \"evaluation results of LongLoRA\"}\n",
      "=== Function Output ===\n",
      "The evaluation results of LongLoRA indicate that it achieves comparable or even superior performance compared to other Llama2-based long-context models like Vicuna and LongChat. It demonstrates efficiency in terms of training hours and GPU memory cost, requiring significantly lower computational overhead and fewer training hours. Additionally, LongLoRA is effective in providing structured and concise answers, particularly excelling in producing such answers when given detailed context from long documents.\n",
      "=== LLM Response ===\n",
      "The evaluation dataset used in LongLoRA includes the RedPajama dataset, the PG19 dataset, the PG19 validation set, the PG19 test split, the LongAlpaca-12k dataset, and the SAFECONV dataset. \n",
      "\n",
      "In terms of evaluation results, LongLoRA achieves comparable or superior performance compared to other Llama2-based long-context models like Vicuna and LongChat. It demonstrates efficiency in terms of training hours and GPU memory cost, requiring significantly lower computational overhead and fewer training hours. LongLoRA is effective in providing structured and concise answers, particularly excelling in producing such answers when given detailed context from long documents.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\n",
    "    \"Tell me about the evaluation dataset used in LongLoRA, \"\n",
    "    \"and then tell me about the evaluation results\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ace340b1-761f-4058-be41-68cf131541e4",
   "metadata": {
    "height": 47,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Give me a summary of both Self-RAG and LongLoRA\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_selfrag with args: {\"input\": \"Self-RAG\"}\n",
      "=== Function Output ===\n",
      "Self-RAG is a framework that enhances the quality and factuality of a large language model through retrieval and self-reflection. It trains a single arbitrary language model to adaptively retrieve passages on-demand, generate text informed by these passages, and reflect on both the retrieved passages and its own generations using special tokens called reflection tokens. This framework aims to improve the generation quality and factuality of the language model without compromising its original creativity and versatility.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_longlora with args: {\"input\": \"LongLoRA\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 0.4290864797568116 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-LXJdh8mBCdaeYl79io2xzzqI on tokens per min (TPM): Limit 80000, Used 77288, Requested 3393. Please try again in 510ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 0.08311059922243091 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-LXJdh8mBCdaeYl79io2xzzqI on tokens per min (TPM): Limit 80000, Used 77283, Requested 3410. Please try again in 519ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 0.3044450848238208 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-LXJdh8mBCdaeYl79io2xzzqI on tokens per min (TPM): Limit 80000, Used 77109, Requested 3586. Please try again in 521ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.9257209617094435 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-LXJdh8mBCdaeYl79io2xzzqI on tokens per min (TPM): Limit 80000, Used 76900, Requested 3586. Please try again in 364ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Function Output ===\n",
      "LongLoRA is an efficient method for extending the context length of Large Language Models (LLMs) while minimizing computational costs. It combines shifted sparse attention (S2-Attn) with LoRA to achieve this extension, allowing for significant computation savings compared to full fine-tuning. LongLoRA retains the original model architectures and is compatible with existing techniques like Flash-Attention2. Additionally, an improved version called LongLoRA+ enables better performance by allowing the normalization and embedding layers to be trainable during the adaptation process. This method has shown strong empirical results on various tasks and models of different sizes, demonstrating its effectiveness in adapting LLMs to longer context lengths.\n",
      "=== LLM Response ===\n",
      "Here are summaries of Self-RAG and LongLoRA:\n",
      "\n",
      "1. Self-RAG: Self-RAG is a framework that enhances the quality and factuality of a large language model through retrieval and self-reflection. It trains a single arbitrary language model to adaptively retrieve passages on-demand, generate text informed by these passages, and reflect on both the retrieved passages and its own generations using special tokens called reflection tokens. This framework aims to improve the generation quality and factuality of the language model without compromising its original creativity and versatility.\n",
      "\n",
      "2. LongLoRA: LongLoRA is an efficient method for extending the context length of Large Language Models (LLMs) while minimizing computational costs. It combines shifted sparse attention (S2-Attn) with LoRA to achieve this extension, allowing for significant computation savings compared to full fine-tuning. LongLoRA retains the original model architectures and is compatible with existing techniques like Flash-Attention2. Additionally, an improved version called LongLoRA+ enables better performance by allowing the normalization and embedding layers to be trainable during the adaptation process. This method has shown strong empirical results on various tasks and models of different sizes, demonstrating its effectiveness in adapting LLMs to longer context lengths.\n",
      "Here are summaries of Self-RAG and LongLoRA:\n",
      "\n",
      "1. Self-RAG: Self-RAG is a framework that enhances the quality and factuality of a large language model through retrieval and self-reflection. It trains a single arbitrary language model to adaptively retrieve passages on-demand, generate text informed by these passages, and reflect on both the retrieved passages and its own generations using special tokens called reflection tokens. This framework aims to improve the generation quality and factuality of the language model without compromising its original creativity and versatility.\n",
      "\n",
      "2. LongLoRA: LongLoRA is an efficient method for extending the context length of Large Language Models (LLMs) while minimizing computational costs. It combines shifted sparse attention (S2-Attn) with LoRA to achieve this extension, allowing for significant computation savings compared to full fine-tuning. LongLoRA retains the original model architectures and is compatible with existing techniques like Flash-Attention2. Additionally, an improved version called LongLoRA+ enables better performance by allowing the normalization and embedding layers to be trainable during the adaptation process. This method has shown strong empirical results on various tasks and models of different sizes, demonstrating its effectiveness in adapting LLMs to longer context lengths.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\"Give me a summary of both Self-RAG and LongLoRA\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eede70c",
   "metadata": {},
   "source": [
    "## 2. Setup an agent over 11 papers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18771e69",
   "metadata": {},
   "source": [
    "### Download 11 ICLR papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60d01d2c-547f-4054-b0fe-ed9b1a9cc3b5",
   "metadata": {
    "height": 472,
    "tags": []
   },
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://openreview.net/pdf?id=VtmBAGCN7o\",\n",
    "    \"https://openreview.net/pdf?id=6PmJoRfdaK\",\n",
    "    \"https://openreview.net/pdf?id=LzPWWPAdY4\",\n",
    "    \"https://openreview.net/pdf?id=VTF8yNQM66\",\n",
    "    \"https://openreview.net/pdf?id=hSyW5go0v8\",\n",
    "    \"https://openreview.net/pdf?id=9WD9KwssyT\",\n",
    "    \"https://openreview.net/pdf?id=yV6fD7LYkF\",\n",
    "    \"https://openreview.net/pdf?id=hnrB5YHoYu\",\n",
    "    \"https://openreview.net/pdf?id=WbWtOYIzIK\",\n",
    "    \"https://openreview.net/pdf?id=c5pwL0Soay\",\n",
    "    \"https://openreview.net/pdf?id=TpD2aG1h0D\"\n",
    "]\n",
    "\n",
    "papers = [\n",
    "    \"metagpt.pdf\",\n",
    "    \"longlora.pdf\",\n",
    "    \"loftq.pdf\",\n",
    "    \"swebench.pdf\",\n",
    "    \"selfrag.pdf\",\n",
    "    \"zipformer.pdf\",\n",
    "    \"values.pdf\",\n",
    "    \"finetune_fair_diffusion.pdf\",\n",
    "    \"knowledge_card.pdf\",\n",
    "    \"metra.pdf\",\n",
    "    \"vr_mcl.pdf\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77426cb",
   "metadata": {
    "tags": []
   },
   "source": [
    "To download these papers, below is the needed code:\n",
    "\n",
    "\n",
    "    #for url, paper in zip(urls, papers):\n",
    "         #!wget \"{url}\" -O \"{paper}\"\n",
    "    \n",
    "    \n",
    "**Note**: The pdf files are included with this lesson. To access these papers, go to the `File` menu and select`Open...`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea5ee34d-02ac-4537-ae20-7ef6c5767172",
   "metadata": {
    "height": 149,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting tools for paper: metagpt.pdf\n",
      "Getting tools for paper: longlora.pdf\n",
      "Getting tools for paper: loftq.pdf\n",
      "Getting tools for paper: swebench.pdf\n",
      "Getting tools for paper: selfrag.pdf\n",
      "Getting tools for paper: zipformer.pdf\n",
      "Getting tools for paper: values.pdf\n",
      "Getting tools for paper: finetune_fair_diffusion.pdf\n",
      "Getting tools for paper: knowledge_card.pdf\n",
      "Getting tools for paper: metra.pdf\n",
      "Getting tools for paper: vr_mcl.pdf\n"
     ]
    }
   ],
   "source": [
    "from utils import get_doc_tools\n",
    "from pathlib import Path\n",
    "\n",
    "paper_to_tools_dict = {}\n",
    "for paper in papers:\n",
    "    print(f\"Getting tools for paper: {paper}\")\n",
    "    vector_tool, summary_tool = get_doc_tools(paper, Path(paper).stem)\n",
    "    paper_to_tools_dict[paper] = [vector_tool, summary_tool]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e35d52c",
   "metadata": {},
   "source": [
    "### Extend the Agent with Tool Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "20154923-873e-4941-9a3a-4926ab5f9b8c",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_tools = [t for paper in papers for t in paper_to_tools_dict[paper]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "671582f9-70d7-4a8f-b813-58b2a068ca72",
   "metadata": {
    "height": 149,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define an \"object\" index and retriever over these tools\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.objects import ObjectIndex\n",
    "\n",
    "obj_index = ObjectIndex.from_objects(\n",
    "    all_tools,\n",
    "    index_cls=VectorStoreIndex,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c3929882-e9dc-46ca-b495-53e3ed60340e",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "obj_retriever = obj_index.as_retriever(similarity_top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ba9cfecd-fe14-4da8-b9ba-b3d485d98a03",
   "metadata": {
    "height": 64,
    "tags": []
   },
   "outputs": [],
   "source": [
    "tools = obj_retriever.retrieve(\n",
    "    \"Tell me about the eval dataset used in MetaGPT and SWE-Bench\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c270ffbf-69c7-48ea-a028-9ba25221cde5",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ToolMetadata(description='Use ONLY IF you want to get a holistic summary of MetaGPT. Do NOT use if you have specific questions over MetaGPT.', name='summary_tool_values', fn_schema=<class 'llama_index.core.tools.types.DefaultToolFnSchema'>, return_direct=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools[2].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9cc0a0b6-9858-4348-9ae0-1cd4160f3fb7",
   "metadata": {
    "height": 251,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.core.agent import AgentRunner\n",
    "\n",
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    tool_retriever=obj_retriever,\n",
    "    llm=llm, \n",
    "    system_prompt=\"\"\" \\\n",
    "You are an agent designed to answer queries over a set of given papers.\n",
    "Please always use the tools provided to answer a question. Do not rely on prior knowledge.\\\n",
    "\n",
    "\"\"\",\n",
    "    verbose=True\n",
    ")\n",
    "agent = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a250cf1a-e011-4994-bcca-4e0294f20864",
   "metadata": {
    "height": 98,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Tell me about the evaluation dataset used in MetaGPT and compare it against SWE-Bench\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_metra with args: {\"input\": \"evaluation dataset used in MetaGPT\"}\n",
      "=== Function Output ===\n",
      "The evaluation dataset used in MetaGPT is not explicitly mentioned in the provided context information.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_swebench with args: {\"input\": \"comparison of evaluation datasets in MetaGPT and SWE-Bench\"}\n",
      "=== Function Output ===\n",
      "The evaluation datasets in MetaGPT and SWE-Bench differ significantly in terms of the tasks they focus on and the complexity of the challenges presented. MetaGPT's dataset involves self-contained problems that can be solved with a few lines of code, while SWE-Bench's dataset consists of real-world software engineering tasks that are more intricate and involve navigating large code repositories, understanding interactions between functions in different files, and making edits to codebases to resolve issues. This contrast highlights that MetaGPT's evaluation tasks are more contrived and simplistic compared to the challenging and realistic tasks found in SWE-Bench's dataset.\n",
      "=== LLM Response ===\n",
      "The evaluation dataset used in MetaGPT is not explicitly mentioned in the provided context information. \n",
      "\n",
      "The evaluation datasets in MetaGPT and SWE-Bench differ significantly in terms of the tasks they focus on and the complexity of the challenges presented. MetaGPT's dataset involves self-contained problems that can be solved with a few lines of code, while SWE-Bench's dataset consists of real-world software engineering tasks that are more intricate and involve navigating large code repositories, understanding interactions between functions in different files, and making edits to codebases to resolve issues. This contrast highlights that MetaGPT's evaluation tasks are more contrived and simplistic compared to the challenging and realistic tasks found in SWE-Bench's dataset.\n",
      "The evaluation dataset used in MetaGPT is not explicitly mentioned in the provided context information. \n",
      "\n",
      "The evaluation datasets in MetaGPT and SWE-Bench differ significantly in terms of the tasks they focus on and the complexity of the challenges presented. MetaGPT's dataset involves self-contained problems that can be solved with a few lines of code, while SWE-Bench's dataset consists of real-world software engineering tasks that are more intricate and involve navigating large code repositories, understanding interactions between functions in different files, and making edits to codebases to resolve issues. This contrast highlights that MetaGPT's evaluation tasks are more contrived and simplistic compared to the challenging and realistic tasks found in SWE-Bench's dataset.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\n",
    "    \"Tell me about the evaluation dataset used \"\n",
    "    \"in MetaGPT and compare it against SWE-Bench\"\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8084c8cb-98ed-4835-aaa4-5b0c7254be6d",
   "metadata": {
    "height": 81,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Compare and contrast the LoRA papers (LongLoRA, LoftQ). Analyze the approach in each paper first. \n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_longlora with args: {\"input\": \"LongLoRA paper\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 0.21019268753197506 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-LXJdh8mBCdaeYl79io2xzzqI on tokens per min (TPM): Limit 80000, Used 79817, Requested 3071. Please try again in 2.166s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 0.7341919421383195 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-LXJdh8mBCdaeYl79io2xzzqI on tokens per min (TPM): Limit 80000, Used 79308, Requested 3585. Please try again in 2.169s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 0.9378144480075631 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-LXJdh8mBCdaeYl79io2xzzqI on tokens per min (TPM): Limit 80000, Used 76993, Requested 3820. Please try again in 609ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 0.78818912126986 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-LXJdh8mBCdaeYl79io2xzzqI on tokens per min (TPM): Limit 80000, Used 77243, Requested 3585. Please try again in 621ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 0.5189760480191403 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-LXJdh8mBCdaeYl79io2xzzqI on tokens per min (TPM): Limit 80000, Used 76510, Requested 3820. Please try again in 247ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Function Output ===\n",
      "The LongLoRA paper introduces an efficient method for extending the context sizes of pre-trained large language models (LLMs) with limited computational cost. It utilizes shifted sparse attention (S2-Attn) during training to approximate long contexts effectively and efficiently. By combining this with parameter-efficient fine-tuning techniques, LongLoRA extends the context window of LLMs while retaining their original architectures. The approach demonstrates strong empirical results on various tasks and models, showcasing significant context extensions while maintaining performance and reducing memory costs compared to full fine-tuning. Additionally, LongLoRA is compatible with existing techniques like Flash-Attention2 and enables supervised fine-tuning with the LongAlpaca dataset.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_loftq with args: {\"input\": \"LoftQ paper\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 0.18852901443953807 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-LXJdh8mBCdaeYl79io2xzzqI on tokens per min (TPM): Limit 80000, Used 79751, Requested 3226. Please try again in 2.232s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 0.5396290620688187 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-LXJdh8mBCdaeYl79io2xzzqI on tokens per min (TPM): Limit 80000, Used 79620, Requested 3353. Please try again in 2.229s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.5763193959089554 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-LXJdh8mBCdaeYl79io2xzzqI on tokens per min (TPM): Limit 80000, Used 77299, Requested 3353. Please try again in 488ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Function Output ===\n",
      "The LoftQ paper introduces a quantization framework for Large Language Models (LLMs) that involves applying quantization and low-rank approximation alternately to the original high-precision pre-trained weights. The method aims to provide an initialization for subsequent LoRA fine-tuning. LoftQ has been evaluated on various natural language processing tasks, demonstrating superior performance compared to existing methods like QLoRA, especially in challenging low-bit quantization scenarios. The paper discusses model compression ratios, memory footprints, quantization time, and GLUE dataset statistics, showcasing the effectiveness and robustness of LoftQ in tasks such as natural language understanding, question answering, summarization, and natural language generation.\n",
      "=== LLM Response ===\n",
      "The LongLoRA paper introduces an efficient method for extending the context sizes of pre-trained large language models (LLMs) with limited computational cost. It utilizes shifted sparse attention (S2-Attn) during training to approximate long contexts effectively and efficiently. By combining this with parameter-efficient fine-tuning techniques, LongLoRA extends the context window of LLMs while retaining their original architectures. The approach demonstrates strong empirical results on various tasks and models, showcasing significant context extensions while maintaining performance and reducing memory costs compared to full fine-tuning. Additionally, LongLoRA is compatible with existing techniques like Flash-Attention2 and enables supervised fine-tuning with the LongAlpaca dataset.\n",
      "\n",
      "On the other hand, the LoftQ paper introduces a quantization framework for Large Language Models (LLMs) that involves applying quantization and low-rank approximation alternately to the original high-precision pre-trained weights. The method aims to provide an initialization for subsequent LoRA fine-tuning. LoftQ has been evaluated on various natural language processing tasks, demonstrating superior performance compared to existing methods like QLoRA, especially in challenging low-bit quantization scenarios. The paper discusses model compression ratios, memory footprints, quantization time, and GLUE dataset statistics, showcasing the effectiveness and robustness of LoftQ in tasks such as natural language understanding, question answering, summarization, and natural language generation.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\n",
    "    \"Compare and contrast the LoRA papers (LongLoRA, LoftQ). \"\n",
    "    \"Analyze the approach in each paper first. \"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b1774c-c50d-4ca6-81d3-8e682c51986a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
