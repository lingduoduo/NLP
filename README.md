## NLP

NLP with Transformers

Hands-On Machine Learning with Scikit-Learn and TensorFlow, by Aurélien Géron (O’Reilly)

Deep Learning for Coders with fastai and PyTorch, by Jeremy Howard and Sylvain Gugger (O’Reilly)

Natural Language Processing with PyTorch, by Delip Rao and Brian McMahan (O’Reilly)

The Hugging Face Course, by the open source team at Hugging Face

- regular expressions

- text normalization

- string similarity

- language models

- vector semantics

- embeddings


(Yoav Goldberg)

http://u.cs.biu.ac.il/~yogo/nnlp.pdf 

自然语言处理权威指南(monkeylearn.com)

https://monkeylearn.com/blog/the-definitive-guide-to-natural-language-processing/

自然语言处理入门(algorithmia.com)

https://blog.algorithmia.com/introduction-natural-language-processing-nlp/

自然语言处理教程 (vikparuchuri.com)

http://www.vikparuchuri.com/blog/natural-language-processing-tutorial/

Natural Language Processing (almost) from Scratch (arxiv.org)

初高中生课程：自然语言处理 (arxiv.org)

https://arxiv.org/pdf/1103.0398.pdf  

## 深度学习和 NLP

基于深度学习的NLP应用(arxiv.org)

https://arxiv.org/pdf/1703.03091.pdf

基于深度学习的NLP(Richard Socher)

https://nlp.stanford.edu/courses/NAACL2013/NAACL2013-Socher-Manning-DeepLearning.pdf

理解卷积神经网络在NLP中的应用(wildml.com)

http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/

深度学习，NLP，表示学习(colah.github.io)

http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/

嵌入表示，编码，注意力，预测 : 新一代深度学习因NLP的精妙而存在(explosion.ai)

https://explosion.ai/blog/deep-learning-formula-nlp

理解基于神经网络的自然语言处理(Torch实现) (nvidia.com)

https://devblogs.nvidia.com/parallelforall/understanding-natural-language-deep-neural-networks-using-torch/

深度学习在NLP中的应用(Pytorch实现) (pytorich.org)

http://pytorch.org/tutorials/beginner/deep_learning_nlp_tutorial.html

 ## 词向量（Word Vectors）

词袋法遇到感知器装袋法(kaggle.com)

https://www.kaggle.com/c/word2vec-nlp-tutorial

学习单词嵌入表示法(sebastianruder.com)

Part I：http://sebastianruder.com/word-embeddings-1/index.html

Part II：http://sebastianruder.com/word-embeddings-softmax/index.html

Part III：http://sebastianruder.com/secret-word2vec/index.html

单词嵌入表示的神奇力量(acolyer.org)

https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/

解释word2vec 的参数学习(arxiv.org)

https://arxiv.org/pdf/1411.2738.pdf

word2vec教程 skip-gram 模型，负采样(mccormickml.com)

http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/

## Encoder-Decoder

注意力机制与记忆机制在深度学习与NLP中的应用(wildml.com)

http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/

序列到序列模型(tensorflow.org)

https://www.tensorflow.org/tutorials/seq2seq

利用神经网络学习序列到序列模型(NIPS 2014)

https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf

基于深度学习和魔法序列的语言翻译(medium.com/@ageitgey)

https://medium.com/@ageitgey/machine-learning-is-fun-part-5-language-translation-with-deep-learning-and-the-magic-of-sequences-2ace0acca0aa

如何使用编码-解码LSTM输出随机整数对应的序列(machinelearningmastery.com)

http://machinelearningmastery.com/how-to-use-an-encoder-decoder-lstm-to-echo-sequences-of-random-integers/

tf-seq2seq (google.github.io)

https://google.github.io/seq2seq

