{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b97d18c",
   "metadata": {},
   "source": [
    "## 6.1 Flavors of BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55fc4c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90a1c04d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.bert.modeling_bert.BertForMaskedLM'>\n",
      "If you don’t *** at the sign, you will get a ticket\n",
      "Token:look. Score: 51.35%\n",
      "Token:stop. Score: 39.66%\n",
      "Token:glance. Score: 1.02%\n",
      "Token:wait. Score: 0.60%\n",
      "Token:turn. Score: 0.57%\n"
     ]
    }
   ],
   "source": [
    "nlp = pipeline(\"fill-mask\", model='bert-base-cased')\n",
    "\n",
    "print(type(nlp.model))\n",
    "\n",
    "preds = nlp(f\"If you don’t {nlp.tokenizer.mask_token} at the sign, you will get a ticket\")\n",
    "\n",
    "print('If you don’t *** at the sign, you will get a ticket')\n",
    "\n",
    "for p in preds:\n",
    "    print(f\"Token:{p['token_str']}. Score: {100*p['score']:,.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c993940",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForMaskedLM were not initialized from the model checkpoint at roberta-base and are newly initialized: ['lm_head.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.roberta.modeling_roberta.RobertaForMaskedLM'>\n",
      "If you don’t *** at the sign, you will get a ticket\n",
      "Token:Ġlook. Score: 47.69%\n",
      "Token:Ġstop. Score: 36.82%\n",
      "Token:Ġstand. Score: 2.54%\n",
      "Token:Ġstay. Score: 2.52%\n",
      "Token:Ġwave. Score: 1.01%\n"
     ]
    }
   ],
   "source": [
    "nlp = pipeline(\"fill-mask\", model='roberta-base')\n",
    "\n",
    "print(type(nlp.model))\n",
    "\n",
    "preds = nlp(f\"If you don’t {nlp.tokenizer.mask_token} at the sign, you will get a ticket\")\n",
    "\n",
    "print('If you don’t *** at the sign, you will get a ticket')\n",
    "\n",
    "for p in preds:\n",
    "    print(f\"Token:{p['token_str']}. Score: {100*p['score']:,.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81cceeea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForMaskedLM were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['lm_head.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.roberta.modeling_roberta.RobertaForMaskedLM'>\n",
      "If you don’t *** at the sign, you will get a ticket\n",
      "Token:Ġstop. Score: 42.11%\n",
      "Token:Ġlook. Score: 7.53%\n",
      "Token:Ġpark. Score: 4.92%\n",
      "Token:Ġarrive. Score: 4.65%\n",
      "Token:Ġsign. Score: 4.27%\n"
     ]
    }
   ],
   "source": [
    "nlp = pipeline(\"fill-mask\", model='distilroberta-base')\n",
    "\n",
    "print(type(nlp.model))\n",
    "\n",
    "preds = nlp(f\"If you don’t {nlp.tokenizer.mask_token} at the sign, you will get a ticket\")\n",
    "\n",
    "print('If you don’t *** at the sign, you will get a ticket')\n",
    "\n",
    "for p in preds:\n",
    "    print(f\"Token:{p['token_str']}. Score: {100*p['score']:,.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6dbf2ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.distilbert.modeling_distilbert.DistilBertForMaskedLM'>\n",
      "If you don’t *** at the sign, you will get a ticket\n",
      "Token:look. Score: 57.47%\n",
      "Token:stop. Score: 7.37%\n",
      "Token:glance. Score: 3.74%\n",
      "Token:arrive. Score: 2.16%\n",
      "Token:appear. Score: 1.87%\n"
     ]
    }
   ],
   "source": [
    "nlp = pipeline(\"fill-mask\", model='distilbert-base-cased')  # Using a flavor of BERT called DistilBERT\n",
    "\n",
    "print(type(nlp.model))  \n",
    "\n",
    "preds = nlp(f\"If you don’t {nlp.tokenizer.mask_token} at the sign, you will get a ticket\")\n",
    "\n",
    "print('If you don’t *** at the sign, you will get a ticket')\n",
    "\n",
    "for p in preds:\n",
    "    print(f\"Token:{p['token_str']}. Score: {100*p['score']:,.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9098dcb1",
   "metadata": {},
   "source": [
    "## 6.2 BERT for sequence classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2484167a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, DistilBertForSequenceClassification, DistilBertTokenizerFast, \\\n",
    "     DataCollatorWithPadding, pipeline\n",
    "from datasets import load_metric, Dataset\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "major-driver",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "listen O\r\n",
      "to O\r\n",
      "westbam B-artist\r\n",
      "alumb O\r\n",
      "allergic B-album\r\n",
      "on O\r\n",
      "google B-service\r\n",
      "music I-service\r\n",
      "PlayMusic\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!head snips.train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4e17957",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'listen O\\n',\n",
       " b'to O\\n',\n",
       " b'westbam B-artist\\n',\n",
       " b'alumb O\\n',\n",
       " b'allergic B-album\\n',\n",
       " b'on O\\n',\n",
       " b'google B-service\\n',\n",
       " b'music I-service\\n',\n",
       " b'PlayMusic\\n',\n",
       " b'\\n',\n",
       " b'add O\\n',\n",
       " b'step B-entity_name\\n',\n",
       " b'to I-entity_name\\n',\n",
       " b'me I-entity_name\\n',\n",
       " b'to O\\n',\n",
       " b'the O\\n',\n",
       " b'50 B-playlist\\n',\n",
       " b'cl\\xc3\\xa1sicos I-playlist\\n',\n",
       " b'playlist O\\n',\n",
       " b'AddToPlaylist\\n']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snips_file = open('./snips.train.txt', 'rb')\n",
    "\n",
    "snips_rows = snips_file.readlines()\n",
    "\n",
    "snips_rows[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f7b8a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code segment parses the snips dataset into a more manageable format\n",
    "\n",
    "utterances = []\n",
    "tokenized_utterances = []\n",
    "labels_for_tokens = []\n",
    "sequence_labels = []\n",
    "\n",
    "utterance, tokenized_utterance, label_for_utterances = '', [], []\n",
    "for snip_row in snips_rows:\n",
    "    if len(snip_row) == 2:  # skip over rows with no data\n",
    "        continue\n",
    "    if ' ' not in snip_row.decode():  # we've hit a sequence label\n",
    "        sequence_labels.append(snip_row.decode().strip())\n",
    "        utterances.append(utterance.strip())\n",
    "        tokenized_utterances.append(tokenized_utterance)\n",
    "        labels_for_tokens.append(label_for_utterances)\n",
    "        utterance = ''\n",
    "        tokenized_utterance = []\n",
    "        label_for_utterances = []\n",
    "        continue\n",
    "    token, token_label = snip_row.decode().split(' ')\n",
    "    token_label = token_label.strip()\n",
    "    utterance += f'{token} '\n",
    "    tokenized_utterance.append(token)\n",
    "    label_for_utterances.append(token_label)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c78793ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26167, 26167, 26167, 26167)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels_for_tokens), len(tokenized_utterances), len(utterances), len(sequence_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "774c7c64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb5d631e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'PlayMusic',\n",
       " 'SearchCreativeWork',\n",
       " 'GetWeather',\n",
       " 'SearchScreeningEvent',\n",
       " 'AddToPlaylist',\n",
       " 'RateBook',\n",
       " 'BookRestaurant']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_sequence_labels = list(set(sequence_labels))\n",
    "unique_sequence_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e9fce2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 8 unique sequence labels\n"
     ]
    }
   ],
   "source": [
    "sequence_labels = [unique_sequence_labels.index(l) for l in sequence_labels]\n",
    "\n",
    "print(f'There are {len(unique_sequence_labels)} unique sequence labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6de12c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 72 unique token labels\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "\n",
    "unique_token_labels = list(set(reduce(lambda x, y: x + y, labels_for_tokens)))\n",
    "labels_for_tokens = [[unique_token_labels.index(_) for _ in l] for l in labels_for_tokens]\n",
    "\n",
    "print(f'There are {len(unique_token_labels)} unique token labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3a5251b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['listen', 'to', 'westbam', 'alumb', 'allergic', 'on', 'google', 'music']\n",
      "[25, 25, 27, 25, 28, 25, 34, 22]\n",
      "[25, 25, 27, 25, 28, 25, 34, 22]\n",
      "listen to westbam alumb allergic on google music\n",
      "1\n",
      "PlayMusic\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_utterances[0])\n",
    "print(labels_for_tokens[0])\n",
    "print([unique_token_labels[l] for l in labels_for_tokens[0]])\n",
    "print(utterances[0])\n",
    "print(sequence_labels[0])\n",
    "print(unique_sequence_labels[sequence_labels[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eec863af",
   "metadata": {},
   "outputs": [],
   "source": [
    "snips_dataset = Dataset.from_dict(\n",
    "    dict(\n",
    "        utterance=utterances, \n",
    "        label=sequence_labels,\n",
    "        tokens=tokenized_utterances,\n",
    "        token_labels=labels_for_tokens\n",
    "    )\n",
    ")\n",
    "snips_dataset = snips_dataset.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "67a6b70d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'utterance': 'in 1 second i need a restaurant in san marino for 9',\n",
       " 'label': 7,\n",
       " 'tokens': ['in',\n",
       "  '1',\n",
       "  'second',\n",
       "  'i',\n",
       "  'need',\n",
       "  'a',\n",
       "  'restaurant',\n",
       "  'in',\n",
       "  'san',\n",
       "  'marino',\n",
       "  'for',\n",
       "  '9'],\n",
       " 'token_labels': [44, 4, 4, 25, 25, 25, 36, 25, 50, 3, 25, 49]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snips_dataset['train'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc13ba97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple function to batch tokenize utterances with truncation\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"utterance\"], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bee6381f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98431e6041534634ba48b44a98fd5985",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20933 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edfcb850ec2a40cd81ce265fc854a657",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5234 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "seq_clf_tokenized_snips = snips_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3da2a0bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'utterance': 'add to my playlist lazy chill afternoon nothing can stop us',\n",
       " 'label': 5,\n",
       " 'tokens': ['add',\n",
       "  'to',\n",
       "  'my',\n",
       "  'playlist',\n",
       "  'lazy',\n",
       "  'chill',\n",
       "  'afternoon',\n",
       "  'nothing',\n",
       "  'can',\n",
       "  'stop',\n",
       "  'us'],\n",
       " 'token_labels': [25, 25, 14, 25, 70, 40, 40, 67, 8, 8, 8],\n",
       " 'input_ids': [101,\n",
       "  5194,\n",
       "  1106,\n",
       "  1139,\n",
       "  1505,\n",
       "  7276,\n",
       "  16688,\n",
       "  11824,\n",
       "  4427,\n",
       "  1720,\n",
       "  1169,\n",
       "  1831,\n",
       "  1366,\n",
       "  102],\n",
       " 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_clf_tokenized_snips['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9a2333ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataCollatorWithPadding creates batch of data. It also dynamically pads text to the \n",
    "#  length of the longest element in the batch, making them all the same length. \n",
    "#  It's possible to pad your text in the tokenizer function with padding=True, dynamic padding is more efficient.\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "93b14125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Collator will pad data so that all examples are the same input length.\n",
    "#  Attention mask is how we ignore attention scores for padding tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d647d950",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "sequence_clf_model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    'distilbert-base-cased', \n",
    "    num_labels=len(unique_sequence_labels),\n",
    ")\n",
    "\n",
    "# set an index -> label dictionary\n",
    "sequence_clf_model.config.id2label = {i: l for i, l in enumerate(unique_sequence_labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a4db4d8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PlayMusic'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_clf_model.config.id2label[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6352abe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):  # custom method to take in logits and calculate accuracy of the eval set\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a04da07b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W&B installed but not logged in. Run `wandb login` or set the WANDB_API_KEY env variable.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "# Set the logging level\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "train_dataset_size = len(seq_clf_tokenized_snips['train'])\n",
    "batch_size = 32  # This should be your actual batch size per device\n",
    "steps_per_epoch = train_dataset_size // batch_size\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./snips_clf/results\",\n",
    "    num_train_epochs=epochs,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    load_best_model_at_end=True,\n",
    "    warmup_steps=steps_per_epoch // 5,  # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.05,\n",
    "    logging_steps=1,\n",
    "    evaluation_strategy='epoch',\n",
    "    eval_steps=50,\n",
    "    save_steps=steps_per_epoch  # Save at the end of each epoch\n",
    ")\n",
    "# Assume the rest of your definitions (model, data_collator, etc.) are correctly defined above.\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=sequence_clf_model,\n",
    "    args=training_args,\n",
    "    train_dataset=seq_clf_tokenized_snips['train'],\n",
    "    eval_dataset=seq_clf_tokenized_snips['test'],\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d1c7d229",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='328' max='164' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [164/164 21:36]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 2.1448607444763184,\n",
       " 'eval_accuracy': 0.07737867787542989,\n",
       " 'eval_runtime': 54.3648,\n",
       " 'eval_samples_per_second': 96.275}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get initial metrics\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "08f1cf8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='1310' max='1310' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1310/1310 41:20, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.003100</td>\n",
       "      <td>0.023414</td>\n",
       "      <td>0.993313</td>\n",
       "      <td>74.077100</td>\n",
       "      <td>70.656000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.024112</td>\n",
       "      <td>0.993886</td>\n",
       "      <td>71.189600</td>\n",
       "      <td>73.522000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1310, training_loss=0.11484244197232912, metrics={'train_runtime': 2482.4207, 'train_samples_per_second': 0.528, 'total_flos': 365421482511360, 'epoch': 2.0})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f8fe9537",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='164' max='164' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [164/164 01:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.02341417223215103,\n",
       " 'eval_accuracy': 0.9933129537638518,\n",
       " 'eval_runtime': 73.0449,\n",
       " 'eval_samples_per_second': 71.655,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e93471e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'AddToPlaylist', 'score': 0.9994660019874573}]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = pipeline(\"sentiment-analysis\", sequence_clf_model, tokenizer=tokenizer)\n",
    "pipe('Please add Here We Go by Dispatch to my road trip playlist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5b373004",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7d7d27bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'AddToPlaylist', 'score': 0.998703122138977}]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = pipeline(\"sentiment-analysis\", \"./snips_clf/results\", tokenizer=tokenizer)\n",
    "pipe('Please add Here We Go by Dispatch to my road trip playlist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "52fef242",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "frozen_sequence_clf_model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    'distilbert-base-cased', \n",
    "    num_labels=len(unique_sequence_labels),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e8fa35be",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in frozen_sequence_clf_model.distilbert.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8dad2bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W&B installed but not logged in. Run `wandb login` or set the WANDB_API_KEY env variable.\n",
      "W&B installed but not logged in. Run `wandb login` or set the WANDB_API_KEY env variable.\n"
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./snips_clf/results\",\n",
    "    num_train_epochs=epochs,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    load_best_model_at_end=True,\n",
    "    warmup_steps=steps_per_epoch // 5,  # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.05,\n",
    "    logging_steps=1,\n",
    "    evaluation_strategy='epoch',\n",
    "    eval_steps=50,\n",
    "    save_steps=steps_per_epoch  # Save at the end of each epoch\n",
    ")\n",
    "# Assume the rest of your definitions (model, data_collator, etc.) are correctly defined above.\n",
    "\n",
    "# Define the trainer:\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=frozen_sequence_clf_model,\n",
    "    args=training_args,\n",
    "    train_dataset=seq_clf_tokenized_snips['train'],\n",
    "    eval_dataset=seq_clf_tokenized_snips['test'],\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cc1d2ed0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='328' max='164' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [164/164 06:47]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 2.1084353923797607,\n",
       " 'eval_accuracy': 0.07508597630875048,\n",
       " 'eval_runtime': 47.2123,\n",
       " 'eval_samples_per_second': 110.861}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bfa45688",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='1310' max='1310' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1310/1310 12:17, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.860300</td>\n",
       "      <td>0.560359</td>\n",
       "      <td>0.930264</td>\n",
       "      <td>67.375000</td>\n",
       "      <td>77.685000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.099400</td>\n",
       "      <td>0.409285</td>\n",
       "      <td>0.951471</td>\n",
       "      <td>70.897200</td>\n",
       "      <td>73.825000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1310, training_loss=0.7485144596802824, metrics={'train_runtime': 738.1337, 'train_samples_per_second': 1.775, 'total_flos': 365421482511360, 'epoch': 2.0})"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()  # ~23min -> ~6min on my laptop with all of distilbert frozen with a worse loss/accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b911ff29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='164' max='164' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [164/164 01:08]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.409284770488739,\n",
       " 'eval_accuracy': 0.9514711501719526,\n",
       " 'eval_runtime': 69.2081,\n",
       " 'eval_samples_per_second': 75.627,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d99579a",
   "metadata": {},
   "source": [
    "## 6.3 BERT for token classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bb6edfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification, DistilBertForTokenClassification, \\\n",
    "                         DistilBertTokenizerFast, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3cdb1bcc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# using a cased tokenizer because I think case will matter\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "446479c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'utterance': 'add to my playlist lazy chill afternoon nothing can stop us',\n",
       " 'label': 5,\n",
       " 'tokens': ['add',\n",
       "  'to',\n",
       "  'my',\n",
       "  'playlist',\n",
       "  'lazy',\n",
       "  'chill',\n",
       "  'afternoon',\n",
       "  'nothing',\n",
       "  'can',\n",
       "  'stop',\n",
       "  'us'],\n",
       " 'token_labels': [25, 25, 14, 25, 70, 40, 40, 67, 8, 8, 8]}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snips_dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "36d41d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The given \"token_labels\" may not match up with the BERT wordpiece tokenization so\n",
    "#  this function will map them to the tokenization that BERT uses\n",
    "#  -100 is a reserved for labels where we do not want to calculate losses so BERT doesn't waste time\n",
    "#  trying to predict tokens like CLS or SEP\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"token_labels\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:  # Set the special tokens to -100.\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)  # CLS and SEP are labeled as -100\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "50a7ac1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'utterance': 'add to my playlist lazy chill afternoon nothing can stop us',\n",
       " 'label': 5,\n",
       " 'tokens': ['add',\n",
       "  'to',\n",
       "  'my',\n",
       "  'playlist',\n",
       "  'lazy',\n",
       "  'chill',\n",
       "  'afternoon',\n",
       "  'nothing',\n",
       "  'can',\n",
       "  'stop',\n",
       "  'us'],\n",
       " 'token_labels': [25, 25, 14, 25, 70, 40, 40, 67, 8, 8, 8]}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snips_dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "832dd8e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a228c99c673f4e0eac4f7f187f1eaab5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20933 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9963eca5e2ee4d1798d89d7ce8d8cc91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5234 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# map our dataset from sequence classification to be for token classification\n",
    "tok_clf_tokenized_snips = snips_dataset.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d4b31d63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'utterance': 'add to my playlist lazy chill afternoon nothing can stop us',\n",
       " 'label': 5,\n",
       " 'tokens': ['add',\n",
       "  'to',\n",
       "  'my',\n",
       "  'playlist',\n",
       "  'lazy',\n",
       "  'chill',\n",
       "  'afternoon',\n",
       "  'nothing',\n",
       "  'can',\n",
       "  'stop',\n",
       "  'us'],\n",
       " 'token_labels': [25, 25, 14, 25, 70, 40, 40, 67, 8, 8, 8],\n",
       " 'input_ids': [101,\n",
       "  5194,\n",
       "  1106,\n",
       "  1139,\n",
       "  1505,\n",
       "  7276,\n",
       "  16688,\n",
       "  11824,\n",
       "  4427,\n",
       "  1720,\n",
       "  1169,\n",
       "  1831,\n",
       "  1366,\n",
       "  102],\n",
       " 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " 'labels': [-100, 25, 25, 14, 25, -100, 70, 40, 40, 67, 8, 8, 8, -100]}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_clf_tokenized_snips['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f4b4c9c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 20933\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 5234\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_clf_tokenized_snips['train'] = tok_clf_tokenized_snips['train'].remove_columns(\n",
    "    ['utterance', 'label', 'tokens', 'token_labels']\n",
    ")\n",
    "\n",
    "tok_clf_tokenized_snips['test'] = tok_clf_tokenized_snips['test'].remove_columns(\n",
    "    ['utterance', 'label', 'tokens', 'token_labels']\n",
    ")\n",
    "\n",
    "tok_clf_tokenized_snips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1f29865e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b715cf3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForTokenClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tok_clf_model = DistilBertForTokenClassification.from_pretrained(\n",
    "    'distilbert-base-cased', num_labels=len(unique_token_labels)\n",
    ")\n",
    "\n",
    "# Set our label dictionary\n",
    "tok_clf_model.config.id2label = {i: l for i, l in enumerate(unique_token_labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "08bda49b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_clf_model.config.id2label[0], tok_clf_model.config.id2label[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c848c3f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W&B installed but not logged in. Run `wandb login` or set the WANDB_API_KEY env variable.\n"
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./snips_clf/results\",\n",
    "    num_train_epochs=epochs,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    load_best_model_at_end=True,\n",
    "    warmup_steps=steps_per_epoch // 5,  # number of warmup steps for learning rate scheduler\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy='epoch',\n",
    "    eval_steps=50,\n",
    "    save_steps=steps_per_epoch  # Save at the end of each epoch\n",
    ")\n",
    "# Assume the rest of your definitions (model, data_collator, etc.) are correctly defined above.\n",
    "\n",
    "# Define the trainer:\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=frozen_sequence_clf_model,\n",
    "    args=training_args,\n",
    "    train_dataset=seq_clf_tokenized_snips['train'],\n",
    "    eval_dataset=seq_clf_tokenized_snips['test'],\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "respiratory-novelty",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.409284770488739,\n",
       " 'eval_accuracy': 0.9514711501719526,\n",
       " 'eval_runtime': 49.4839,\n",
       " 'eval_samples_per_second': 105.772}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f6cd1c90",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='1310' max='1310' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1310/1310 12:50, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.243800</td>\n",
       "      <td>0.213216</td>\n",
       "      <td>0.960451</td>\n",
       "      <td>72.264300</td>\n",
       "      <td>72.429000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.167800</td>\n",
       "      <td>0.179031</td>\n",
       "      <td>0.965227</td>\n",
       "      <td>73.968800</td>\n",
       "      <td>70.760000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1310, training_loss=0.2654997528964327, metrics={'train_runtime': 770.9428, 'train_samples_per_second': 1.699, 'total_flos': 364730712123360, 'epoch': 2.0})"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "01b18ae8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='164' max='164' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [164/164 01:13]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.1790306270122528,\n",
       " 'eval_accuracy': 0.965227359572029,\n",
       " 'eval_runtime': 73.6744,\n",
       " 'eval_samples_per_second': 71.042,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8fb6a994",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'word': 'Please',\n",
       "  'score': 0.027198350057005882,\n",
       "  'entity': 37,\n",
       "  'index': 1,\n",
       "  'start': 0,\n",
       "  'end': 6},\n",
       " {'word': 'add',\n",
       "  'score': 0.024076612666249275,\n",
       "  'entity': 37,\n",
       "  'index': 2,\n",
       "  'start': 7,\n",
       "  'end': 10},\n",
       " {'word': 'Here',\n",
       "  'score': 0.024415116757154465,\n",
       "  'entity': 64,\n",
       "  'index': 3,\n",
       "  'start': 11,\n",
       "  'end': 15},\n",
       " {'word': 'We',\n",
       "  'score': 0.025688957422971725,\n",
       "  'entity': 64,\n",
       "  'index': 4,\n",
       "  'start': 16,\n",
       "  'end': 18},\n",
       " {'word': 'Go',\n",
       "  'score': 0.02504601702094078,\n",
       "  'entity': 64,\n",
       "  'index': 5,\n",
       "  'start': 19,\n",
       "  'end': 21},\n",
       " {'word': 'by',\n",
       "  'score': 0.026233388110995293,\n",
       "  'entity': 64,\n",
       "  'index': 6,\n",
       "  'start': 22,\n",
       "  'end': 24},\n",
       " {'word': 'Di',\n",
       "  'score': 0.0220949649810791,\n",
       "  'entity': 37,\n",
       "  'index': 7,\n",
       "  'start': 25,\n",
       "  'end': 27},\n",
       " {'word': '##sp',\n",
       "  'score': 0.025229359045624733,\n",
       "  'entity': 64,\n",
       "  'index': 8,\n",
       "  'start': 27,\n",
       "  'end': 29},\n",
       " {'word': '##atch',\n",
       "  'score': 0.027838239446282387,\n",
       "  'entity': 37,\n",
       "  'index': 9,\n",
       "  'start': 29,\n",
       "  'end': 33},\n",
       " {'word': 'to',\n",
       "  'score': 0.027561308816075325,\n",
       "  'entity': 64,\n",
       "  'index': 10,\n",
       "  'start': 34,\n",
       "  'end': 36},\n",
       " {'word': 'my',\n",
       "  'score': 0.0243206974118948,\n",
       "  'entity': 64,\n",
       "  'index': 11,\n",
       "  'start': 37,\n",
       "  'end': 39},\n",
       " {'word': 'road',\n",
       "  'score': 0.026441151276230812,\n",
       "  'entity': 64,\n",
       "  'index': 12,\n",
       "  'start': 40,\n",
       "  'end': 44},\n",
       " {'word': 'trip',\n",
       "  'score': 0.03134581446647644,\n",
       "  'entity': 64,\n",
       "  'index': 13,\n",
       "  'start': 45,\n",
       "  'end': 49},\n",
       " {'word': 'play',\n",
       "  'score': 0.02508917823433876,\n",
       "  'entity': 64,\n",
       "  'index': 14,\n",
       "  'start': 50,\n",
       "  'end': 54},\n",
       " {'word': '##list',\n",
       "  'score': 0.026814335957169533,\n",
       "  'entity': 64,\n",
       "  'index': 15,\n",
       "  'start': 54,\n",
       "  'end': 58}]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = pipeline(\"ner\", tok_clf_model, tokenizer=tokenizer)\n",
    "pipe('Please add Here We Go by Dispatch to my road trip playlist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6749678a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'word': 'Rate',\n",
       "  'score': 0.03451903909444809,\n",
       "  'entity': 37,\n",
       "  'index': 1,\n",
       "  'start': 0,\n",
       "  'end': 4},\n",
       " {'word': 'the',\n",
       "  'score': 0.02664942853152752,\n",
       "  'entity': 64,\n",
       "  'index': 2,\n",
       "  'start': 5,\n",
       "  'end': 8},\n",
       " {'word': 'do',\n",
       "  'score': 0.02363862656056881,\n",
       "  'entity': 64,\n",
       "  'index': 3,\n",
       "  'start': 9,\n",
       "  'end': 11},\n",
       " {'word': '##og',\n",
       "  'score': 0.03238973021507263,\n",
       "  'entity': 64,\n",
       "  'index': 4,\n",
       "  'start': 11,\n",
       "  'end': 13},\n",
       " {'word': 'food',\n",
       "  'score': 0.027224943041801453,\n",
       "  'entity': 37,\n",
       "  'index': 5,\n",
       "  'start': 14,\n",
       "  'end': 18},\n",
       " {'word': '5',\n",
       "  'score': 0.03654227405786514,\n",
       "  'entity': 64,\n",
       "  'index': 6,\n",
       "  'start': 19,\n",
       "  'end': 20},\n",
       " {'word': 'out',\n",
       "  'score': 0.032032933086156845,\n",
       "  'entity': 12,\n",
       "  'index': 7,\n",
       "  'start': 21,\n",
       "  'end': 24},\n",
       " {'word': 'of',\n",
       "  'score': 0.028461666777729988,\n",
       "  'entity': 12,\n",
       "  'index': 8,\n",
       "  'start': 25,\n",
       "  'end': 27},\n",
       " {'word': '5',\n",
       "  'score': 0.030903268605470657,\n",
       "  'entity': 64,\n",
       "  'index': 9,\n",
       "  'start': 28,\n",
       "  'end': 29}]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = pipeline(\"ner\", tok_clf_model, tokenizer=tokenizer)\n",
    "pipe('Rate the doog food 5 out of 5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ac8a2a",
   "metadata": {},
   "source": [
    "## 6.4 BERT for question/answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "bb67c5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast, BertForQuestionAnswering, pipeline, \\\n",
    "                         DataCollatorWithPadding, TrainingArguments, Trainer, \\\n",
    "                         AutoModelForQuestionAnswering, AutoTokenizer\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "81682de5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:filelock:Lock 5471113072 acquired on /Users/ling/.cache/huggingface/transformers/e12f02d630da91a0982ce6db1ad595231d155a2b725ab106971898276d842ecc.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c78dea045324943a84385d23173aba4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:filelock:Lock 5471113072 released on /Users/ling/.cache/huggingface/transformers/e12f02d630da91a0982ce6db1ad595231d155a2b725ab106971898276d842ecc.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock\n",
      "INFO:filelock:Lock 5454409440 acquired on /Users/ling/.cache/huggingface/transformers/475d46024228961ca8770cead39e1079f135fd2441d14cf216727ffac8d41d78.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4.lock\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23e8970ce57f4efdae8f7a16ef1db1f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:filelock:Lock 5454409440 released on /Users/ling/.cache/huggingface/transformers/475d46024228961ca8770cead39e1079f135fd2441d14cf216727ffac8d41d78.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4.lock\n",
      "INFO:filelock:Lock 5451486160 acquired on /Users/ling/.cache/huggingface/transformers/1cf090f220f9674b67b3434decfe4d40a6532d7849653eac435ff94d31a4904c.1d03e5e4fa2db2532c517b2cd98290d8444b237619bd3d2039850a6d5e86473d.lock\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef98ca72e5b445cfa6dbaad9ea53d51e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:filelock:Lock 5451486160 released on /Users/ling/.cache/huggingface/transformers/1cf090f220f9674b67b3434decfe4d40a6532d7849653eac435ff94d31a4904c.1d03e5e4fa2db2532c517b2cd98290d8444b237619bd3d2039850a6d5e86473d.lock\n",
      "INFO:filelock:Lock 5468996128 acquired on /Users/ling/.cache/huggingface/transformers/1d959166dd7e047e57ea1b2d9b7b9669938a7e90c5e37a03961ad9f15eaea17f.fea64cd906e3766b04c92397f9ad3ff45271749cbe49829a079dd84e34c1697d.lock\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb47117e43a24decba5e10264411a33f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:filelock:Lock 5468996128 released on /Users/ling/.cache/huggingface/transformers/1d959166dd7e047e57ea1b2d9b7b9669938a7e90c5e37a03961ad9f15eaea17f.fea64cd906e3766b04c92397f9ad3ff45271749cbe49829a079dd84e34c1697d.lock\n",
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForQuestionAnswering: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "bert_tokenizer = BertTokenizerFast.from_pretrained('bert-large-uncased', return_token_type_ids=True)\n",
    "\n",
    "qa_bert = BertForQuestionAnswering.from_pretrained('bert-large-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "415ca621",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29989, 5)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_df = pd.read_csv('./qa.csv')\n",
    "\n",
    "qa_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "95833b38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>context</th>\n",
       "      <th>start_positions</th>\n",
       "      <th>end_positions</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What sare the benifts of the blood brain barrir?</td>\n",
       "      <td>Another approach to brain function is to exami...</td>\n",
       "      <td>56</td>\n",
       "      <td>60</td>\n",
       "      <td>isolated from the bloodstream</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is surrounded by cerebrospinal fluid?</td>\n",
       "      <td>Another approach to brain function is to exami...</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>brain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What does the skull protect?</td>\n",
       "      <td>Another approach to brain function is to exami...</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>brain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What has been injected into rats to produce pr...</td>\n",
       "      <td>Another approach to brain function is to exami...</td>\n",
       "      <td>153</td>\n",
       "      <td>153</td>\n",
       "      <td>chemicals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What can cause issues with how the brain works?</td>\n",
       "      <td>Another approach to brain function is to exami...</td>\n",
       "      <td>93</td>\n",
       "      <td>94</td>\n",
       "      <td>brain damage</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0   What sare the benifts of the blood brain barrir?   \n",
       "1         What is surrounded by cerebrospinal fluid?   \n",
       "2                       What does the skull protect?   \n",
       "3  What has been injected into rats to produce pr...   \n",
       "4    What can cause issues with how the brain works?   \n",
       "\n",
       "                                             context  start_positions  \\\n",
       "0  Another approach to brain function is to exami...               56   \n",
       "1  Another approach to brain function is to exami...               16   \n",
       "2  Another approach to brain function is to exami...               11   \n",
       "3  Another approach to brain function is to exami...              153   \n",
       "4  Another approach to brain function is to exami...               93   \n",
       "\n",
       "   end_positions                         answer  \n",
       "0             60  isolated from the bloodstream  \n",
       "1             16                          brain  \n",
       "2             11                          brain  \n",
       "3            153                      chemicals  \n",
       "4             94                   brain damage  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f65283a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "question            What sare the benifts of the blood brain barrir?\n",
       "context            Another approach to brain function is to exami...\n",
       "start_positions                                                   56\n",
       "end_positions                                                     60\n",
       "answer                                 isolated from the bloodstream\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9fe5e66d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'isolated from the bloodstream'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# index 56, 57, 58, 59, and 60 including question while encoding\n",
    "bert_tokenizer.decode(bert_tokenizer.encode(qa_df.iloc[0].question, qa_df.iloc[0].context)[56:61])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "645f97db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only grab 4,000 examples\n",
    "qa_dataset = Dataset.from_pandas(qa_df.sample(4000, random_state=42))\n",
    "\n",
    "# Dataset has a built in train test split method\n",
    "qa_dataset = qa_dataset.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "91087a5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfbf5c0fd8d949f8b2c1f06704c78e52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4f912224505492e96a2cb79bbe292ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# standard preprocessing here with truncation on to truncate longer text\n",
    "def preprocess(data):\n",
    "    return bert_tokenizer(data['question'], data['context'], truncation=True)\n",
    "\n",
    "qa_dataset = qa_dataset.map(preprocess, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "bcf73c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze all but the last 2 encoder layers in BERT to speed up training\n",
    "for name, param in qa_bert.bert.named_parameters():\n",
    "    if 'encoder.layer.22' in name:\n",
    "        break\n",
    "    param.requires_grad = False  # disable training in BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d4bc2428",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=bert_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1b7c6f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W&B installed but not logged in. Run `wandb login` or set the WANDB_API_KEY env variable.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='50' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25/25 3:12:35]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 5.804353713989258,\n",
       " 'eval_runtime': 1436.2112,\n",
       " 'eval_samples_per_second': 0.557}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 2\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./qa/results',\n",
    "    num_train_epochs=epochs,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy='epoch',\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=qa_bert,\n",
    "    args=training_args,\n",
    "    train_dataset=qa_dataset['train'],\n",
    "    eval_dataset=qa_dataset['test'],\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "# Get initial metrics\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e29d48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='102' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [102/200 2:49:39 < 2:46:15, 0.01 it/s, Epoch 1.01/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.398100</td>\n",
       "      <td>4.297838</td>\n",
       "      <td>1798.862500</td>\n",
       "      <td>0.445000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea45180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q/A models are very large and take a long time to fine-tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb1fa00",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c2405e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fa4fbb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pipe = pipeline(\"question-answering\", './qa/results', tokenizer=bert_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56c0676",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pipe(\"Where is Sinan living these days?\", \"Sinan lives in California but Matt lives in Boston.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad03889",
   "metadata": {},
   "outputs": [],
   "source": [
    "princeton = \"\"\"In 1675, a Quaker missionary from England, encouraged by New Jersey proprietors John Lord \n",
    "              \"Berkeley and Sir George Carteret, arrived to establish a settlement in this area near the \n",
    "              \"Delaware River, which was inhabited by the Lenni-Lenape Indians. The Keith survey of 1685 \n",
    "              \"established the western boundary of Middlesex and Somerset Counties and later, the Township \n",
    "              \"of Princeton. Today Keith's Line is recognized as Province Line Road. With the laying of the \n",
    "              \"cornerstone for Nassau Hall in 1754, Princeton began its development as a location for \n",
    "              \"quality education. Nassau Hall was named for William III, Prince of Orange-Nassau. This simple stone \n",
    "              \"edifice was one of the largest public buildings in the colonies and became a model for many other \n",
    "              \"structures in New Jersey and Pennsylvania.\"\"\"\n",
    "\n",
    "pipe(\"What survey led to the founding of Princeton?\", princeton)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9a5a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "PERSON = 'Sinan Ozdemir'\n",
    "\n",
    "# Note this is NOT an efficient way to search on google. This is done simply for education purposes\n",
    "google_html = BeautifulSoup(requests.get(f'https://www.google.com/search?q={PERSON}').text).get_text()[:512]\n",
    "\n",
    "pipe(f'Who is {PERSON}?', google_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54c484d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# From Huggingface: https://huggingface.co/bert-large-uncased-whole-word-masking-finetuned-squad\n",
    "\n",
    "squad_pipe = pipeline(\"question-answering\", \"bert-large-uncased-whole-word-masking-finetuned-squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b73ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "squad_pipe(\"Where is Sinan living these days?\", \"Sinan lives in California but Matt lives in Boston.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3801222a",
   "metadata": {},
   "outputs": [],
   "source": [
    "squad_pipe(\"What survey led to the founding of Princeton?\", princeton)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bee15f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# visualize logits\n",
    "large_tokenizer = AutoTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "\n",
    "qa_input = large_tokenizer(  # tokenize our example\n",
    "    \"What survey led to the founding of Princeton?\", princeton,\n",
    "    return_tensors='pt'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa803db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "large_qa_bert = AutoModelForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "\n",
    "output = large_qa_bert(**qa_input)  # pass the input through our QA model\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fa3bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_labels = large_tokenizer.convert_ids_to_tokens(qa_input['input_ids'].squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf6c4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot start and end logits for our fine-tuned model\n",
    "\n",
    "sns.set(rc={\"figure.figsize\":(20, 5)}) \n",
    "\n",
    "# Create a barplot showing the start word score for all of the tokens.\n",
    "ax = sns.barplot(x=[f'{i} - {t}' for i, t in enumerate(token_labels)], y=output.start_logits.squeeze().tolist(), ci=None)\n",
    "# Turn the xlabels vertical.\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90, ha=\"center\")\n",
    "# Turn on the vertical grid to help align words to scores.\n",
    "plt.title('Start Word Logits')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Create a barplot showing the start word score for all of the tokens.\n",
    "ax = sns.barplot(x=[f'{i} - {t}' for i, t in enumerate(token_labels)], y=output.end_logits.squeeze().tolist(), ci=None)\n",
    "# Turn the xlabels vertical.\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90, ha=\"center\")\n",
    "# Turn on the vertical grid to help align words to scores.\n",
    "plt.title('End Word Logits')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8804d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5ff9ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
