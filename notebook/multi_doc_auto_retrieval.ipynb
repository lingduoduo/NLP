{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "010a246d-56ec-44b6-98a1-cca3723d784c",
   "metadata": {
    "id": "010a246d-56ec-44b6-98a1-cca3723d784c"
   },
   "source": [
    "# Structured Hierarchical Retrieval\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/examples/query_engine/multi_doc_auto_retrieval/multi_doc_auto_retrieval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "Doing RAG well over multiple documents is hard. A general framework is given a user query, first select the relevant documents before selecting the content inside.\n",
    "\n",
    "But selecting the documents can be tough - how can we dynamically select documents based on different properties depending on the user query?\n",
    "\n",
    "In this notebook we show you our multi-document RAG architecture:\n",
    "\n",
    "- Represent each document as a concise **metadata** dictionary containing different properties: an extracted summary along with structured metadata.\n",
    "- Store this metadata dictionary as filters within a vector database.\n",
    "- Given a user query, first do **auto-retrieval** - infer the relevant semantic query and the set of filters to query this data (effectively combining text-to-SQL and semantic search)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d4bb83",
   "metadata": {
    "id": "b1d4bb83"
   },
   "outputs": [],
   "source": [
    "%pip install llama-index-readers-github\n",
    "%pip install llama-index-vector-stores-weaviate\n",
    "%pip install llama-index-llms-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a599ce1-48b1-44f6-846e-b9d3463635fd",
   "metadata": {
    "id": "7a599ce1-48b1-44f6-846e-b9d3463635fd"
   },
   "outputs": [],
   "source": [
    "!pip install llama-index llama-hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8ab861-ea33-4057-8634-b3529c577d29",
   "metadata": {
    "id": "da8ab861-ea33-4057-8634-b3529c577d29"
   },
   "source": [
    "## Setup and Download Data\n",
    "\n",
    "In this section, we'll load in LlamaIndex Github issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b49dc9d9-668a-4046-b86a-0ab39d74c9a8",
   "metadata": {
    "id": "b49dc9d9-668a-4046-b86a-0ab39d74c9a8"
   },
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12776eb2-c61d-4661-89a5-b7d3be2d8c93",
   "metadata": {
    "id": "12776eb2-c61d-4661-89a5-b7d3be2d8c93"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"GITHUB_TOKEN\"] = os.getenv('GITHUB_TOKEN')\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')\n",
    "os.environ[\"WEAVIATE_API_KEY\"] = os.getenv('WEAVIATE_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a33a74cb-413c-4827-aabf-5744ecb5c37f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100 issues in the repo page 1\n",
      "Resulted in 100 documents\n",
      "Found 100 issues in the repo page 2\n",
      "Resulted in 200 documents\n",
      "Found 100 issues in the repo page 3\n",
      "Resulted in 300 documents\n",
      "Found 100 issues in the repo page 4\n",
      "Resulted in 400 documents\n",
      "Found 100 issues in the repo page 5\n",
      "Resulted in 500 documents\n",
      "Found 8 issues in the repo page 6\n",
      "Resulted in 508 documents\n",
      "No more issues found, stopping\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from llama_index.readers.github import (\n",
    "    GitHubRepositoryIssuesReader,\n",
    "    GitHubIssuesClient,\n",
    ")\n",
    "\n",
    "github_client = GitHubIssuesClient()\n",
    "loader = GitHubRepositoryIssuesReader(\n",
    "    github_client,\n",
    "    owner=\"run-llama\",\n",
    "    repo=\"llama_index\",\n",
    "    verbose=True,\n",
    ")\n",
    "orig_docs = loader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c423120f-0dfe-4e77-aee5-3325c5d1442e",
   "metadata": {
    "id": "c423120f-0dfe-4e77-aee5-3325c5d1442e",
    "outputId": "3a52458e-5fa7-4dbc-c002-bec61e9dd8d0"
   },
   "outputs": [],
   "source": [
    "limit = 100\n",
    "\n",
    "docs = []\n",
    "for idx, doc in enumerate(orig_docs):\n",
    "    doc.metadata[\"index_id\"] = int(doc.id_)\n",
    "    if idx >= limit:\n",
    "        break\n",
    "    docs.append(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f142129c",
   "metadata": {
    "id": "f142129c"
   },
   "source": [
    "## Setup the Vector Store and Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f17643d",
   "metadata": {
    "id": "4f17643d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/linghuang/miniconda3/envs/llm-rag/lib/python3.10/site-packages/weaviate/warnings.py:121: DeprecationWarning: Dep005: You are using weaviate-client version 3.26.2. The latest version is 4.5.0.\n",
      "            Please consider upgrading to the latest version. See https://weaviate.io/developers/weaviate/client-libraries/python for details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import weaviate\n",
    "\n",
    "# cloud\n",
    "auth_config = weaviate.AuthApiKey(\n",
    "    api_key=os.environ[\"WEAVIATE_API_KEY\"]\n",
    ")\n",
    "client = weaviate.Client(\n",
    "    \"https://ling-sandbox-cluster-9222t71k.weaviate.network\",\n",
    "    auth_client_secret=auth_config,\n",
    ")\n",
    "\n",
    "class_name = \"LlamaIndex_docs\"\n",
    "\n",
    "\n",
    "# from langchain.embeddings import OpenAIEmbeddings\n",
    "# from langchain.vectorstores import Weaviate\n",
    "# import weaviate\n",
    "# from weaviate.embedded import EmbeddedOptions\n",
    "\n",
    "# client = weaviate.Client(  embedded_options = EmbeddedOptions())\n",
    "# vectorstore = Weaviate.from_documents(    \n",
    "#     client = client,        \n",
    "#     documents = chunks,    \n",
    "#     embedding = OpenAIEmbeddings(),    \n",
    "#     by_text = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75615d58",
   "metadata": {
    "id": "75615d58"
   },
   "outputs": [],
   "source": [
    "# optional: delete schema\n",
    "client.schema.delete_class(class_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b67acd54",
   "metadata": {
    "id": "b67acd54"
   },
   "outputs": [],
   "source": [
    "from llama_index.vector_stores.weaviate import WeaviateVectorStore\n",
    "from llama_index.core import VectorStoreIndex, StorageContext\n",
    "\n",
    "vector_store = WeaviateVectorStore(\n",
    "    weaviate_client=client, index_name=class_name\n",
    ")\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82d8ffb5",
   "metadata": {
    "id": "82d8ffb5"
   },
   "outputs": [],
   "source": [
    "doc_index = VectorStoreIndex.from_documents(\n",
    "    docs, storage_context=storage_context\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb80399",
   "metadata": {
    "id": "4cb80399"
   },
   "source": [
    "## Create IndexNodes for retrieval and filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee7f89b0-4c55-4bfe-83c8-8539b7939de2",
   "metadata": {
    "id": "ee7f89b0-4c55-4bfe-83c8-8539b7939de2"
   },
   "outputs": [],
   "source": [
    "from llama_index.core import SummaryIndex\n",
    "from llama_index.core.async_utils import run_jobs\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.schema import IndexNode\n",
    "from llama_index.core.vector_stores import (\n",
    "    FilterOperator,\n",
    "    MetadataFilter,\n",
    "    MetadataFilters,\n",
    ")\n",
    "\n",
    "\n",
    "async def aprocess_doc(doc, include_summary: bool = True):\n",
    "    \"\"\"Process doc.\"\"\"\n",
    "    metadata = doc.metadata\n",
    "\n",
    "    date_tokens = metadata[\"created_at\"].split(\"T\")[0].split(\"-\")\n",
    "    year = int(date_tokens[0])\n",
    "    month = int(date_tokens[1])\n",
    "    day = int(date_tokens[2])\n",
    "\n",
    "    assignee = (\n",
    "        \"\" if \"assignee\" not in doc.metadata else doc.metadata[\"assignee\"]\n",
    "    )\n",
    "    size = \"\"\n",
    "    if len(doc.metadata[\"labels\"]) > 0:\n",
    "        size_arr = [l for l in doc.metadata[\"labels\"] if \"size:\" in l]\n",
    "        size = size_arr[0].split(\":\")[1] if len(size_arr) > 0 else \"\"\n",
    "    new_metadata = {\n",
    "        \"state\": metadata[\"state\"],\n",
    "        \"year\": year,\n",
    "        \"month\": month,\n",
    "        \"day\": day,\n",
    "        \"assignee\": assignee,\n",
    "        \"size\": size,\n",
    "    }\n",
    "\n",
    "    # now extract out summary\n",
    "    summary_index = SummaryIndex.from_documents([doc])\n",
    "    query_str = \"Give a one-sentence concise summary of this issue.\"\n",
    "    query_engine = summary_index.as_query_engine(\n",
    "        llm=OpenAI(model=\"gpt-3.5-turbo\")\n",
    "    )\n",
    "    summary_txt = await query_engine.aquery(query_str)\n",
    "    summary_txt = str(summary_txt)\n",
    "\n",
    "    index_id = doc.metadata[\"index_id\"]\n",
    "    # filter for the specific doc id\n",
    "    filters = MetadataFilters(\n",
    "        filters=[\n",
    "            MetadataFilter(\n",
    "                key=\"index_id\", operator=FilterOperator.EQ, value=int(index_id)\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # create an index node using the summary text\n",
    "    index_node = IndexNode(\n",
    "        text=summary_txt,\n",
    "        metadata=new_metadata,\n",
    "        obj=doc_index.as_retriever(filters=filters),\n",
    "        index_id=doc.id_,\n",
    "    )\n",
    "\n",
    "    return index_node\n",
    "\n",
    "\n",
    "async def aprocess_docs(docs):\n",
    "    \"\"\"Process metadata on docs.\"\"\"\n",
    "\n",
    "    index_nodes = []\n",
    "    tasks = []\n",
    "    for doc in docs:\n",
    "        task = aprocess_doc(doc)\n",
    "        tasks.append(task)\n",
    "\n",
    "    index_nodes = await run_jobs(tasks, show_progress=True, workers=3)\n",
    "\n",
    "    return index_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "17728251-e7c8-47eb-b139-ee0a7246f894",
   "metadata": {
    "id": "17728251-e7c8-47eb-b139-ee0a7246f894",
    "outputId": "346f1099-854f-4c57-c3b7-0a90b3097e08"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█████████████████████████████▉                                                                                                                                                  | 17/100 [00:13<01:19,  1.05it/s]/Users/linghuang/miniconda3/envs/llm-rag/lib/python3.10/genericpath.py:77: ResourceWarning: unclosed <socket.socket fd=107, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.154', 49761), raddr=('104.18.6.192', 443)>\n",
      "  m = tuple(map(os.fspath, m))\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-LXJdh8mBCdaeYl79io2xzzqI on tokens per min (TPM): Limit 60000, Used 59605, Requested 2612. Please try again in 2.217s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m index_nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m aprocess_docs(docs)\n",
      "Cell \u001b[0;32mIn[13], line 76\u001b[0m, in \u001b[0;36maprocess_docs\u001b[0;34m(docs)\u001b[0m\n\u001b[1;32m     73\u001b[0m     task \u001b[38;5;241m=\u001b[39m aprocess_doc(doc)\n\u001b[1;32m     74\u001b[0m     tasks\u001b[38;5;241m.\u001b[39mappend(task)\n\u001b[0;32m---> 76\u001b[0m index_nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m run_jobs(tasks, show_progress\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m index_nodes\n",
      "File \u001b[0;32m~/miniconda3/envs/llm-rag/lib/python3.10/site-packages/llama_index/core/async_utils.py:110\u001b[0m, in \u001b[0;36mrun_jobs\u001b[0;34m(jobs, show_progress, workers)\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m job\n\u001b[1;32m    108\u001b[0m pool_jobs \u001b[38;5;241m=\u001b[39m [worker(job) \u001b[38;5;28;01mfor\u001b[39;00m job \u001b[38;5;129;01min\u001b[39;00m jobs]\n\u001b[0;32m--> 110\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m asyncio_mod\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m*\u001b[39mpool_jobs)\n",
      "File \u001b[0;32m~/miniconda3/envs/llm-rag/lib/python3.10/site-packages/tqdm/asyncio.py:79\u001b[0m, in \u001b[0;36mtqdm_asyncio.gather\u001b[0;34m(cls, loop, timeout, total, *fs, **tqdm_kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m i, \u001b[38;5;28;01mawait\u001b[39;00m f\n\u001b[1;32m     78\u001b[0m ifs \u001b[38;5;241m=\u001b[39m [wrap_awaitable(i, f) \u001b[38;5;28;01mfor\u001b[39;00m i, f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(fs)]\n\u001b[0;32m---> 79\u001b[0m res \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mawait\u001b[39;00m f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mas_completed(ifs, loop\u001b[38;5;241m=\u001b[39mloop, timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m     80\u001b[0m                                          total\u001b[38;5;241m=\u001b[39mtotal, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtqdm_kwargs)]\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [i \u001b[38;5;28;01mfor\u001b[39;00m _, i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(res)]\n",
      "File \u001b[0;32m~/miniconda3/envs/llm-rag/lib/python3.10/site-packages/tqdm/asyncio.py:79\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m i, \u001b[38;5;28;01mawait\u001b[39;00m f\n\u001b[1;32m     78\u001b[0m ifs \u001b[38;5;241m=\u001b[39m [wrap_awaitable(i, f) \u001b[38;5;28;01mfor\u001b[39;00m i, f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(fs)]\n\u001b[0;32m---> 79\u001b[0m res \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mawait\u001b[39;00m f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mas_completed(ifs, loop\u001b[38;5;241m=\u001b[39mloop, timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m     80\u001b[0m                                          total\u001b[38;5;241m=\u001b[39mtotal, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtqdm_kwargs)]\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [i \u001b[38;5;28;01mfor\u001b[39;00m _, i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(res)]\n",
      "File \u001b[0;32m~/miniconda3/envs/llm-rag/lib/python3.10/asyncio/tasks.py:571\u001b[0m, in \u001b[0;36mas_completed.<locals>._wait_for_one\u001b[0;34m()\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;66;03m# Dummy value from _on_timeout().\u001b[39;00m\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTimeoutError\n\u001b[0;32m--> 571\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm-rag/lib/python3.10/asyncio/futures.py:201\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__log_traceback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 201\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception_tb)\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[0;32m~/miniconda3/envs/llm-rag/lib/python3.10/asyncio/tasks.py:232\u001b[0m, in \u001b[0;36mTask.__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    230\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[1;32m    231\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[0;32m--> 232\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    234\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
      "File \u001b[0;32m~/miniconda3/envs/llm-rag/lib/python3.10/site-packages/tqdm/asyncio.py:76\u001b[0m, in \u001b[0;36mtqdm_asyncio.gather.<locals>.wrap_awaitable\u001b[0;34m(i, f)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrap_awaitable\u001b[39m(i, f):\n\u001b[0;32m---> 76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m i, \u001b[38;5;28;01mawait\u001b[39;00m f\n",
      "File \u001b[0;32m~/miniconda3/envs/llm-rag/lib/python3.10/site-packages/llama_index/core/async_utils.py:106\u001b[0m, in \u001b[0;36mrun_jobs.<locals>.worker\u001b[0;34m(job)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mworker\u001b[39m(job: Coroutine) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m semaphore:\n\u001b[0;32m--> 106\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m job\n",
      "Cell \u001b[0;32mIn[13], line 43\u001b[0m, in \u001b[0;36maprocess_doc\u001b[0;34m(doc, include_summary)\u001b[0m\n\u001b[1;32m     39\u001b[0m query_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGive a one-sentence concise summary of this issue.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     40\u001b[0m query_engine \u001b[38;5;241m=\u001b[39m summary_index\u001b[38;5;241m.\u001b[39mas_query_engine(\n\u001b[1;32m     41\u001b[0m     llm\u001b[38;5;241m=\u001b[39mOpenAI(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-3.5-turbo\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     42\u001b[0m )\n\u001b[0;32m---> 43\u001b[0m summary_txt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m query_engine\u001b[38;5;241m.\u001b[39maquery(query_str)\n\u001b[1;32m     44\u001b[0m summary_txt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(summary_txt)\n\u001b[1;32m     46\u001b[0m index_id \u001b[38;5;241m=\u001b[39m doc\u001b[38;5;241m.\u001b[39mmetadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/llm-rag/lib/python3.10/site-packages/llama_index/core/base/base_query_engine.py:46\u001b[0m, in \u001b[0;36mBaseQueryEngine.aquery\u001b[0;34m(self, str_or_query_bundle)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(str_or_query_bundle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m     45\u001b[0m     str_or_query_bundle \u001b[38;5;241m=\u001b[39m QueryBundle(str_or_query_bundle)\n\u001b[0;32m---> 46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aquery(str_or_query_bundle)\n",
      "File \u001b[0;32m~/miniconda3/envs/llm-rag/lib/python3.10/site-packages/llama_index/core/query_engine/retriever_query_engine.py:203\u001b[0m, in \u001b[0;36mRetrieverQueryEngine._aquery\u001b[0;34m(self, query_bundle)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mevent(\n\u001b[1;32m    199\u001b[0m     CBEventType\u001b[38;5;241m.\u001b[39mQUERY, payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mQUERY_STR: query_bundle\u001b[38;5;241m.\u001b[39mquery_str}\n\u001b[1;32m    200\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m query_event:\n\u001b[1;32m    201\u001b[0m     nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maretrieve(query_bundle)\n\u001b[0;32m--> 203\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_synthesizer\u001b[38;5;241m.\u001b[39masynthesize(\n\u001b[1;32m    204\u001b[0m         query\u001b[38;5;241m=\u001b[39mquery_bundle,\n\u001b[1;32m    205\u001b[0m         nodes\u001b[38;5;241m=\u001b[39mnodes,\n\u001b[1;32m    206\u001b[0m     )\n\u001b[1;32m    208\u001b[0m     query_event\u001b[38;5;241m.\u001b[39mon_end(payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mRESPONSE: response})\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/miniconda3/envs/llm-rag/lib/python3.10/site-packages/llama_index/core/response_synthesizers/base.py:221\u001b[0m, in \u001b[0;36mBaseSynthesizer.asynthesize\u001b[0;34m(self, query, nodes, additional_source_nodes, **response_kwargs)\u001b[0m\n\u001b[1;32m    216\u001b[0m     query \u001b[38;5;241m=\u001b[39m QueryBundle(query_str\u001b[38;5;241m=\u001b[39mquery)\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback_manager\u001b[38;5;241m.\u001b[39mevent(\n\u001b[1;32m    219\u001b[0m     CBEventType\u001b[38;5;241m.\u001b[39mSYNTHESIZE, payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mQUERY_STR: query\u001b[38;5;241m.\u001b[39mquery_str}\n\u001b[1;32m    220\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m event:\n\u001b[0;32m--> 221\u001b[0m     response_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maget_response(\n\u001b[1;32m    222\u001b[0m         query_str\u001b[38;5;241m=\u001b[39mquery\u001b[38;5;241m.\u001b[39mquery_str,\n\u001b[1;32m    223\u001b[0m         text_chunks\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m    224\u001b[0m             n\u001b[38;5;241m.\u001b[39mnode\u001b[38;5;241m.\u001b[39mget_content(metadata_mode\u001b[38;5;241m=\u001b[39mMetadataMode\u001b[38;5;241m.\u001b[39mLLM) \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m nodes\n\u001b[1;32m    225\u001b[0m         ],\n\u001b[1;32m    226\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kwargs,\n\u001b[1;32m    227\u001b[0m     )\n\u001b[1;32m    229\u001b[0m     additional_source_nodes \u001b[38;5;241m=\u001b[39m additional_source_nodes \u001b[38;5;129;01mor\u001b[39;00m []\n\u001b[1;32m    230\u001b[0m     source_nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(nodes) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(additional_source_nodes)\n",
      "File \u001b[0;32m~/miniconda3/envs/llm-rag/lib/python3.10/site-packages/llama_index/core/response_synthesizers/compact_and_refine.py:19\u001b[0m, in \u001b[0;36mCompactAndRefine.aget_response\u001b[0;34m(self, query_str, text_chunks, prev_response, **response_kwargs)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maget_response\u001b[39m(\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     13\u001b[0m     query_str: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kwargs: Any,\n\u001b[1;32m     17\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RESPONSE_TEXT_TYPE:\n\u001b[1;32m     18\u001b[0m     compact_texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_compact_text_chunks(query_str, text_chunks)\n\u001b[0;32m---> 19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39maget_response(\n\u001b[1;32m     20\u001b[0m         query_str\u001b[38;5;241m=\u001b[39mquery_str,\n\u001b[1;32m     21\u001b[0m         text_chunks\u001b[38;5;241m=\u001b[39mcompact_texts,\n\u001b[1;32m     22\u001b[0m         prev_response\u001b[38;5;241m=\u001b[39mprev_response,\n\u001b[1;32m     23\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kwargs,\n\u001b[1;32m     24\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/llm-rag/lib/python3.10/site-packages/llama_index/core/response_synthesizers/refine.py:338\u001b[0m, in \u001b[0;36mRefine.aget_response\u001b[0;34m(self, query_str, text_chunks, prev_response, **response_kwargs)\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text_chunk \u001b[38;5;129;01min\u001b[39;00m text_chunks:\n\u001b[1;32m    335\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m prev_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    336\u001b[0m         \u001b[38;5;66;03m# if this is the first chunk, and text chunk already\u001b[39;00m\n\u001b[1;32m    337\u001b[0m         \u001b[38;5;66;03m# is an answer, then return it\u001b[39;00m\n\u001b[0;32m--> 338\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agive_response_single(\n\u001b[1;32m    339\u001b[0m             query_str, text_chunk, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kwargs\n\u001b[1;32m    340\u001b[0m         )\n\u001b[1;32m    341\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    342\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_arefine_response_single(\n\u001b[1;32m    343\u001b[0m             prev_response, query_str, text_chunk, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kwargs\n\u001b[1;32m    344\u001b[0m         )\n",
      "File \u001b[0;32m~/miniconda3/envs/llm-rag/lib/python3.10/site-packages/llama_index/core/response_synthesizers/refine.py:439\u001b[0m, in \u001b[0;36mRefine._agive_response_single\u001b[0;34m(self, query_str, text_chunk, **response_kwargs)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_streaming:\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 439\u001b[0m         structured_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m program\u001b[38;5;241m.\u001b[39macall(\n\u001b[1;32m    440\u001b[0m             context_str\u001b[38;5;241m=\u001b[39mcur_text_chunk,\n\u001b[1;32m    441\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kwargs,\n\u001b[1;32m    442\u001b[0m         )\n\u001b[1;32m    443\u001b[0m         structured_response \u001b[38;5;241m=\u001b[39m cast(\n\u001b[1;32m    444\u001b[0m             StructuredRefineResponse, structured_response\n\u001b[1;32m    445\u001b[0m         )\n\u001b[1;32m    446\u001b[0m         query_satisfied \u001b[38;5;241m=\u001b[39m structured_response\u001b[38;5;241m.\u001b[39mquery_satisfied\n",
      "File \u001b[0;32m~/miniconda3/envs/llm-rag/lib/python3.10/site-packages/llama_index/core/response_synthesizers/refine.py:83\u001b[0m, in \u001b[0;36mDefaultRefineProgram.acall\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m     81\u001b[0m     answer \u001b[38;5;241m=\u001b[39m answer\u001b[38;5;241m.\u001b[39mjson()\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 83\u001b[0m     answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_llm\u001b[38;5;241m.\u001b[39mapredict(\n\u001b[1;32m     84\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prompt,\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds,\n\u001b[1;32m     86\u001b[0m     )\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m StructuredRefineResponse(answer\u001b[38;5;241m=\u001b[39manswer, query_satisfied\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/llm-rag/lib/python3.10/site-packages/llama_index/core/llms/llm.py:294\u001b[0m, in \u001b[0;36mLLM.apredict\u001b[0;34m(self, prompt, **prompt_args)\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mis_chat_model:\n\u001b[1;32m    293\u001b[0m     messages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_messages(prompt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mprompt_args)\n\u001b[0;32m--> 294\u001b[0m     chat_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39machat(messages)\n\u001b[1;32m    295\u001b[0m     output \u001b[38;5;241m=\u001b[39m chat_response\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/llm-rag/lib/python3.10/site-packages/llama_index/core/llms/callbacks.py:50\u001b[0m, in \u001b[0;36mllm_chat_callback.<locals>.wrap.<locals>.wrapped_async_llm_chat\u001b[0;34m(_self, messages, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m wrapper_logic(_self) \u001b[38;5;28;01mas\u001b[39;00m callback_manager:\n\u001b[1;32m     41\u001b[0m     event_id \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_event_start(\n\u001b[1;32m     42\u001b[0m         CBEventType\u001b[38;5;241m.\u001b[39mLLM,\n\u001b[1;32m     43\u001b[0m         payload\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     47\u001b[0m         },\n\u001b[1;32m     48\u001b[0m     )\n\u001b[0;32m---> 50\u001b[0m     f_return_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m f(_self, messages, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(f_return_val, AsyncGenerator):\n\u001b[1;32m     52\u001b[0m         \u001b[38;5;66;03m# intercept the generator and add a callback to the end\u001b[39;00m\n\u001b[1;32m     53\u001b[0m         \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_gen\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatResponseAsyncGen:\n",
      "File \u001b[0;32m~/miniconda3/envs/llm-rag/lib/python3.10/site-packages/llama_index/llms/openai/base.py:494\u001b[0m, in \u001b[0;36mOpenAI.achat\u001b[0;34m(self, messages, **kwargs)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    493\u001b[0m     achat_fn \u001b[38;5;241m=\u001b[39m acompletion_to_chat_decorator(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_acomplete)\n\u001b[0;32m--> 494\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m achat_fn(messages, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/llm-rag/lib/python3.10/site-packages/llama_index/llms/openai/base.py:538\u001b[0m, in \u001b[0;36mOpenAI._achat\u001b[0;34m(self, messages, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m aclient \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_aclient()\n\u001b[1;32m    537\u001b[0m message_dicts \u001b[38;5;241m=\u001b[39m to_openai_message_dicts(messages)\n\u001b[0;32m--> 538\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m aclient\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m    539\u001b[0m     messages\u001b[38;5;241m=\u001b[39mmessage_dicts, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_model_kwargs(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    540\u001b[0m )\n\u001b[1;32m    541\u001b[0m message_dict \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\n\u001b[1;32m    542\u001b[0m message \u001b[38;5;241m=\u001b[39m from_openai_message(message_dict)\n",
      "File \u001b[0;32m~/miniconda3/envs/llm-rag/lib/python3.10/site-packages/openai/resources/chat/completions.py:1330\u001b[0m, in \u001b[0;36mAsyncCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   1278\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m   1279\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m   1280\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1328\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m   1329\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m AsyncStream[ChatCompletionChunk]:\n\u001b[0;32m-> 1330\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[1;32m   1331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1332\u001b[0m         body\u001b[38;5;241m=\u001b[39mmaybe_transform(\n\u001b[1;32m   1333\u001b[0m             {\n\u001b[1;32m   1334\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[1;32m   1335\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[1;32m   1336\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[1;32m   1337\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[1;32m   1338\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[1;32m   1339\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[1;32m   1340\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[1;32m   1341\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[1;32m   1342\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[1;32m   1343\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[1;32m   1344\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[1;32m   1345\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[1;32m   1346\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[1;32m   1347\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[1;32m   1348\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[1;32m   1349\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[1;32m   1350\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[1;32m   1351\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[1;32m   1352\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[1;32m   1353\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[1;32m   1354\u001b[0m             },\n\u001b[1;32m   1355\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParams,\n\u001b[1;32m   1356\u001b[0m         ),\n\u001b[1;32m   1357\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[1;32m   1358\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m   1359\u001b[0m         ),\n\u001b[1;32m   1360\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[1;32m   1361\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1362\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mAsyncStream[ChatCompletionChunk],\n\u001b[1;32m   1363\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/llm-rag/lib/python3.10/site-packages/openai/_base_client.py:1725\u001b[0m, in \u001b[0;36mAsyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1711\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1712\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1713\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1720\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_AsyncStreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1721\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _AsyncStreamT:\n\u001b[1;32m   1722\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1723\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1724\u001b[0m     )\n\u001b[0;32m-> 1725\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls)\n",
      "File \u001b[0;32m~/miniconda3/envs/llm-rag/lib/python3.10/site-packages/openai/_base_client.py:1428\u001b[0m, in \u001b[0;36mAsyncAPIClient.request\u001b[0;34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[0m\n\u001b[1;32m   1419\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m   1420\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1421\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1426\u001b[0m     remaining_retries: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1427\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _AsyncStreamT:\n\u001b[0;32m-> 1428\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m   1429\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1430\u001b[0m         options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   1431\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1432\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1433\u001b[0m         remaining_retries\u001b[38;5;241m=\u001b[39mremaining_retries,\n\u001b[1;32m   1434\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/llm-rag/lib/python3.10/site-packages/openai/_base_client.py:1504\u001b[0m, in \u001b[0;36mAsyncAPIClient._request\u001b[0;34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[0m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m   1503\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39maclose()\n\u001b[0;32m-> 1504\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[1;32m   1505\u001b[0m         options,\n\u001b[1;32m   1506\u001b[0m         cast_to,\n\u001b[1;32m   1507\u001b[0m         retries,\n\u001b[1;32m   1508\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m   1509\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1510\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1511\u001b[0m     )\n\u001b[1;32m   1513\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/miniconda3/envs/llm-rag/lib/python3.10/site-packages/openai/_base_client.py:1550\u001b[0m, in \u001b[0;36mAsyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1546\u001b[0m log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrying request to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m, options\u001b[38;5;241m.\u001b[39murl, timeout)\n\u001b[1;32m   1548\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m anyio\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1550\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m   1551\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   1552\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1553\u001b[0m     remaining_retries\u001b[38;5;241m=\u001b[39mremaining,\n\u001b[1;32m   1554\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1555\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1556\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/llm-rag/lib/python3.10/site-packages/openai/_base_client.py:1504\u001b[0m, in \u001b[0;36mAsyncAPIClient._request\u001b[0;34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[0m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m   1503\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39maclose()\n\u001b[0;32m-> 1504\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[1;32m   1505\u001b[0m         options,\n\u001b[1;32m   1506\u001b[0m         cast_to,\n\u001b[1;32m   1507\u001b[0m         retries,\n\u001b[1;32m   1508\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m   1509\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1510\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1511\u001b[0m     )\n\u001b[1;32m   1513\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/miniconda3/envs/llm-rag/lib/python3.10/site-packages/openai/_base_client.py:1550\u001b[0m, in \u001b[0;36mAsyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1546\u001b[0m log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrying request to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m, options\u001b[38;5;241m.\u001b[39murl, timeout)\n\u001b[1;32m   1548\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m anyio\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1550\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m   1551\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   1552\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1553\u001b[0m     remaining_retries\u001b[38;5;241m=\u001b[39mremaining,\n\u001b[1;32m   1554\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1555\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1556\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/llm-rag/lib/python3.10/site-packages/openai/_base_client.py:1504\u001b[0m, in \u001b[0;36mAsyncAPIClient._request\u001b[0;34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[0m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m   1503\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39maclose()\n\u001b[0;32m-> 1504\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[1;32m   1505\u001b[0m         options,\n\u001b[1;32m   1506\u001b[0m         cast_to,\n\u001b[1;32m   1507\u001b[0m         retries,\n\u001b[1;32m   1508\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m   1509\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1510\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1511\u001b[0m     )\n\u001b[1;32m   1513\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/miniconda3/envs/llm-rag/lib/python3.10/site-packages/openai/_base_client.py:1550\u001b[0m, in \u001b[0;36mAsyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1546\u001b[0m log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrying request to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m, options\u001b[38;5;241m.\u001b[39murl, timeout)\n\u001b[1;32m   1548\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m anyio\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1550\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m   1551\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   1552\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1553\u001b[0m     remaining_retries\u001b[38;5;241m=\u001b[39mremaining,\n\u001b[1;32m   1554\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1555\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1556\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/llm-rag/lib/python3.10/site-packages/openai/_base_client.py:1519\u001b[0m, in \u001b[0;36mAsyncAPIClient._request\u001b[0;34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[0m\n\u001b[1;32m   1516\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39maread()\n\u001b[1;32m   1518\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1519\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1521\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1522\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1523\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1526\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1527\u001b[0m )\n",
      "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-LXJdh8mBCdaeYl79io2xzzqI on tokens per min (TPM): Limit 60000, Used 59605, Requested 2612. Please try again in 2.217s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/linghuang/miniconda3/envs/llm-rag/lib/python3.10/typing.py:329: ResourceWarning: unclosed <socket.socket fd=137, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.154', 49797), raddr=('104.18.6.192', 443)>\n",
      "  ev_args = tuple(_eval_type(a, globalns, localns, recursive_guard) for a in t.__args__)\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/Users/linghuang/miniconda3/envs/llm-rag/lib/python3.10/typing.py:329: ResourceWarning: unclosed <socket.socket fd=148, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.154', 49798), raddr=('104.18.6.192', 443)>\n",
      "  ev_args = tuple(_eval_type(a, globalns, localns, recursive_guard) for a in t.__args__)\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/Users/linghuang/miniconda3/envs/llm-rag/lib/python3.10/typing.py:329: ResourceWarning: unclosed <socket.socket fd=173, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.154', 49807), raddr=('104.18.6.192', 443)>\n",
      "  ev_args = tuple(_eval_type(a, globalns, localns, recursive_guard) for a in t.__args__)\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/Users/linghuang/miniconda3/envs/llm-rag/lib/python3.10/typing.py:329: ResourceWarning: unclosed <socket.socket fd=167, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.154', 49816), raddr=('104.18.6.192', 443)>\n",
      "  ev_args = tuple(_eval_type(a, globalns, localns, recursive_guard) for a in t.__args__)\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/Users/linghuang/miniconda3/envs/llm-rag/lib/python3.10/site-packages/httpcore/_async/connection_pool.py:265: ResourceWarning: unclosed <socket.socket fd=174, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.154', 49829), raddr=('104.18.6.192', 443)>\n",
      "  avilable_connections = [\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/Users/linghuang/miniconda3/envs/llm-rag/lib/python3.10/asyncio/base_events.py:736: ResourceWarning: unclosed <socket.socket fd=173, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('192.168.1.154', 49838), raddr=('104.18.6.192', 443)>\n",
      "  timer = events.TimerHandle(when, callback, args, self, context)\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "index_nodes = await aprocess_docs(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c6907607-47b7-4966-9501-6c5320ec66e5",
   "metadata": {
    "id": "c6907607-47b7-4966-9501-6c5320ec66e5",
    "outputId": "1735704f-667d-4c07-bf40-a447e89467ec"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'index_nodes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mindex_nodes\u001b[49m[\u001b[38;5;241m5\u001b[39m]\u001b[38;5;241m.\u001b[39mmetadata\n",
      "\u001b[0;31mNameError\u001b[0m: name 'index_nodes' is not defined"
     ]
    }
   ],
   "source": [
    "index_nodes[5].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773585a7-3027-4a12-9349-7320822514e0",
   "metadata": {
    "id": "773585a7-3027-4a12-9349-7320822514e0"
   },
   "source": [
    "## Create the Top-Level AutoRetriever\n",
    "\n",
    "We load both the summarized metadata as well as the original docs into the vector database.\n",
    "1. **Summarized Metadata**: This goes into the `LlamaIndex_auto` collection.\n",
    "2. **Original Docs**: This goes into the `LlamaIndex_docs` collection.\n",
    "\n",
    "By storing both the summarized metadata as well as the original documents, we can execute our structured, hierarchical retrieval strategies.\n",
    "\n",
    "We load into a vector database that supports auto-retrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c7baf6-4267-4543-8a2c-7b44a8fcd017",
   "metadata": {
    "id": "55c7baf6-4267-4543-8a2c-7b44a8fcd017"
   },
   "source": [
    "### Load Summarized Metadata\n",
    "\n",
    "This goes into `LlamaIndex_auto`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7fcbe94d-0fe4-48b1-954c-31d0f278ebc0",
   "metadata": {
    "id": "7fcbe94d-0fe4-48b1-954c-31d0f278ebc0"
   },
   "outputs": [],
   "source": [
    "import weaviate\n",
    "\n",
    "# cloud\n",
    "auth_config = weaviate.AuthApiKey(\n",
    "    api_key=os.environ[\"WEAVIATE_API_KEY\"]\n",
    ")\n",
    "client = weaviate.Client(\n",
    "    \"https://ling-sandbox-cluster-9222t71k.weaviate.network\",\n",
    "    auth_client_secret=auth_config,\n",
    ")\n",
    "\n",
    "class_name = \"LlamaIndex_docs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "782b544b-2e8e-4fde-a35f-fb77133a0cef",
   "metadata": {
    "id": "782b544b-2e8e-4fde-a35f-fb77133a0cef"
   },
   "outputs": [],
   "source": [
    "# optional: delete schema\n",
    "client.schema.delete_class(class_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dd78a330",
   "metadata": {
    "id": "dd78a330"
   },
   "outputs": [],
   "source": [
    "from llama_index.vector_stores.weaviate import WeaviateVectorStore\n",
    "from llama_index.core import VectorStoreIndex, StorageContext\n",
    "\n",
    "vector_store_auto = WeaviateVectorStore(\n",
    "    weaviate_client=client, index_name=class_name\n",
    ")\n",
    "storage_context_auto = StorageContext.from_defaults(\n",
    "    vector_store=vector_store_auto\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "87910444-2cfb-4c47-8821-83d679486700",
   "metadata": {
    "id": "87910444-2cfb-4c47-8821-83d679486700"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'index_nodes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Since \"index_nodes\" are concise summaries, we can directly feed them as objects into VectorStoreIndex\u001b[39;00m\n\u001b[1;32m      2\u001b[0m index \u001b[38;5;241m=\u001b[39m VectorStoreIndex(\n\u001b[0;32m----> 3\u001b[0m     objects\u001b[38;5;241m=\u001b[39m\u001b[43mindex_nodes\u001b[49m, storage_context\u001b[38;5;241m=\u001b[39mstorage_context_auto\n\u001b[1;32m      4\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'index_nodes' is not defined"
     ]
    }
   ],
   "source": [
    "# Since \"index_nodes\" are concise summaries, we can directly feed them as objects into VectorStoreIndex\n",
    "index = VectorStoreIndex(\n",
    "    objects=index_nodes, storage_context=storage_context_auto\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44433ac-3a4c-4bda-bb5b-6bbfd18c1ddf",
   "metadata": {
    "id": "f44433ac-3a4c-4bda-bb5b-6bbfd18c1ddf"
   },
   "source": [
    "## Setup Composable Auto-Retriever\n",
    "\n",
    "In this section we setup our auto-retriever. There's a few steps that we need to perform.\n",
    "\n",
    "1. **Define the Schema**: Define the vector db schema (e.g. the metadata fields). This will be put into the LLM input prompt when it's deciding what metadata filters to infer.\n",
    "2. **Instantiate the VectorIndexAutoRetriever class**: This creates a retriever on top of our summarized metadata index, and takes in the defined schema as input.\n",
    "3. **Define a wrapper retriever**: This allows us to postprocess each node into an `IndexNode`, with an index id linking back source document. This will allow us to do recursive retrieval in the next section (which depends on IndexNode objects linking to downstream retrievers/query engines/other Nodes). **NOTE**: We are working on improving this abstraction.\n",
    "\n",
    "Running this retriever will retrieve based on our text summaries and metadat of our top-level `IndeNode` objects. Then, their underlying retrievers will be used to retrieve content from the specific github issue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c926128-c1aa-4f42-8061-b3f2df1272d0",
   "metadata": {
    "id": "8c926128-c1aa-4f42-8061-b3f2df1272d0"
   },
   "source": [
    "### 1. Define the Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d01aade-d676-49db-b851-2ac62b4e53c5",
   "metadata": {
    "id": "5d01aade-d676-49db-b851-2ac62b4e53c5"
   },
   "outputs": [],
   "source": [
    "from llama_index.core.vector_stores import MetadataInfo, VectorStoreInfo\n",
    "\n",
    "\n",
    "vector_store_info = VectorStoreInfo(\n",
    "    content_info=\"Github Issues\",\n",
    "    metadata_info=[\n",
    "        MetadataInfo(\n",
    "            name=\"state\",\n",
    "            description=\"Whether the issue is `open` or `closed`\",\n",
    "            type=\"string\",\n",
    "        ),\n",
    "        MetadataInfo(\n",
    "            name=\"year\",\n",
    "            description=\"The year issue was created\",\n",
    "            type=\"integer\",\n",
    "        ),\n",
    "        MetadataInfo(\n",
    "            name=\"month\",\n",
    "            description=\"The month issue was created\",\n",
    "            type=\"integer\",\n",
    "        ),\n",
    "        MetadataInfo(\n",
    "            name=\"day\",\n",
    "            description=\"The day issue was created\",\n",
    "            type=\"integer\",\n",
    "        ),\n",
    "        MetadataInfo(\n",
    "            name=\"assignee\",\n",
    "            description=\"The assignee of the ticket\",\n",
    "            type=\"string\",\n",
    "        ),\n",
    "        MetadataInfo(\n",
    "            name=\"size\",\n",
    "            description=\"How big the issue is (XS, S, M, L, XL, XXL)\",\n",
    "            type=\"string\",\n",
    "        ),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a69271-e597-40c6-88aa-a755bfd75754",
   "metadata": {
    "id": "d6a69271-e597-40c6-88aa-a755bfd75754"
   },
   "source": [
    "### 2. Instantiate VectorIndexAutoRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9125a832-940a-44f3-be91-ad17cdfc267b",
   "metadata": {
    "id": "9125a832-940a-44f3-be91-ad17cdfc267b"
   },
   "outputs": [],
   "source": [
    "from llama_index.core.retrievers import VectorIndexAutoRetriever\n",
    "\n",
    "retriever = VectorIndexAutoRetriever(\n",
    "    index,\n",
    "    vector_store_info=vector_store_info,\n",
    "    similarity_top_k=2,\n",
    "    empty_query_top_k=10,  # if only metadata filters are specified, this is the limit\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a91a51-9a2c-447e-9848-53ee0a705baa",
   "metadata": {
    "id": "97a91a51-9a2c-447e-9848-53ee0a705baa"
   },
   "source": [
    "## Try It Out\n",
    "\n",
    "Now we can start retrieving relevant context over Github Issues!\n",
    "\n",
    "To complete the RAG pipeline setup we'll combine our recursive retriever with our `RetrieverQueryEngine` to generate a response in addition to the retrieved nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2032ef-e776-49d8-b8d4-f8b79fbd3599",
   "metadata": {
    "id": "6a2032ef-e776-49d8-b8d4-f8b79fbd3599"
   },
   "source": [
    "### Try Out Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7136a40b-c615-4f32-bc98-fdc3f72f3085",
   "metadata": {
    "id": "7136a40b-c615-4f32-bc98-fdc3f72f3085",
    "outputId": "4aaa91ab-42f7-4c44-daa2-10f0da1e91c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using query str: issues\n",
      "Using filters: [('day', '==', '11'), ('month', '==', '01')]\n",
      "\u001b[1;3;38;2;11;159;203mRetrieval entering 9995: VectorIndexRetriever\n",
      "\u001b[0m\u001b[1;3;38;2;237;90;200mRetrieving from object VectorIndexRetriever with query issues\n",
      "\u001b[0m\u001b[1;3;38;2;11;159;203mRetrieval entering 9985: VectorIndexRetriever\n",
      "\u001b[0m\u001b[1;3;38;2;237;90;200mRetrieving from object VectorIndexRetriever with query issues\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "from llama_index.core import QueryBundle\n",
    "\n",
    "nodes = retriever.retrieve(QueryBundle(\"Tell me about some issues on 01/11\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002f3135-6b88-45fa-a25d-1b4e7e977489",
   "metadata": {
    "id": "002f3135-6b88-45fa-a25d-1b4e7e977489"
   },
   "source": [
    "The result is the source chunks in the relevant docs.\n",
    "\n",
    "Let's look at the date attached to the source chunk (was present in the original metadata)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3804c74f-2bb5-4935-b15b-ff16ce0a7475",
   "metadata": {
    "id": "3804c74f-2bb5-4935-b15b-ff16ce0a7475",
    "outputId": "52b8be42-af2b-4065-d30f-f22610b5681b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of source nodes: 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'state': 'open',\n",
       " 'created_at': '2024-01-11T20:37:34Z',\n",
       " 'url': 'https://api.github.com/repos/run-llama/llama_index/issues/9995',\n",
       " 'source': 'https://github.com/run-llama/llama_index/pull/9995',\n",
       " 'labels': ['size:XXL'],\n",
       " 'index_id': 9995}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Number of source nodes: {len(nodes)}\")\n",
    "nodes[0].node.metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cba7dd-b5f5-4759-9dcf-b9ee06a7ec29",
   "metadata": {
    "id": "97cba7dd-b5f5-4759-9dcf-b9ee06a7ec29"
   },
   "source": [
    "### Plug into `RetrieverQueryEngine`\n",
    "\n",
    "We plug into RetrieverQueryEngine to synthesize a result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bdaf13-71b4-43c4-b49c-8c9a109819f7",
   "metadata": {
    "id": "c9bdaf13-71b4-43c4-b49c-8c9a109819f7"
   },
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "query_engine = RetrieverQueryEngine.from_args(retriever, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d52659-c297-458c-aba6-496995655ba2",
   "metadata": {
    "id": "10d52659-c297-458c-aba6-496995655ba2",
    "outputId": "b3df059a-bdde-4718-9294-45781c09e906"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using query str: issues\n",
      "Using filters: [('day', '==', '11'), ('month', '==', '01')]\n",
      "\u001b[1;3;38;2;11;159;203mRetrieval entering 9995: VectorIndexRetriever\n",
      "\u001b[0m\u001b[1;3;38;2;237;90;200mRetrieving from object VectorIndexRetriever with query issues\n",
      "\u001b[0m\u001b[1;3;38;2;11;159;203mRetrieval entering 9985: VectorIndexRetriever\n",
      "\u001b[0m\u001b[1;3;38;2;237;90;200mRetrieving from object VectorIndexRetriever with query issues\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"Tell me about some issues on 01/11\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f591dae0-0438-4018-9732-c5aa9357938a",
   "metadata": {
    "id": "f591dae0-0438-4018-9732-c5aa9357938a",
    "outputId": "f683ec2a-d3bd-4812-e106-4b2ff78e0ffb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are two issues that were created on 01/11. The first issue is related to ensuring backwards compatibility with the new Pinecone client version bifurcation. The second issue is a feature request to implement the Language Agent Tree Search (LATS) agent in llama-index.\n"
     ]
    }
   ],
   "source": [
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6450df09-f8c4-4dee-aa10-6c85c0ea362b",
   "metadata": {
    "id": "6450df09-f8c4-4dee-aa10-6c85c0ea362b",
    "outputId": "a8082cfd-5259-4bf5-901c-d101cc13aef6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using query str: agents\n",
      "Using filters: [('state', '==', 'open')]\n",
      "\u001b[1;3;38;2;11;159;203mRetrieval entering 10058: VectorIndexRetriever\n",
      "\u001b[0m\u001b[1;3;38;2;237;90;200mRetrieving from object VectorIndexRetriever with query agents\n",
      "\u001b[0m\u001b[1;3;38;2;11;159;203mRetrieval entering 9899: VectorIndexRetriever\n",
      "\u001b[0m\u001b[1;3;38;2;237;90;200mRetrieving from object VectorIndexRetriever with query agents\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\n",
    "    \"Tell me about some open issues related to agents\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0d36e2-f4c4-4cca-a4b9-ddd4f6d2e9a1",
   "metadata": {
    "id": "cb0d36e2-f4c4-4cca-a4b9-ddd4f6d2e9a1",
    "outputId": "97daf242-a6a1-45d6-fa98-ae80801ff5f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are two open issues related to agents. One issue is about adding context for agents, updating a stale link, and adding a notebook to demo a react agent with context. The other issue is a feature request for parallelism when using the top agent from a multi-document agent while comparing multiple documents.\n"
     ]
    }
   ],
   "source": [
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230a7945-0a77-4d0c-a324-f24e2398c146",
   "metadata": {
    "id": "230a7945-0a77-4d0c-a324-f24e2398c146"
   },
   "source": [
    "## Concluding Thoughts\n",
    "\n",
    "This shows you how to create a structured retrieval layer over your document summaries, allowing you to dynamically pull in the relevant documents based on the user query.\n",
    "\n",
    "You may notice similarities between this and our [multi-document agents](https://docs.llamaindex.ai/en/stable/examples/agent/multi_document_agents.html). Both architectures are aimed for powerful multi-document retrieval.\n",
    "\n",
    "The goal of this notebook is to show you how to apply structured querying in a multi-document setting. You can actually apply this auto-retrieval algorithm to our multi-agent setup too. The multi-agent setup is primarily focused on adding agentic reasoning across documents and per documents, alloinwg multi-part queries using chain-of-thought."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
