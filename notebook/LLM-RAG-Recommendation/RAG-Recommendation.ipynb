{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5bb05de-41f8-48f9-b450-926a3a491b8f",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddc8af4e-7b5a-4727-aeb5-8c1ba7197c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['user_id', 'movie_id', 'rating', 'timestamp']\n",
    "train_data = pd.read_csv(\"ml-100k/u1.base\",sep='\\t', names=cols).drop(columns=['timestamp']).astype(int)\n",
    "test_data = pd.read_csv(\"ml-100k/u1.test\",sep='\\t', names=cols).drop(columns=['timestamp']).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f90c14fb-59b3-459c-9202-6d27b73bab79",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_users, n_items = 943,1682"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cfff9e86-ed2a-4c82-a672-e66f9632894f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  movie_id  rating\n",
       "0        1         1       5\n",
       "1        1         2       3\n",
       "2        1         3       4\n",
       "3        1         4       3\n",
       "4        1         5       3"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ea8f45c1-0873-4702-bfcc-a5fc1329ca21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Matrixfactorization(nn.Module):\n",
    "    def __init__(self, n_users, n_items, n_factors=20):\n",
    "        super().__init__()\n",
    "        self.user_factors = nn.Embedding( n_users,n_factors)\n",
    "        self.item_factors = nn.Embedding( n_items,n_factors)\n",
    "#         self.l1 = nn.Linear(n_factors*2,16)\n",
    "#         self.l2 = nn.Linear(16,1)\n",
    "#         self.drop = nn.Dropout()\n",
    "        \n",
    "    def forward(self, user, item):\n",
    "        user = torch.LongTensor(user) - 1\n",
    "        item = torch.LongTensor(item) - 1\n",
    "        u,it = self.user_factors(user),self.item_factors(item)\n",
    "#         x = torch.cat([u,it],dim=1)\n",
    "#         x = F.relu(self.l1(x))\n",
    "#         x = self.drop(x)\n",
    "#         x = self.l2(x)\n",
    "#         return x\n",
    "        x = (u*it).sum(1)\n",
    "        assert x.shape==user.shape\n",
    "        return x * 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e1715271-7c77-45a2-9eea-4f0783465af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Matrixfactorization(n_users,n_items)\n",
    "opt = Adam(model.parameters(),lr=1e-3)\n",
    "criterion = nn.L1Loss()\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "19ed781b-cc7c-4393-a05a-9c48be2ddb0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 16.061074742507934\n",
      "Epoch 2: 12.612734846878052\n",
      "Epoch 3: 9.837263370323182\n",
      "Epoch 4: 7.819109057235718\n",
      "Epoch 5: 6.2363761887550355\n",
      "Epoch 6: 5.1391187036514285\n",
      "Epoch 7: 4.314232008361817\n",
      "Epoch 8: 3.5944116067886354\n",
      "Epoch 9: 2.9559540061473846\n",
      "Epoch 10: 2.3333496863365175\n",
      "Epoch 11: 1.8750979673147201\n",
      "Epoch 12: 1.5423662888288499\n",
      "Epoch 13: 1.3099110254526138\n",
      "Epoch 14: 1.143747090792656\n",
      "Epoch 15: 1.0276083385705947\n",
      "Epoch 16: 0.9569112715601921\n",
      "Epoch 17: 0.8840741533756256\n",
      "Epoch 18: 0.8514112945437431\n",
      "Epoch 19: 0.8070766439795494\n",
      "Epoch 20: 0.7789720355033874\n"
     ]
    }
   ],
   "source": [
    "avg = []\n",
    "mx = []\n",
    "states = {}\n",
    "model.train(True)\n",
    "for e in range(20):\n",
    "    for it in range(len(train_data)//batch_size):\n",
    "        #---------------SETUP BATCH DATA-------------\n",
    "        df = train_data.sample(frac=batch_size/len(train_data))\n",
    "        users = df.user_id.values\n",
    "        items = df.movie_id.values\n",
    "        targets = torch.FloatTensor(df.rating.values)\n",
    "        assert users.shape==(batch_size,)==items.shape\n",
    "        \n",
    "        #----------------TRAIN MODEL------------------------\n",
    "        opt.zero_grad()\n",
    "        preds = model(users,items)\n",
    "        mx.append((preds.max().item(),preds.min().item()))\n",
    "        loss = criterion(preds,targets)\n",
    "        assert preds.shape==targets.shape\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        avg.append(loss.item())\n",
    "\n",
    "#         if it%500==0:\n",
    "#             print(f\"Iter {it}: {sum(avg)/len(avg)}\")\n",
    "\n",
    "    print(f\"Epoch {e+1}:\",sum(avg)/len(avg))\n",
    "    avg = []\n",
    "    states[e+1] = model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "93ab0f86-4dc9-4231-b2e0-0643558f2198",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.view(-1).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5acfcc3e-b1b3-49e7-8ff3-71d27be3b9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    #model.load_state_dict(states[20])\n",
    "    model.train(False)\n",
    "    users = test_data.user_id.values\n",
    "    items = test_data.movie_id.values\n",
    "    test_data['pred'] = model(users,items).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f6e7f1c1-4a90-4d88-b58b-ad4d52b88954",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.768317251415737"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(test_data.pred,test_data.rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a6c906b9-4e7c-476e-8189-4a2c3776e240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.153680085156602 1.4462609 3.137181\n"
     ]
    }
   ],
   "source": [
    "test_data['pp'] = test_data.pred.clip(0,5).round()\n",
    "print(test_data.rating.std(),test_data.pp.std(),test_data.pred.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07433d9b-da6a-49b0-8ca7-6cf876b5e937",
   "metadata": {},
   "source": [
    "### LLM Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "349420a9-c93d-473e-bf80-7c5968337eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/linghuang/miniconda3/envs/llama-index5/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"xiyuez/red-dot-design-award-product-description\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfa236b2-d5ca-48bd-8d10-e6c1751a16e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['product', 'category', 'description', 'text'],\n",
      "        num_rows: 21183\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1731c5ce-f8ac-41ba-849a-30343d861622",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataset['train'].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21fc8c0e-6cfb-41da-b15e-54627818f0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18210f5d-3536-4ae4-8227-819c2e9ac474",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/linghuang/miniconda3/envs/llama-index5/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61a3514c-55e8-42a5-9145-8eb41d6c09cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some sample sentences to convert to embeddings\n",
    "sentences = [\n",
    "  \"This is an example sentence\", \n",
    "  \"Each sentence is converted into an embedding\",\n",
    "  \"An embedding is nothing more than a large, numerical representation of the contents of a unit of text\"\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffdf47ec-6890-423a-b4eb-1f4b192009d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = model.encode(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48841f83-be63-48ab-96be-3931d2adb907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.06765681,  0.06349584,  0.04871313, ...,  0.03807075,\n",
       "         0.05996534, -0.04222878],\n",
       "       [ 0.06446958,  0.01299917,  0.06469141, ...,  0.07068044,\n",
       "         0.08054673, -0.05417692],\n",
       "       [ 0.01413998, -0.03184138, -0.03253142, ...,  0.01166508,\n",
       "         0.02897943, -0.02880162]], dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ed8e296f-1d9b-4e86-9f7f-2673e63a78a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Document\n",
    "\n",
    "df['combined'] = 'product: ' + df['product'] + ', category: ' + df['category'] + ', description: ' + df['description'] + ', text: ' + df['text']\n",
    "\n",
    "documents = [\n",
    "    Document(\n",
    "        text=row['combined']\n",
    "    )\n",
    "    for _, row in df.iterrows()\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8aa1bc2-8e24-49e2-9d88-8bf1a4978f99",
   "metadata": {},
   "source": [
    "### Weaviate Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "479853d1-89b8-4d52-aa5f-2e36b390105e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5c649647-c59b-4a27-b8ee-7d205c41b35c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/linghuang/miniconda3/envs/llama-index5/lib/python3.10/site-packages/weaviate/warnings.py:121: DeprecationWarning: Dep005: You are using weaviate-client version 3.26.2. The latest version is 4.6.4.\n",
      "            Please consider upgrading to the latest version. See https://weaviate.io/developers/weaviate/client-libraries/python for details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import weaviate\n",
    "\n",
    "client = weaviate.Client(\n",
    "    url=os.getenv(\"WEAVIATE_URL\"), # Replace with your Weaviate Cloud URL\n",
    "    auth_client_secret=weaviate.auth.AuthApiKey(os.getenv(\"WEAVIATE_API_KEY\")),  # Replace w/ your Weaviate instance API key\n",
    "    additional_headers={\"X-Cohere-Api-Key\": os.environ['COHERE_API_KEY'],}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eca8a29e-ea2c-4476-bfef-ab221c396e75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.is_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1a221874-e21e-408e-8b6f-270e3e59ebd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, StorageContext\n",
    "from llama_index.vector_stores.weaviate import WeaviateVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3e30ab53-91ca-4e69-80a6-6f82af7b4388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct vector store\n",
    "vector_store = WeaviateVectorStore(\n",
    "    weaviate_client=client, \n",
    "    index_name=\"LlamaIndex\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0966e67d-f684-4d57-b440-bd40373ecb16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# if client.schema.exists(\"LlamaIndex\"):\n",
    "#     client.schema.delete_class(\"LlamaIndex\")\n",
    "    \n",
    "# index = VectorStoreIndex(\n",
    "#     documents,\n",
    "#     storage_context = storage_context,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c263ff2a-ec6a-4c90-a9cf-05cae697f450",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"Can you recommend Mobile Computer?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "31e892d2-7740-4f8a-b64d-a8ce9a4647b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I recommend the MC33 mobile computer for hard-working environments such as retail, warehouse management, and manufacturing. It offers a familiar keypad, fully functional touchscreen, and various configurations to suit different needs. The MC33 provides a high level of operating comfort with features like finger ledges for multiple grip points and an optional scan angle of 45 degrees for improved screen readability.'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bd57dace-e5aa-40c0-860a-9086cfed5406",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.memory import ChatMemoryBuffer\n",
    "\n",
    "memory = ChatMemoryBuffer.from_defaults(token_limit=3900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6a926e05-edb3-47b6-af78-c52ee1f4875f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_engine = index.as_chat_engine(\n",
    "    chat_mode=\"condense_plus_context\",\n",
    "    memory=memory,\n",
    "    context_prompt=(\n",
    "        \"You are a friendly, conversational retail shopping assistant. Use the following context including product names, descriptions, and keywords to show the shopper whats available, help find what they want, and answer any questions\"\n",
    "        \"It's ok if you don't know the answer.\"\n",
    "        \"Here are the relevant documents for the context:\\n\"\n",
    "        \"{context_str}\"\n",
    "        \"\\nInstruction: Use the previous chat history, or the context above, to interact and help the user.\"\n",
    "    ),\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6bc9c7dd-61f6-4b50-bc2a-1f073e14f3a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! Are you looking for gold-plated earrings in particular? We have a beautiful option called Miyu in the Needleworks Earrings category. The Miyu earrings are available in either silver or yellow gold, and they feature a modern design with geometric embellishments. Would you like more information about the Miyu earrings or help with anything else?"
     ]
    }
   ],
   "source": [
    "response = chat_engine.stream_chat(\"gold-plated earrings\")\n",
    "for token in response.response_gen:\n",
    "    print(token, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07c446f-41d3-4b7d-a7cf-e06355e8d13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3dcfeb-8bc8-468c-b963-baec381a2fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = 'You are an AI assistant functioning as a recommendation system for an ecommerce website. Be specific and limit your answers to the requested format. Keep your answers short and concise.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e448710-a773-43d1-b450-377a978d66a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_prompt(ordered_list_of_items):\n",
    "\n",
    "   # assemble user prompt\n",
    "  prompt = None\n",
    "  if len(ordered_list_of_items) > 0:\n",
    "    items = ', '.join(ordered_list_of_items)\n",
    "    prompt =  f\"A user bought the following items: {items}. What next ten items would he/she be likely to purchase next?\"\n",
    "    prompt += \" Express your response as a JSON object with a key of 'next_items' and a value representing your array of recommended items.\"\n",
    " \n",
    "  return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18528a3b-47a8-4644-8176-63503d2140ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get prompt and results\n",
    "user_prompt = get_user_prompt(\n",
    "    ['scarf', 'beanie', 'ear muffs', 'thermal underwear']\n",
    "    )\n",
    "\n",
    "print(user_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51eed9b-2c36-481f-91bf-62c6a0fe9c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ChatCompletion.create(\n",
    "  model='llama-2-70b-chat',\n",
    "  messages=[\n",
    "    {'role': 'system', 'content': system_prompt},\n",
    "    {'role': 'user','content': user_prompt}\n",
    "    ],\n",
    "  max_tokens=128\n",
    "  )\n",
    "print(f'response.message:{response.message}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdb0c11-1efc-49d8-a3d6-4cdd228d0a66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289c6931-fea4-40fb-8ae2-2a01cb120874",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4dc4ecd0-651f-4c7f-8410-c82803d9898b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding \n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "import torch\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.core import VectorStoreIndex, ServiceContext, download_loader\n",
    "import accelerate\n",
    "from llama_index.core.memory import ChatMemoryBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94bd78e5-91bf-422b-82dd-b898b93e912d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TEXT_LENGTH=1024 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6074389-316e-4d13-9319-83316188152f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_truncate(val):\n",
    "    \"\"\"Truncate the given text.\"\"\"\n",
    "    return val[:MAX_TEXT_LENGTH]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcf01d8-8966-48d8-8fac-3fc2735af0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_prods_df = pd.read_csv(\"product_data.csv\", converters={\n",
    "    'bullet_point': auto_truncate,\n",
    "    'item_keywords': auto_truncate,\n",
    "    'item_name': auto_truncate,\n",
    "    'material': auto_truncate\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "256a152b-b99b-4299-8b68-23952648ac1a",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'dataset/product.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m product_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdataset/product.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\t\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m product_df\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[0;32m~/miniconda3/envs/llama-index5/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llama-index5/lib/python3.10/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/miniconda3/envs/llama-index5/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llama-index5/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/miniconda3/envs/llama-index5/lib/python3.10/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dataset/product.csv'"
     ]
    }
   ],
   "source": [
    "product_df = pd.read_csv(\"dataset/product.csv\", sep='\\t')\n",
    "product_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53f0242-f80f-4da7-8287-ed7cb7a7d692",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
