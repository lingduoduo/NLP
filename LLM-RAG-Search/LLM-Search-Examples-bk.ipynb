{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cd32ff2-6fa5-4d75-bde5-7fd88cbbeef5",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8f8a129-73dd-4891-a5a8-a65e72216ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD:notebook/LLM-Search-Examples.ipynb
      "/Users/ling/miniconda3/envs/nlp/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-02-04 18:48:30.848168: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
=======
      "/Users/linghuang/miniconda3/envs/llm-rag/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
>>>>>>> 622f5347e3daaca02360b5b7ad9cc57bd0b21394:notebook/LLM-RAG/LLM-Search-Examples-bk.ipynb
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import BertModel\n",
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7c3a28-bd0e-4499-9069-574a45115ccb",
   "metadata": {},
   "source": [
    "### Search semantics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be43f978-986b-4f01-beb7-7e2a1c70bcce",
   "metadata": {},
   "source": [
    "The Hugging Face summarization task page lists models that support summarization. In this section, we will the following resources:\n",
    "\n",
    "- data: We will work with the 'xsum dataset,' containing a collection of BBC articles and their corresponding summaries. This dataset serves as the foundation for our tasks.\n",
    "\n",
    "- model: Our chosen model is the 't5-small model,' with 60 million parameters (equivalent to 242MB for PyTorch). T5, an encoder-decoder model developed by Google, boasts versatility, supporting various tasks including summarization, translation, question-answering, and text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b08e40f8-a60a-4905-83f9-78aa158392c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD:notebook/LLM-Search-Examples.ipynb
      "Found cached dataset xsum (/Users/ling/.cache/huggingface/datasets/xsum/default/1.2.0/082863bf4754ee058a5b6f6525d0cb2b18eadb62c7b370b095d1364050a52b71)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 35.24it/s]\n"
=======
      "/Users/linghuang/miniconda3/envs/llm-rag/lib/python3.10/site-packages/datasets/load.py:1429: FutureWarning: The repository for xsum contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/xsum\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
>>>>>>> 622f5347e3daaca02360b5b7ad9cc57bd0b21394:notebook/LLM-RAG/LLM-Search-Examples-bk.ipynb
     ]
    }
   ],
   "source": [
    "xsum_dataset = load_dataset(\n",
    "    \"xsum\", version=\"1.2.0\"\n",
    ")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1397a4f0-aab2-4211-b9cc-f0e06dc5c390",
   "metadata": {},
   "source": [
    "This dataset provides 3 columns:\n",
    "\n",
    "- document: the BBC article text\n",
    "- summary: a \"ground-truth\" summary --> Note how subjective this \"ground-truth\" is. Is this the same summary you would write? This a great example of how many LLM applications do not have obvious \"right\" answers.\n",
    "- id: article ID"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD:notebook/LLM-Search-Examples.ipynb
   "execution_count": 3,
=======
   "execution_count": 8,
>>>>>>> 622f5347e3daaca02360b5b7ad9cc57bd0b21394:notebook/LLM-RAG/LLM-Search-Examples-bk.ipynb
   "id": "4dbab8df-467b-4004-8ad1-9aecdd5ac626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking a sample of 100 rows\n",
    "xsum_sample = xsum_dataset[\"train\"].select(range(1000)).to_pandas()\n",
    "\n",
    "# Combining 'document' and 'summary' columns\n",
    "xsum_sample[\"combined\"] = (\n",
    "    \"Document: \" + xsum_sample.document.str.strip() + \"; Summary: \" + xsum_sample.summary.str.strip()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD:notebook/LLM-Search-Examples.ipynb
   "execution_count": 4,
=======
   "execution_count": 9,
>>>>>>> 622f5347e3daaca02360b5b7ad9cc57bd0b21394:notebook/LLM-RAG/LLM-Search-Examples-bk.ipynb
   "id": "f7ddf71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "#encoding the data\n",
    "encoder = SentenceTransformer(\"paraphrase-mpnet-base-v2\")\n",
    "encoded_data = encoder.encode(xsum_sample[\"combined\"])"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD:notebook/LLM-Search-Examples.ipynb
   "execution_count": 5,
=======
   "execution_count": 10,
>>>>>>> 622f5347e3daaca02360b5b7ad9cc57bd0b21394:notebook/LLM-RAG/LLM-Search-Examples-bk.ipynb
   "id": "36ea4500",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.12973613, -0.07995621, -0.02103525, ...,  0.01458147,\n",
       "        -0.04181118,  0.05969834],\n",
       "       [-0.10183043, -0.00813398,  0.01535375, ...,  0.03995895,\n",
       "        -0.10245819,  0.08624592],\n",
       "       [-0.06544365, -0.22466174,  0.01042669, ...,  0.06865789,\n",
       "         0.0731439 ,  0.01244215],\n",
       "       ...,\n",
       "       [-0.00177471, -0.11989915,  0.03554984, ...,  0.01920111,\n",
       "        -0.06250598, -0.0915705 ],\n",
       "       [-0.15317537,  0.04223507, -0.00396573, ...,  0.15491395,\n",
       "        -0.01295559, -0.09967731],\n",
       "       [-0.11358325,  0.03419029,  0.06424171, ...,  0.04367112,\n",
       "        -0.21225762,  0.05421832]], dtype=float32)"
      ]
     },
<<<<<<< HEAD:notebook/LLM-Search-Examples.ipynb
     "execution_count": 5,
=======
     "execution_count": 10,
>>>>>>> 622f5347e3daaca02360b5b7ad9cc57bd0b21394:notebook/LLM-RAG/LLM-Search-Examples-bk.ipynb
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_data"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD:notebook/LLM-Search-Examples.ipynb
   "execution_count": 6,
   "id": "81792db3",
=======
   "execution_count": 11,
   "id": "c7d4dc80-4f67-4f55-98ce-63b8ad737ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e7c5f7b-b546-4986-aad5-f289efc713ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = faiss.IndexIDMap(faiss.IndexFlatIP(768))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1883d64c-fed8-46e0-b808-e59bc5300c31",
>>>>>>> 622f5347e3daaca02360b5b7ad9cc57bd0b21394:notebook/LLM-RAG/LLM-Search-Examples-bk.ipynb
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss.normalize_L2(encoded_data)\n",
    "index.add_with_ids(encoded_data, np.arange(len(encoded_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3c08df",
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD:notebook/LLM-Search-Examples.ipynb
    "search_text = \"Harry Potter\""
=======
    "search_text = \"harry potter\""
>>>>>>> 622f5347e3daaca02360b5b7ad9cc57bd0b21394:notebook/LLM-RAG/LLM-Search-Examples-bk.ipynb
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd69adea",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_vector = encoder.encode(search_text)\n",
    "_vector = np.array([search_vector])\n",
    "faiss.normalize_L2(_vector)\n",
    "\n",
    "k = 3\n",
    "distances, ann = index.search(_vector, k=k)\n",
    "results = pd.DataFrame({'distances': distances[0], 'ann': ann[0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce097a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84623a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "xsum_sample[\"combined\"][results['ann'][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a14265",
   "metadata": {},
   "outputs": [],
   "source": [
    "xsum_sample[\"summary\"][results['ann'][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9919ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "xsum_sample[\"summary\"][results['ann'][2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9716c604",
   "metadata": {},
   "source": [
    "### Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8e051c8e-7fb7-47c9-9164-78490e6331c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer = pipeline(\n",
    "    task=\"summarization\",\n",
    "    model=\"t5-small\",\n",
    "    min_length=20,\n",
    "    max_length=60,\n",
    "    truncation=True,\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "68ff9cbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Clean-up operations are continuing across the Scottish Borders and Dumfries and Galloway after flooding caused by Storm Frank.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xsum_sample[\"summary\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b1ed3372",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': 'the full cost of damage in Newton Stewart is still being assessed . many roads in peeblesshire remain badly affected by standing water . the water breached a retaining wall, flooding many commercial properties .'}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarizer(xsum_sample[\"document\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "49c7fb3d-a209-4b3a-bed0-7df30a3f9a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply to a batch of articles\n",
    "results = summarizer(xsum_sample[\"document\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a39a4766-ce68-4f01-a3e3-6d500728b02c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>generated_summary</th>\n",
       "      <th>summary</th>\n",
       "      <th>document</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the full cost of damage in Newton Stewart is s...</td>\n",
       "      <td>Clean-up operations are continuing across the ...</td>\n",
       "      <td>The full cost of damage in Newton Stewart, one...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a fire alarm went off at the Holiday Inn in Ho...</td>\n",
       "      <td>Two tourist buses have been destroyed by fire ...</td>\n",
       "      <td>A fire alarm went off at the Holiday Inn in Ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sebastian Vettel will start third ahead of tea...</td>\n",
       "      <td>Lewis Hamilton stormed to pole position at the...</td>\n",
       "      <td>Ferrari appeared in a position to challenge un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the 67-year-old is accused of committing the o...</td>\n",
       "      <td>A former Lincolnshire Police officer carried o...</td>\n",
       "      <td>John Edward Bates, formerly of Spalding, Linco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a man receiving psychiatric treatment at the c...</td>\n",
       "      <td>An armed man who locked himself into a room at...</td>\n",
       "      <td>Patients and staff were evacuated from Cerahpa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Gregor Townsend gave a debut to powerhouse win...</td>\n",
       "      <td>Defending Pro12 champions Glasgow Warriors bag...</td>\n",
       "      <td>Simone Favaro got the crucial try with the las...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Veronica Vanessa Chango-Alverez, 31, was kille...</td>\n",
       "      <td>A man with links to a car that was involved in...</td>\n",
       "      <td>Veronica Vanessa Chango-Alverez, 31, was kille...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>the 25-year-old was hit by a motorbike during ...</td>\n",
       "      <td>Welsh cyclist Luke Rowe says changes to the sp...</td>\n",
       "      <td>Belgian cyclist Demoitie died after a collisio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>gundogan will not be fit for the start of the ...</td>\n",
       "      <td>Manchester City midfielder Ilkay Gundogan says...</td>\n",
       "      <td>Gundogan, 26, told BBC Sport he \"can see the f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>the crash happened about 07:20 GMT at the junc...</td>\n",
       "      <td>A jogger has been hit by an unmarked police ca...</td>\n",
       "      <td>The crash happened about 07:20 GMT at the junc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   generated_summary  \\\n",
       "0  the full cost of damage in Newton Stewart is s...   \n",
       "1  a fire alarm went off at the Holiday Inn in Ho...   \n",
       "2  Sebastian Vettel will start third ahead of tea...   \n",
       "3  the 67-year-old is accused of committing the o...   \n",
       "4  a man receiving psychiatric treatment at the c...   \n",
       "5  Gregor Townsend gave a debut to powerhouse win...   \n",
       "6  Veronica Vanessa Chango-Alverez, 31, was kille...   \n",
       "7  the 25-year-old was hit by a motorbike during ...   \n",
       "8  gundogan will not be fit for the start of the ...   \n",
       "9  the crash happened about 07:20 GMT at the junc...   \n",
       "\n",
       "                                             summary  \\\n",
       "0  Clean-up operations are continuing across the ...   \n",
       "1  Two tourist buses have been destroyed by fire ...   \n",
       "2  Lewis Hamilton stormed to pole position at the...   \n",
       "3  A former Lincolnshire Police officer carried o...   \n",
       "4  An armed man who locked himself into a room at...   \n",
       "5  Defending Pro12 champions Glasgow Warriors bag...   \n",
       "6  A man with links to a car that was involved in...   \n",
       "7  Welsh cyclist Luke Rowe says changes to the sp...   \n",
       "8  Manchester City midfielder Ilkay Gundogan says...   \n",
       "9  A jogger has been hit by an unmarked police ca...   \n",
       "\n",
       "                                            document  \n",
       "0  The full cost of damage in Newton Stewart, one...  \n",
       "1  A fire alarm went off at the Holiday Inn in Ho...  \n",
       "2  Ferrari appeared in a position to challenge un...  \n",
       "3  John Edward Bates, formerly of Spalding, Linco...  \n",
       "4  Patients and staff were evacuated from Cerahpa...  \n",
       "5  Simone Favaro got the crucial try with the las...  \n",
       "6  Veronica Vanessa Chango-Alverez, 31, was kille...  \n",
       "7  Belgian cyclist Demoitie died after a collisio...  \n",
       "8  Gundogan, 26, told BBC Sport he \"can see the f...  \n",
       "9  The crash happened about 07:20 GMT at the junc...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(\n",
    "    pd.DataFrame.from_dict(results)\n",
    "    .rename({\"summary_text\": \"generated_summary\"}, axis=1)\n",
    "    .join(pd.DataFrame.from_dict(xsum_sample))[\n",
    "        [\"generated_summary\", \"summary\", \"document\"]\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e428da-bb6a-4757-b341-0e90a6c02b2f",
   "metadata": {},
   "source": [
    "### Search and sampling in inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15021d61-8839-4d7a-b8db-7d9335178fdc",
   "metadata": {},
   "source": [
    "You may see parameters like `num_beams`, `do_sample`, etc. specified in Hugging Face pipelines.  These are inference configurations.\n",
    "\n",
    "LLMs work by predicting (generating) the next token, then the next, and so on.  The goal is to generate a high probability sequence of tokens, which is essentially a search through the (enormous) space of potential sequences.\n",
    "\n",
    "To do this search, LLMs use one of two main methods:\n",
    "* **Search**: Given the tokens generated so far, pick the next most likely token in a \"search.\"\n",
    "   * **Greedy search** (default): Pick the single next most likely token in a greedy search.\n",
    "   * **Beam search**: Greedy search can be extended via beam search, which searches down several sequence paths, via the parameter `num_beams`.\n",
    "* **Sampling**: Given the tokens generated so far, pick the next token by sampling from the predicted distribution of tokens.\n",
    "   * **Top-K sampling**: The parameter `top_k` modifies sampling by limiting it to the `k` most likely tokens.\n",
    "   * **Top-p sampling**: The parameter `top_p` modifies sampling by limiting it to the most likely tokens up to probability mass `p`.\n",
    "\n",
    "You can toggle between search and sampling via parameter `do_sample`.\n",
    "\n",
    "For more background on search and sampling, see [this Hugging Face blog post](https://huggingface.co/blog/how-to-generate).\n",
    "\n",
    "We will illustrate these various options below using our summarization pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d39ed141-bdd3-42fc-81e2-05c759939c0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': 'the full cost of damage in Newton Stewart is still being assessed . many roads in peeblesshire remain badly affected by standing water . the water breached a retaining wall, flooding many commercial properties .'}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can instead do a beam search by specifying num_beams.\n",
    "summarizer(xsum_sample[\"document\"][0], num_beams=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "212e9573-1594-4c99-a641-cf4c93de77e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': 'the full cost of damage in Newton Stewart is still being assessed . many roads in peeblesshire remain badly affected by standing water . the waters breached a retaining wall, flooding many commercial properties .'}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Alternatively, we could use sampling.\n",
    "summarizer(xsum_sample[\"document\"][0], do_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a885e929-8161-4ded-bf31-9a46ac392000",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': 'the full cost of damage in Newton Stewart is still being assessed . many roads in peeblesshire remain badly affected by standing water . the water breached a retaining wall, flooding many commercial properties .'}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can modify sampling to be more greedy by limiting sampling to the top_k or top_p most likely next tokens.\n",
    "summarizer(xsum_sample[\"document\"][0], do_sample=True, top_k=10, top_p=0.8)â€º"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc314c9b-15c3-47b2-a81a-0db194755a9a",
   "metadata": {},
   "source": [
    "#### --- The end ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "24ff1057-b148-443b-9624-5125fa59693f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "from rich import print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "57f1506b-f136-42a7-996b-9123e1abadd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">the full cost of damage in Newton Stewart is still being assessed. many roads in peeblesshire remain badly affected\n",
       "by standing water. the water breached a retaining wall, flooding many commercial properties.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "the full cost of damage in Newton Stewart is still being assessed. many roads in peeblesshire remain badly affected\n",
       "by standing water. the water breached a retaining wall, flooding many commercial properties.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the pre-trained tokenizer and model.\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")\n",
    "\n",
    "\n",
    "# For summarization, T5-small expects a prefix \"summarize: \"\n",
    "# so we prepend that to each article as a prompt.\n",
    "\n",
    "articles = list(map(lambda article: \"summarize: \" + article, xsum_sample[\"document\"]))\n",
    "\n",
    "# Tokenize the input\n",
    "inputs = tokenizer(\n",
    "    articles, max_length=1024,\n",
    "    return_tensors=\"pt\", padding=True, truncation=True\n",
    ")\n",
    "# Generate summaries\n",
    "summary_ids = model.generate(\n",
    "    inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    num_beams=2,\n",
    "    min_length=0,\n",
    "    max_length=60,\n",
    ")\n",
    "# Decode the generated summaries\n",
    "decoded_summaries = tokenizer.batch_decode(summary_ids, skip_special_tokens=True)\n",
    "\n",
    "print(decoded_summaries[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d74048-beb3-4805-a3be-f56f884b7e3f",
   "metadata": {},
   "source": [
    "### Zero-shot classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4087bee1-58ce-46ff-a4eb-94892d3f8315",
   "metadata": {},
   "source": [
    "Zero-shot classification (or zero-shot learning) is the task of classifying a piece of text into one of a few given categories or labels, without having explicitly trained the model to predict those categories beforehand. The idea appeared in literature before modern LLMs, but recent advances in LLMs have made zero-shot learning much more flexible and powerful.\n",
    "\n",
    "Background reading: See the Hugging Face task page on zero-shot classification or Wikipedia on zero-shot learning.\n",
    "\n",
    "In this section, we will use:\n",
    "\n",
    "Data: a few example articles from the xsum dataset used in the Summarization section above. Our goal is to label news articles under a few categories.\n",
    "Model: nli-deberta-v3-small, a fine-tuned version of the DeBERTa model. The DeBERTa base model was developed by Microsoft and is one of several models derived from BERT; for more details on DeBERTa, see the Hugging Face doc page, the code on GitHub, or the research paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04b39106-5d0b-4314-9c5e-494c9114e9c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/linghuang/miniconda3/envs/langchain/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:473: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "zero_shot_pipeline = pipeline(\n",
    "    task=\"zero-shot-classification\",\n",
    "    model=\"cross-encoder/nli-deberta-v3-small\",\n",
    ")\n",
    "\n",
    "\n",
    "def categorize_article(article: str) -> None:\n",
    "    \"\"\"\n",
    "    This helper function defines the categories (labels) which the model must use to label articles.\n",
    "    Note that our model was NOT fine-tuned to use these specific labels,\n",
    "    but it \"knows\" what the labels mean from its more general training.\n",
    "\n",
    "    This function then prints out the predicted labels alongside their confidence scores.\n",
    "    \"\"\"\n",
    "    results = zero_shot_pipeline(\n",
    "        article,\n",
    "        candidate_labels=[\n",
    "            \"politics\",\n",
    "            \"finance\",\n",
    "            \"sports\",\n",
    "            \"science and technology\",\n",
    "            \"pop culture\",\n",
    "            \"breaking news\",\n",
    "        ],\n",
    "    )\n",
    "    # Print the results nicely\n",
    "    del results[\"sequence\"]\n",
    "    display(pd.DataFrame(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d5593d5-a67d-4dea-bc2a-49604993a33d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>science and technology</td>\n",
       "      <td>0.762033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>breaking news</td>\n",
       "      <td>0.089980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pop culture</td>\n",
       "      <td>0.059526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>finance</td>\n",
       "      <td>0.036244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sports</td>\n",
       "      <td>0.027556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>politics</td>\n",
       "      <td>0.024660</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   labels    scores\n",
       "0  science and technology  0.762033\n",
       "1           breaking news  0.089980\n",
       "2             pop culture  0.059526\n",
       "3                 finance  0.036244\n",
       "4                  sports  0.027556\n",
       "5                politics  0.024660"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "categorize_article(\n",
    "    \"\"\"\n",
    "A large language model (LLM) is a language model notable for its ability to achieve general-purpose language understanding and generation. LLMs acquire these abilities by learning statistical relationships from text documents during a computationally intensive self-supervised and semi-supervised training process.[1] LLMs are artificial neural networks following a transformer architecture.[2]\n",
    "\n",
    "They can be used for text generation by taking an input text and repeatedly predicting the next token or word.[3] Up to 2020, fine tuning was the only way a model could be adapted to be able to accomplish specific tasks. Larger sized models, such as GPT-3, however, can be prompt-engineered to achieve similar results.[4] They are thought to acquire knowledge about syntax, semantics and \"ontology\" inherent in human language corpora, but also inaccuracies and biases present in the corpora.[5]\n",
    "\n",
    "Notable examples include OpenAI's GPT models (e.g., GPT-3.5 and GPT-4, used in ChatGPT), Google's PaLM (used in Bard), and Meta's LLaMA, as well as BLOOM, Ernie 3.0 Titan, and Anthropic's Claude 2.\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1721a6d3-dfe4-4ba7-96e2-0ba5c4ffc760",
   "metadata": {},
   "source": [
    "### Few-shot learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb37eff-eac4-4307-81d4-50d274c12d41",
   "metadata": {},
   "source": [
    "In few-shot learning tasks, you give the model an instruction, a few query-response examples of how to follow that instruction, and then a new query. The model must generate the response for that new query. This technique has pros and cons: it is very powerful and allows models to be reused for many more applications, but it can be finicky and require significant prompt engineering to get good and reliable results.\n",
    "\n",
    "Background reading: See the Wikipedia page on few-shot learning or this Hugging Face blog about few-shot learning.\n",
    "\n",
    "In this section, we will use:\n",
    "\n",
    "- Task: Few-shot learning can be applied to many tasks. Here, we will do sentiment analysis, which was covered earlier. However, you will see how few-shot learning allows us to specify custom labels, whereas the previous model was tuned for a specific set of labels. We will also show other (toy) tasks at the end. In terms of the Hugging Face task specified in the pipeline constructor, few-shot learning is handled as a text-generation task.\n",
    "- Data: We use a few examples, including a tweet example from the blog post linked above.\n",
    "- Model: gpt-neo-1.3B, a version of the GPT-Neo model discussed in the blog linked above. It is a transformer model with 1.3 billion parameters developed by Eleuther AI. For more details, see the code on GitHub or the research paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "644a356f-d0fc-4014-955a-c0959d700d3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a1f15f5ce524568acf13950dae761ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.35k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d0d5aaa11f543ae8dfaff0f30e1bf8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/5.31G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bfda51a0f2e45f78d20d50b418a046f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/200 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96ecfea224f3482491bb4b437874df4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "920bafb37ddc4ba5b5457b0cb1c8b879",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f83a014ac69d42ea9664f706b083eac7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/90.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We will limit the response length for our few-shot learning tasks.\n",
    "few_shot_pipeline = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=\"EleutherAI/gpt-neo-1.3B\",\n",
    "    max_new_tokens=10,\n",
    "    # model_kwargs={\"cache_dir\": DA.paths.datasets},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2c3c2c61-538e-4a57-ae8d-50a9ea4c94e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the token ID for \"###\", which we will use as the EOS token below.\n",
    "eos_token_id = few_shot_pipeline.tokenizer.encode(\"###\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2fd4cabc-7142-4fa2-8188-1e488f918ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:21017 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For each tweet, describe its sentiment:\n",
      "\n",
      "[Tweet]: \"This new music video was incredible\"\n",
      "[Sentiment]: \"Very positive\"\n",
      "\n",
      "And the sentiment:\n"
     ]
    }
   ],
   "source": [
    "# Without any examples, the model output is inconsistent and usually incorrect.\n",
    "results = few_shot_pipeline(\n",
    "    \"\"\"For each tweet, describe its sentiment:\n",
    "\n",
    "[Tweet]: \"This new music video was incredible\"\n",
    "[Sentiment]:\"\"\",\n",
    "    eos_token_id=eos_token_id,\n",
    ")\n",
    "\n",
    "print(results[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2921d329-99c8-40f2-ac18-1db254d6becb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:21017 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For each tweet, describe its sentiment:\n",
      "\n",
      "[Tweet]: \"This is the link to the article\"\n",
      "[Sentiment]: Neutral\n",
      "###\n",
      "[Tweet]: \"This new music video was incredible\"\n",
      "[Sentiment]: Strong\n",
      "###\n"
     ]
    }
   ],
   "source": [
    "# With only 1 example, the model may or may not get the answer right.\n",
    "results = few_shot_pipeline(\n",
    "    \"\"\"For each tweet, describe its sentiment:\n",
    "\n",
    "[Tweet]: \"This is the link to the article\"\n",
    "[Sentiment]: Neutral\n",
    "###\n",
    "[Tweet]: \"This new music video was incredible\"\n",
    "[Sentiment]:\"\"\",\n",
    "    eos_token_id=eos_token_id,\n",
    ")\n",
    "\n",
    "print(results[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "af09b8a1-bc3e-4f77-939d-6546755ee0a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:21017 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For each tweet, describe its sentiment:\n",
      "\n",
      "[Tweet]: \"I hate it when my phone battery dies.\"\n",
      "[Sentiment]: Negative\n",
      "###\n",
      "[Tweet]: \"My day has been ðŸ‘\"\n",
      "[Sentiment]: Positive\n",
      "###\n",
      "[Tweet]: \"This is the link to the article\"\n",
      "[Sentiment]: Neutral\n",
      "###\n",
      "[Tweet]: \"This new music video was incredible\"\n",
      "[Sentiment]: Positive\n",
      "###\n"
     ]
    }
   ],
   "source": [
    "# With 1 example for each sentiment, the model is more likely to understand!\n",
    "results = few_shot_pipeline(\n",
    "    \"\"\"For each tweet, describe its sentiment:\n",
    "\n",
    "[Tweet]: \"I hate it when my phone battery dies.\"\n",
    "[Sentiment]: Negative\n",
    "###\n",
    "[Tweet]: \"My day has been ðŸ‘\"\n",
    "[Sentiment]: Positive\n",
    "###\n",
    "[Tweet]: \"This is the link to the article\"\n",
    "[Sentiment]: Neutral\n",
    "###\n",
    "[Tweet]: \"This new music video was incredible\"\n",
    "[Sentiment]:\"\"\",\n",
    "    eos_token_id=eos_token_id,\n",
    ")\n",
    "\n",
    "print(results[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7a12ec15-961d-4c54-8909-ddc7f2649941",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:21017 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For each food, suggest a good drink pairing:\n",
      "\n",
      "[food]: tapas\n",
      "[drink]: wine\n",
      "###\n",
      "[food]: pizza\n",
      "[drink]: soda\n",
      "###\n",
      "[food]: jalapenos poppers\n",
      "[drink]: beer\n",
      "###\n",
      "[food]: scone\n",
      "[drink]: soda\n",
      "[food]: poppers\n",
      "###\n"
     ]
    }
   ],
   "source": [
    "# The model isn't ready to serve drinks!\n",
    "results = few_shot_pipeline(\n",
    "    \"\"\"For each food, suggest a good drink pairing:\n",
    "\n",
    "[food]: tapas\n",
    "[drink]: wine\n",
    "###\n",
    "[food]: pizza\n",
    "[drink]: soda\n",
    "###\n",
    "[food]: jalapenos poppers\n",
    "[drink]: beer\n",
    "###\n",
    "[food]: scone\n",
    "[drink]:\"\"\",\n",
    "    eos_token_id=eos_token_id,\n",
    ")\n",
    "\n",
    "print(results[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "784ee2d8-26af-459e-ae24-6049edeaf95d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:21017 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given a word describing how someone is feeling, suggest a description of that person.  The description should not include the original word.\n",
      "\n",
      "[word]: happy\n",
      "[description]: smiling, laughing, clapping\n",
      "###\n",
      "[word]: nervous\n",
      "[description]: glancing around quickly, sweating, fidgeting\n",
      "###\n",
      "[word]: sleepy\n",
      "[description]: heavy-lidded, slumping, rubbing eyes\n",
      "###\n",
      "[word]: confused\n",
      "[description]: wandering around, turning in circles, confused\n",
      "###\n"
     ]
    }
   ],
   "source": [
    "results = few_shot_pipeline(\n",
    "    \"\"\"Given a word describing how someone is feeling, suggest a description of that person.  The description should not include the original word.\n",
    "\n",
    "[word]: happy\n",
    "[description]: smiling, laughing, clapping\n",
    "###\n",
    "[word]: nervous\n",
    "[description]: glancing around quickly, sweating, fidgeting\n",
    "###\n",
    "[word]: sleepy\n",
    "[description]: heavy-lidded, slumping, rubbing eyes\n",
    "###\n",
    "[word]: confused\n",
    "[description]:\"\"\",\n",
    "    eos_token_id=eos_token_id,\n",
    ")\n",
    "print(results[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "efc88c98-dcbf-4148-a930-9be5d5c95571",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:21017 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate a book summary from the title:\n",
      "\n",
      "[book title]: \"Stranger in a Strange Land\"\n",
      "[book description]: \"This novel tells the story of Valentine Michael Smith, a human who comes to Earth in early adulthood after being born on the planet Mars and raised by Martians, and explores his interaction with and eventual transformation of Terran culture.\"\n",
      "###\n",
      "[book title]: \"The Adventures of Tom Sawyer\"\n",
      "[book description]: \"This novel is about a boy growing up along the Mississippi River. It is set in the 1840s in the town of St. Petersburg, which is based on Hannibal, Missouri, where Twain lived as a boy. In the novel, Tom Sawyer has several adventures, often with his friend Huckleberry Finn.\"\n",
      "###\n",
      "[book title]: \"Dune\"\n",
      "[book description]: \"This novel is set in the distant future amidst a feudal interstellar society in which various noble houses control planetary fiefs. It tells the story of young Paul Atreides, whose family accepts the stewardship of the planet Arrakis. While the planet is an inhospitable and sparsely populated desert wasteland, it is the only source of melange, or spice, a drug that extends life and enhances mental abilities.  The story explores the multilayered interactions of politics, religion, ecology, technology, and human emotion, as the factions of the empire confront each other in a struggle for the control of Arrakis and its spice.\"\n",
      "###\n",
      "[book title]: \"Blue Mars\"\n",
      "[book description]: \"This is the first of a series of science fiction novels.  It is set in the future, in a future where humans have become a minority.  They are forced to live in a constant state of tension and competition with Earth, a minority\n"
     ]
    }
   ],
   "source": [
    "# We override max_new_tokens to generate longer answers.\n",
    "# These book descriptions were taken from their corresponding Wikipedia pages.\n",
    "results = few_shot_pipeline(\n",
    "    \"\"\"Generate a book summary from the title:\n",
    "\n",
    "[book title]: \"Stranger in a Strange Land\"\n",
    "[book description]: \"This novel tells the story of Valentine Michael Smith, a human who comes to Earth in early adulthood after being born on the planet Mars and raised by Martians, and explores his interaction with and eventual transformation of Terran culture.\"\n",
    "###\n",
    "[book title]: \"The Adventures of Tom Sawyer\"\n",
    "[book description]: \"This novel is about a boy growing up along the Mississippi River. It is set in the 1840s in the town of St. Petersburg, which is based on Hannibal, Missouri, where Twain lived as a boy. In the novel, Tom Sawyer has several adventures, often with his friend Huckleberry Finn.\"\n",
    "###\n",
    "[book title]: \"Dune\"\n",
    "[book description]: \"This novel is set in the distant future amidst a feudal interstellar society in which various noble houses control planetary fiefs. It tells the story of young Paul Atreides, whose family accepts the stewardship of the planet Arrakis. While the planet is an inhospitable and sparsely populated desert wasteland, it is the only source of melange, or spice, a drug that extends life and enhances mental abilities.  The story explores the multilayered interactions of politics, religion, ecology, technology, and human emotion, as the factions of the empire confront each other in a struggle for the control of Arrakis and its spice.\"\n",
    "###\n",
    "[book title]: \"Blue Mars\"\n",
    "[book description]:\"\"\",\n",
    "    eos_token_id=eos_token_id,\n",
    "    max_new_tokens=50,\n",
    ")\n",
    "\n",
    "print(results[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "obvious-whole",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
